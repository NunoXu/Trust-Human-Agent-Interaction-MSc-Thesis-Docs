% This is "Model.tex"
% A content sub-file for Trustfull Action Suggestion Thesis Extended Abstract

\section{Trust Model}
\label{sec:TrustModel}
We sought out to develop a trust model definition that would be easily implementable, but generic enough to be able to adapt to various testing scenarios. To do this we took inspiration from the work by Sabater et al.\cite{Sabater2006} described in Section \ref{subsubsec:Related work:Trust Models:Repage} by taking a similar approach to architecture where a central memory component holds the model's current state, getting updated by perceptions received from the environment. But while Repage describes a third module that suggests actions to resolve belief conflicts in the model, we instead defined such module to assume the point of view of one of the agents in the scenario and if participating in a social interaction, it suggests actions to improve the trust relationship with a trustor. And so, the model can be described by 3 main components:
\begin{itemize}
    \item \textbf{Memory}, which defines and stores the main model structure;
    \item \textbf{Perceptions}, a series of environment inputs mapped to changes in the Memory;
    \item \textbf{Action Suggestion}, a module that outputs different actions depending on current perceptions and the state of the model.
\end{itemize}

\subsection{Memory}
One of the main concerns while designing the model was how trust would be calculated, as we wanted to use Castlefranchi and Falcone's conceptualization of trust \cite{Castelfranchi2010} as a basis for trust definition, focusing specially on it being dependent on the task entrusted, and the transferability of trust between different tasks. But starting from the five-part definition of trust, as seen in Equation \ref{eq:TrustRelation}, we decided that inserting context (\textbf{C}) and the trustor's goal (\bm{$g_x$}) into the model would bring in too much complexity for the scope of this thesis, as it would require for a world state model to be kept, as well as some way to predict the trustor's goal. So we simplified, defining trust through a simpler three-part relation, involving just the trustor (\textbf{X}), the trustee (\textbf{Y}) and the task ($\bm{\tau}$), represented in Equation \ref{eq:TrustCalc}.
\begin{equation}
TRUST(X\ Y\ \tau)
\label{eq:TrustCalc}
\end{equation}

So we designed the structure with the concepts and relations represented in Figure \ref{fig:MemoryArchitecture}, and describing them as follows:

\begin{itemize}
    \item \textbf{Agent}: a simple representation of the known entities in the scenario world space, serving mostly as an identifier;
    \item \textbf{Trustee}: each agent contains a collection of other agents he has information about,  either by reputation, or by interaction, which we represent as their Trustees;
    \item \textbf{Trust Feature}: a piece of information a trustor has on a trustee is represented in a Trust Feature, which contains the Belief Sources of said information. The Feature Model defines and uniquely identifies what feature is represented.
    \item \textbf{Feature Model}: the possible set of trust features from which a trustee can be assigned is defined in a collection of Feature Models where each one represents a possible piece of trust related information relevant to the model scenario (e.g. The trustee's ability to cook, or the willingness to drive);
    \item \textbf{Category}: a Feature Model must belong to a Category, making it easier to present the different type of Trust Features;
    \item \textbf{Belief Source}: this represents a source of information, providing 3 values to determine the associated feature's belief value: 
        \begin{itemize}
            \item Belief Value, a number between 0.0 and 1.0 describing the trustor evaluation;
            \item Certainty describes how well the trustee was evaluated, in Reputation for instance, this might represent how well we trust in the reporter, and in Direct Contact how well the trustor observed the trustee performing said feature;
            \item Time is just a record of when was this belief source recorded, as older records might have a lower impact in the overall belief value score, compared to newer records.
        \end{itemize} 
    \item \textbf{Task}: a representation of the possible delegation tasks in the scenario, containing the Feature Models associated with the performance of this task (e.g. The ability to serve drinks if the task is bartending). A weight is given to each Feature corresponding to its importance in the task. The various weights are normalized so that their sum is 1.0.
\end{itemize} 

\begin{figure}[hbt]
    \centering
    \includegraphics[width=200px]{TrustMemoryDiagram.png}
    \caption{Memory Architecture (represented in UML)}
    \label{fig:MemoryArchitecture}
\end{figure}


\subsubsection{Ontology} 
The model was structured in a way to easily adapt an ontology to a particular scenario, which can be defined by 4 main components of the trust model:

\subsubsection{Trust Calculation}
Taking a Trustor $X$, a Trustee $Y$ and a delegated task $\tau$, Trust can then be calculated by taking the Trustee's Trust Features $F_y$, the Task's Feature Models $F_\tau$ and checking which they have in common $F_y \cap F_\tau$, represented as $F_{y\cap\tau}$~. Remember that Trust Features are uniquely identified by a Feature Model. So after getting $F_{y\cap\tau}$ we can apply a linear function to each of the features in $F_{y\cap\tau}$, where for each element $F_n$ we multiply the trustee's feature's belief value $B(F_n)$ with the weight of the feature for the task $W(F_n)$

 applying a simple linear function to the features that are both contained in the trustee and the task are shared taking each of the task's Feature Models and check if the trustee  through a simple linear function
 the relevant Feature Models for the task and 

% Agent contains Trustees, which are representations of the other agents.
% A trustee has a set of features that the trustor has assigned as representative of it's trust.
% A feature belongs to a certain category, which in most scenarios, would be Ability and Willingness
% A feature has a belief value, that is calculated from a set of belief sources
% A belief source can either be Direct Contact, Reputation or Bias
% Each belief source has three values, a belief value, a certainty value, and a time value.
% Belief value is a normalized number that describes the trustee's evaluation on that feature
% Certainty describes how well the trustee was evaluated, in Reputation for instance, this might represent how well we trust in the reporter, and in Direct Contact how well the trustor observed the trustee performing said feature.
% Time is just a record of when was this belief source recorded, as older records might have a lower impact in the overall belief value score, compared to newer records.
% Perceptions correspond to the inputs from the environment that are inserted into the model as belief values to associated features.
% 

\subsection{Perception}

\subsection{Action Suggestion}
% This model component describes a mapping of trust increasing oppurtunities to actions that are performed depending on what features are lacking in the trustee for a particular trustor.

% A perception is mapped to relevant actions, and actions are allocated depending on time available and what is the lowest scoring relevant feature.

