\section{Introduction}
\label{sec:Introduction}

% TODO - Insert motivation on the topic:
% - while a lot of reasearch has been done in modeling trust and in automation, little  research trust in HAI has been found
% - trust is an important component of the social fabric, and essential if we want to have collaboration
% - mention that people tend to "anthropomorphize" computers, applying social rules in computer interactions so trust becomes an interesting concern as shown in \cite{Reeves1998a} and \cite{Fogg1997}
% - First paragraph - Say that trust is a fundamental part of social interaction, and as agents try to simulate this,and by this reason there's been quite some focues in computational trust during recent years, (cite reviews). Give applications of the trust models (specially on e-commerce)
% - Second paragraph - Briefly say what has been done (not too much, leave for related work) and comment on what is lacking.
% - Third paragraph - Introduce your solution, and why it's going to improve the field. Relate to the comments mentioned previously
% - Fourth paragraph - Finally give the structure of the paper (and it's scope? check if hay is needed...)
Trust has been described in Psychology as being one of the most important components of interpersonal relationships \cite{Simpson2007}. It is undeniable the need of Trust to promote cooperation and collaboration between two parties, either in deciding who should one collaborate with, or even on what exactly do we trust the other party with. 


As \ac{AI} Research gravitates towards the development of Intelligent Agent Systems \cite{Russell2009a}, out of which a focal concern is the performance of collaborative tasks\cite{Grosz1996, Allen2002, Allen2007}, as well as addressing the problems of interaction between humans and agents\cite{Bradshaw2011}, one would consider that Trust should be one of the main focuses of \ac{HAI}. Since the start of automated machinery, one of the main issues was how to properly manage trust on machines, in order to avoid over or under reliance\cite{Lee2004}. Reeves and Nass have shown that people apply social rules to \ac{HCI}, and this can logically be extended to \ac{HAI}\cite{Reeves1998a}. So as agents evolve to better perform collaborative tasks with humans autonomously, which demands at least some amount of social interaction, the active agent must seek out to improve the trust relationship it has with the user \cite{Lashkari1994}. And while the amount of literature has been increasing, we found it surprising that not enough work has been done in \ac{HAI} focusing on Trust, other than on design issues\cite{Bickmore2005} and the sub-field of \ac{HRI}\cite{Goodrich2007, VandenBrule2014}, specially when so much has been done regarding Trust in Automation \cite{Lee1992, Jones1997, Lee2004}. This reveals that while the area has so much potential, the level of understanding is still very shallow, only deeply focused in specific areas.

\ac{MAS} Trust and Reputation modelling is one of the areas that has been having a great increase of interest lately, specially ever since the advent of \ac{P2P} e-commerce in platforms like \textit{E-bay}\cite{eBay2002}, where tools and solutions to ensure trust were needed for a new reality of a mass amount of anonymous entities constantly entering and exiting the environment and performing trading transactions through an open space. However almost all research focuses purely on the creation and maintenance of the internal trust model structure of the agent, normally with just the purpose of ranking other agents, through the use of statistical and game theoretical based methods. This makes it difficult to create a model that is easy to understand, analyse and, most importantly, describe its evaluative reasoning in a human understandable manner. The introduction of cognitive models by Castelfranchi and Falcone \cite{Castelfranchi1998} tries to solve that problem, mapping the trust model to the agent's mental state, composed by beliefs and goals, very akin to existing cognitive agent architectures like BDI\cite{Rao1995}. Some systems, like Repage\cite{Sabater2006}, created implementations of this new paradigm of trust modelling, until then most of the models were purely theoretical. Nevertheless, there is a gap in this area of research that we wish to address with our work, and that is the lack of an implementation for an action suggester based on the agent's trust model to improve the strength of our beliefs in the model and to improve trust in our agent. While one could argue that this is the responsibility of the decision making or planner component of the agent, we believe that a dedicated module will ease the complexity of decision by making it more modular, and also allowing for a greater degree of integration with the trust model of the agent. To our knowledge, no attempts have been done towards this goal, so we propose to develop two agent modules: firstly, one capable of creating a cognitive model representing the mental state of the user's trust in the agent, using Repage's architecture, and secondly, another to suggest what actions should be used to improve trust on the agent. We will ascertain this project's objectives by integrating the modules in an agent implementation (currently finishing development in our research group, \acs{GAIPS}\footnote{\ac{GAIPS}: \url{http://gaips.inesc-id.pt/}}) that is capable of acting as one of the players in the \textit{Split or Steal} scenario, introduced in the British game show \textit{Golden Balls}\cite{Wikipedia.Golden.Balls}, the scenario is further described in Section \ref{subsec:Evaluation:Split or Steal}. 

We hope that this project will improve agent decision making more interesting, provide some insight on how actions affect trust and budge the field a bit in this unexplored direction.


\subsection{Goals}
\label{subsec:Goals}
% Clearly state what must be done to consider this thesis a success (may prove difficult as it's too abstract) maybe focus on objective marks on trust improvement of the subject.
% Need to go fetch what is the trust measurement questionaires.

% we aim to create a generic agent model that combines the already extensive reasearch in trust modeling in MAS systems with a decision making module with strategies to improve trust in the agent.
% we want to focus on human to agent interaction. so improve and model human trust.
% find out which parameters and features can be explicitly chosen to human modeling.
In sum, this project's purpose will be to:
\begin{itemize}
	\item Create a cognitive trust model capable of representing human trust towards the agent using the Repage architecture;
	\item Develop an action suggestion module that aims to provide actions that improve trust in the agent (or a least the beliefs on this trust) for the Repage;
	\item Calibrate the modules through user testing in the \textit{Split or Steal} scenario.
\end{itemize}


In the rest of the document we will present a brief summary of the main concepts used in this project in Section  \ref{sec:Background}. Then in Section \ref{sec:Related Work} we will discuss some of the work done modelling trust in the field of \ac{MAS} and measuring trust in \ac{HRI} applications. In Section \ref{sec:Solution} a description of our solution architecture will be presented, followed by our plans and schedules in Section \ref{sec:Planning}. Finally in Section \ref{sec:Evaluation} we will describe how we will evaluate the objectives.


