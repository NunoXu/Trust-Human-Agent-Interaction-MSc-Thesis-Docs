Automatically generated by Mendeley Desktop 1.15.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Rapoport1988,
abstract = {The fastener design for the transfer of concentrated transverse (out of plane, pull-out) loads to random glass fiber reinforced thermoset polymers was investigated. The elastic material properties, void content,a nd glass content of the composite were determined and a finite element model was used to analyze and compare the performance of the various washer designs for reducing the stress and strain levels near the edge of the washer at a bolted joint. Experimental studies were conducted to verify the finite element model.},
author = {Rapoport, A.},
doi = {10.1177/0022002788032003003},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/Journal of Conflict Resolution-1988-Rapoport-457-72.pdf:pdf},
journal = {Journal of Conflict Resolution},
month = {sep},
number = {3},
pages = {457--472},
title = {{Experiments with N-Person Social Traps I: Prisoner's Dilemma, Weak Prisoner's Dilemma, Volunteer's Dilemma, and Largest Number}},
url = {http://jcr.sagepub.com/cgi/doi/10.1177/0022002788032003003},
volume = {32},
year = {1988}
}
@misc{Hoc2000,
annote = {Reference as noting the importance of trust in human-machine interaction},
author = {Hoc, Jean-Michel},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/Hoc 2000a.pdf:pdf},
pages = {833 -- 843},
title = {{From human - machine interaction to human - machine cooperation}},
year = {2000}
}
@article{Katagiri2001,
annote = {Tries to perform social persuasion but with no conclusive results.

Not very interesting},
author = {Katagiri, Yasuhiro and Takahashi, Toru and Takeuchi, Yugo},
doi = {10.1.1.23.6671},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/10.1.1.23.6671.pdf:pdf},
journal = {Interactions},
title = {{Social Persuasion in Human-Agent Interaction}},
year = {2001}
}
@article{Sabater-Mir2007,
abstract = {Interest for computational trust and reputation models is on the rise. One of the most important aspects of these models is how they deal with information received from other individuals. More generally, the critical choice is how to represent and how to aggregate social evaluations. In this article, we make an analysis of the current approaches of representation and aggregation of social evaluations under the guidelines of a set of basic requirements. Then we present two different proposals of dealing with uncertainty in the context of the Repage system [J. Sabater, M. Paolucci, R. Conte, Repage: Reputation and image among limited autonomous partners, Journal of Artificial Societies and Social Simulation 9 (2). URL http://jasss.soc.surrey.ac.uk/9/2/3.html], a computational module for management of reputational information based on a cognitive model of imAGE, REPutation and their interplay already developed by the authors. We finally discuss these two proposals in the context of several examples. ?? 2007 Elsevier Inc. All rights reserved.},
author = {Sabater-Mir, Jordi and Paolucci, Mario},
doi = {10.1016/j.ijar.2006.12.013},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/1-s2.0-S0888613X07000059-main.pdf:pdf},
isbn = {0888-613X},
issn = {0888613X},
journal = {International Journal of Approximate Reasoning},
keywords = {Aggregation mechanisms,Reputation,Trust},
number = {3},
pages = {458--483},
title = {{On representation and aggregation of social evaluations in computational trust and reputation models}},
volume = {46},
year = {2007}
}
@article{Rousseau1998,
abstract = {none},
author = {Rousseau, Denise and Sitkin, Sim and Burt, Ronald and Camerer, Colin},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/ppaper{\_}47663385940298{\_}AMR.trust{\_}1998.pdf:pdf},
issn = {0042-0980},
journal = {Academy of Management Review},
number = {3},
pages = {393--404},
title = {{Not so different after all: A cross-discipline view of trust.}},
volume = {23},
year = {1998}
}
@article{Lee1992,
abstract = {As automated controllers supplant human intervention in controlling complex systems, the operators' role often changes from that of an active controller to that of a supervisory controller. Acting as supervisors, operators can choose between automatic and manual control. Improperly allocating function between automatic and manual control can have negative consequences for the performance of a system. Previous research suggests that the decision to perform the job manually or automatically depends, in part, upon the trust the operators invest in the automatic controllers. This paper reports an experiment to characterize the changes in operators' trust during an interaction with a semi-automatic pasteurization plant, and investigates the relationship between changes in operators' control strategies and trust. A regression model identifies the causes of changes in trust, and a 'trust transfer function' is developed using time series analysis to describe the dynamics of trust. Based on a detailed analysis of operators' strategies in response to system faults we suggest a model for the choice between manual and automatic control, based on trust in automatic controllers and self-confidence in the ability to control the system manually.},
author = {Lee, J and Moray, N},
doi = {10.1080/00140139208967392},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/leemoray92.pdf:pdf},
issn = {0014-0139},
journal = {Ergonomics},
month = {oct},
number = {10},
pages = {1243--70},
pmid = {1516577},
title = {{Trust, control strategies and allocation of function in human-machine systems.}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00140139208967392 http://www.ncbi.nlm.nih.gov/pubmed/1516577},
volume = {35},
year = {1992}
}
@book{Reeves1998a,
annote = {Cite about how people see computers as personified entities, even if unconciously},
author = {Reeves, Byron and Nass, Clifford},
booktitle = {Cambridge University Press},
doi = {10.1109/MSPEC.1997.576013},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/the media equation.pdf:pdf},
isbn = {1-57586-053-8},
issn = {0018-9235},
month = {mar},
pages = {19--36},
title = {{The Media Equation: How People Treat Computers, Television, and New Media Like Real People and Places}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=576013},
year = {1998}
}
@book{Castelfranchi2010,
address = {Chichester, UK},
author = {Castelfranchi, Cristiano and Falcone, Rino},
booktitle = {Wiley},
doi = {10.1002/9780470519851},
edition = {1},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/TrustTheoryASocioCognitiveandComputationalModel.pdf:pdf},
isbn = {9780470519851},
issn = {0717-6163},
month = {mar},
pages = {1--387},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Trust Theory}},
url = {http://doi.wiley.com/10.1002/9780470519851},
year = {2010}
}
@article{Pinyol2013,
abstract = {In open environments, agents depend on reputation and trust mechanisms to evaluate the behavior of potential partners. The scientific research in this field has considerably increased, and in fact, reputation and trust mechanisms have been already considered a key elements in the design of multi-agent systems. In this paper we provide a survey that, far from being exhaustive, intends to show the most representative models that currently exist in the literature. For this enterprise we consider several dimensions of analysis that appeared in three existing surveys, and provide new dimensions that can be complementary to the existing ones and that have not been treated directly. Moreover, besides showing the original classification that each one of the surveys provide, we also classify models that where not taken into account by the original surveys. The paper illustrates the proliferation in the past few years of models that follow a more cognitive approach, in which trust and reputation representation as mental attitudes is as important as the final values of trust and reputation. Furthermore, we provide an objective definition of trust, based on Castelfranchi's idea that trust implies a decision to rely on someone. © 2011 Springer Science+Business Media B.V.},
annote = {Makes a revision of the most representative models on the literature.

Of all the models, only the following were classified as cognitive:

Castelfranchi et al., Repage, BDI + Repage and ForTrust},
author = {Pinyol, Isaac and Sabater-Mir, Jordi},
doi = {10.1007/s10462-011-9277-z},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/2013-AIRb.pdf:pdf},
isbn = {0269-2821},
issn = {0269-2821},
journal = {Artificial Intelligence Review},
keywords = {Cognitive trust and reputation,Computational trust and reputation models,Multiagent systems},
month = {jun},
number = {1},
pages = {1--25},
title = {{Computational trust and reputation models for open multi-agent systems: a review}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84878107220{\&}partnerID=tZOtx3y1 http://link.springer.com/10.1007/s10462-011-9277-z},
volume = {40},
year = {2013}
}
@article{Tseng1999,
abstract = {For most of computing’s brief history, people have held computers in high regard. A quick review of the popular culture from the past few decades reflects people’s general confidence in computing systems. In cinema and lit- erature, computers are often portrayed as infallible sidekicks in the service of humanity. In the consumer realm, computer-based information and services have been marketed as bet- ter, more reliable, and more credible sources of information than humans. Consider, for example, computerized weather prediction, computerized automotive analysis, and so- called computer dating. In these and other areas, the public has generally been led to believe that if a computer said it or produced it, it was believable},
author = {Tseng, Shawn and Fogg, B. J.},
doi = {10.1145/301353.301402},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/p39-tseng.pdf:pdf},
isbn = {00010782},
issn = {00010782},
journal = {Communications of the ACM},
number = {5},
pages = {39--44},
title = {{Credibility and computing technology}},
volume = {42},
year = {1999}
}
@article{Ososky2013,
author = {Ososky, Scott and Schuster, David and Phillips, Elizabeth and Jentsch, Florian},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/5784-24579-1-PB.pdf:pdf},
isbn = {9781577356042},
journal = {AAAI Spring Symposium},
keywords = {AAAI Technical Report SS-13-07},
pages = {60--65},
title = {{Building Appropriate Trust in Human-Robot Teams Mental Models : Building Blocks of Trust}},
year = {2013}
}
@article{Noorian2010,
abstract = {We introduce a multidimensional framework for classifying and comparing trust and reputation (T{\&}R) systems. The framework dimensions encompass both hard and soft features of such systems including different witness location approaches, various reputation calculation engines, variety of information sources and rating systems which are categorised as hard features, and also basic reputation measurement parameters, context diversity checking, reliability and honesty assessment and adaptability which are referred to as soft features. Specifically, the framework dimensions answer questions related to major characteristics of T{\&}R systems including those parameters from the real world that should be imitated in a virtual environment. The proposed framework can serve as a basis to understand the current state of the art in the area of computational trust and reputation and also help in designing suitable control mechanisms for online communities. In addition, we haveprovided a critical analysis of some of the existing techniques in the literature compared within the context of the proposed framework dimensions.},
author = {Noorian, Zeinab and Ulieru, Mihaela},
doi = {10.4067/S0718-18762010000200007},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/art07.pdf:pdf},
isbn = {0718-1876},
issn = {0718-1876},
journal = {Journal of theoretical and applied electronic commerce research},
keywords = {Computational trust,Online communities,Reliability assessment,Reputation formalization,T{\&}R taxonomy},
month = {aug},
number = {2},
pages = {97--117},
title = {{The State of the Art in Trust and Reputation Systems: A Framework for Comparison}},
url = {http://www.scielo.cl/scielo.php?script=sci{\_}arttext{\&}pid=S0718-18762010000200007{\&}lng=en{\&}nrm=iso{\&}tlng=en},
volume = {5},
year = {2010}
}
@article{Singh2011,
author = {Singh, MP},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/10.1.1.300.8910.pdf:pdf},
isbn = {978-0-9826571-5-7},
journal = {The 10th International Conference on Autonomous {\ldots}},
keywords = {commitments,service-oriented computing,trust},
pages = {863--870},
title = {{Trust as dependence: a logical approach}},
url = {http://dl.acm.org/citation.cfm?id=2031741},
year = {2011}
}
@article{Parasuraman2000,
author = {Parasuraman, R and Sheridan, T.B.},
doi = {10.1109/3468.844354},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/parasuraman.pdf:pdf},
isbn = {1083-4427},
issn = {1083-4427},
journal = {Systems, Man and {\ldots}},
number = {3},
pages = {286--297},
pmid = {11760769},
title = {{A model for types and levels of human interaction with automation}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=844354},
volume = {30},
year = {2000}
}
@article{Artz2007,
author = {Artz, Donovan and Gil, Yolanda},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/112-128-3-PB.pdf:pdf},
keywords = {policies,reputation,trust,web of trust},
title = {{A survey of trust in computer science and the Semantic Web}},
year = {2007}
}
@article{Esfandiari2001,
abstract = {We need models of trust to facilitate cooperation in multi-agent systems, where agents, human and artificial, do not know each other beforehand. This paper lists and proposes simple mechanisms for trust acquisition based on a very basic and general definition of trust, making no assumptions on the internal cognitive models of the involved agents. We also show how trust acquired one-on-one can be propagated in a social network of agents.},
annote = {Small paper describing the different ways trust can be acquired. Is also described in other papers and seems a bit irrelevant. Cited over 100 though, so maybe just reference for the work done?

Reference to the ways of acquiring trust.},
author = {Esfandiari, Babak and Chandrasekharan, Sanjay},
doi = {10.1.1.11.4683},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/download.pdf:pdf},
journal = {Proceedings of the Fourth Workshop on Deception Fraud and Trust in Agent Societies Montreal Canada},
keywords = {Mechanisms for Trust},
number = {19th of June},
pages = {27--34},
title = {{On how agents make friends: Mechanisms for trust acquisition}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.11.4683{\&}amp;rep=rep1{\&}amp;type=pdf},
volume = {222},
year = {2001}
}
@article{Steinfeld2006,
author = {Steinfeld, A and Fong, T and Kaber, D},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/steinfeld{\_}aaron{\_}m{\_}2006{\_}1.pdf:pdf},
isbn = {1595932941},
journal = {{\ldots}  on Human-robot  {\ldots}},
keywords = {human-robot interaction,metrics,unmanned ground vehicles},
title = {{Common metrics for human-robot interaction}},
url = {http://dl.acm.org/citation.cfm?id=1121249},
year = {2006}
}
@article{VandenBrule2014,
abstract = {An important aspect of a robot’s social be- havior is to convey the right amount of trustworthi- ness. Task performance has shown to be an important source for trustworthiness judgments. Here, we argue that factors such as a robot’s behavioral style, can play an important role as well. Our approach to studying the effects of a robot’s performance and behavioral style on human trust involves experiments with simulated robots in Video Human-Robot Interaction (VHRI) and Immersive Virtual Environments (IVE). Although VHRI and IVE settings cannot substi- tute for the genuine interaction with a real robot, they can provide useful complementary approaches to exper- imental research in social Human Robot Interaction. VHRI enables rapid prototyping of robot behaviors. Simulating Human-Robot Interaction in IVEs can be a useful tool for measuring human responses to robots and help avoid the many constraints caused by real- world hardware. However, there are also difficulties with the generalization of results from one setting (e.g.,VHRI) to another (e.g. IVE or the real world), which we dis- cuss. In this paper, we use animated robot avatars in VHRI to rapidly identify robot behavioral styles that affect human trust assessment of the robot. In a subse- quent study, we use an IVE tomeasure behavioral inter- action between humans and an animated robot avatar equipped with behaviors from the VHRI experiment. Our findings reconfirm that a robot’s task performance R. van den Brule · W.F.G. Haselager Donders Institute for Brain, Cognition and Behaviour, Rad- boud University Nijmegen, Montessorilaan 3, 6525 HR Nij- megen E-mail: r.vandenbrule@donders.ru.nl R. van den Brule · R. Dotsch · G. Bijlstra · D.H.J. Wigboldus Behavioural Science Institute, Radboud University Nijmegen, Montessorilaan 3, 6525 HR Nijmegen influences its trustworthiness, but the effect of the be- havioral style identified in the VHRI study did not in- fluence the robot’s trustworthiness in the IVE study.},
author = {van den Brule, R. and Dotsch, R. and Bijlstra, G. and Wigboldus, D. H. J. and Haselager, W. F. G.},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/Van-den-brule-et-al-2014.pdf:pdf},
journal = {International Journal of Social Robotics},
keywords = {immersive virtual environments,social robotics,trust,video stimuli},
title = {{Do Robot Performance and Behavioral Style affect Human Trust ?}},
year = {2014}
}
@article{Castelfranchi1998,
abstract = {After arguing about the crucial importance of trust for Agents and MAS, we provide a definition of trust both as a mental state and as a social attitude and relation. We present the mental ingredients of trust: its specific beliefs and goals, with special attention to evaluations and expectations. We show the relation between trust and the mental background of delegation. We explain why trust is a bet, and implies some risks, and analyse the most basic forms of non-social trust (reliance on objects and tools) to arrive at the more complex forms of social trust, based on morality and reputation. Finally we present a principled quantification of trust, based on its cognitive ingredients. And use this “degree of trust” as the basis for a rational decision to delegate or not to another agent. The paper is intended to contribute both to the conceptual analysis and to the practical use of trust in social theory and MAS},
author = {Castelfranchi, C and Falcone, R},
doi = {10.1109/ICMAS.1998.699034},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/2001{\_}Castelfranchi.pdf:pdf},
isbn = {0-8186-8500-X},
issn = {0-8186-8500-X},
journal = {Proceedings of the International Conference on Multi Agent Systems},
pages = {72--79},
title = {{Principles of trust for MAS: cognitive anatomy, social importance, and quantification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=699034},
year = {1998}
}
@article{Antos2011,
author = {Antos, Dimitrios and Melo, Celso De and Gratch, Jonathan and Grosz, Barbara J},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/antosDeMelo11-aaai.pdf:pdf},
isbn = {9781577355083},
journal = {Aaai},
keywords = {Multidisciplinary Topics},
pages = {772--778},
title = {{The Influence of Emotion Expression on Perceptions of Trustworthiness in Negotiation.}},
url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/viewPDFInterstitial/3438/3951},
year = {2011}
}
@article{Pinto2008,
abstract = {No trabalho},
author = {Pinto, Santos},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/Andrio-Pinto-WTDIA-2008.pdf:pdf},
keywords = {behavioral simulation,cooperation,group interation  simulation,multi-agent systems,reputation},
pages = {99},
title = {{Simula{\c{c}}{\~{a}}o e Avalia{\c{c}}{\~{a}}o de Comportamentos em Sistemas Multi-Agentes baseados em Modelos de Reputa{\c{c}}{\~{a}}o e Intera{\c{c}}{\~{a}}o}},
url = {http://www.unissinos.br},
volume = {2},
year = {2008}
}
@article{Simpson2007a,
abstract = {Trust lies at the foundation of nearly all major theories of interpersonal relationships. Despite its great theoretical importance, a limited amount of research has examined how and why trust develops, is maintained, and occasionally unravels in relationships. Following a brief overview of theoretical and empirical milestones in the interpersonal-trust literature, an integrative process model of trust in dyadic relationships is presented},
author = {Simpson, Jeffry A.},
doi = {10.1111/j.1467-8721.2007.00517.x},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/Simpson{\_}{\_}Current{\_}Directions{\_}{\_}2007{\_}.pdf:pdf},
isbn = {0-205-33144-0},
issn = {0963-7214},
journal = {Current Directions in Psychological Science},
keywords = {felt security,interdependence,strain tests,trust},
month = {oct},
number = {5},
pages = {264--268},
title = {{Psychological Foundations of Trust}},
url = {http://cdp.sagepub.com/lookup/doi/10.1111/j.1467-8721.2007.00517.x},
volume = {16},
year = {2007}
}
@article{Bickmore2005,
author = {Bickmore, Timothy W and Picard, ROSALIND W.},
doi = {10.1145/1067860.1067867},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/RAG 2005 - Human-Computer Relationships.pdf:pdf},
issn = {10730516},
journal = {ACM Transactions on Computer-Human},
number = {2},
pages = {293--327},
title = {{Establishing and Maintaining Long-Term Human-Computer Relationships}},
volume = {12},
year = {2005}
}
@incollection{Gambetta1988,
annote = {Provides most cited defiinition of trust, but in contrast with the definition provided by Castelfranchi

While this one is more statistical/numeric based, Castelfranchi provides one more logical based.},
author = {Gambetta, Diego},
booktitle = {Trust: Making and Breaking Cooperative Relations},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/download (1).pdf:pdf},
pages = {213--237},
publisher = {Blackwell},
title = {{Can We Trust Trust?}},
url = {http://sieci.pjwstk.edu.pl/media/bibl/[Gambetta]{\_}[Can We]{\_}[Trust]{\_}[1988].pdf},
year = {1988}
}
@inproceedings{Walter2009,
abstract = {We propose a novel trust metric for social networks which is suitable for application in recommender systems. It is personalised and dynamic and allows to compute the indirect trust between two agents which are not neighbours based on the direct trust between agents that are neighbours. In analogy to some personalised versions of PageRank, this metric makes use of the concept of feedback centrality and overcomes some of the limitations of other trust metrics.In particular, it does not neglect cycles and other patterns characterising social networks, as some other algorithms do. In order to apply the metric to recommender systems, we propose a way to make trust dynamic over time. We show by means of analytical approximations and computer simulations that the metric has the desired properties. Finally, we carry out an empirical validation on a dataset crawled from an Internet community and compare the performance of a recommender system using our metric to one using collaborative filtering.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {0902.1475},
author = {Walter, Frank E. and Battiston, Stefano and Schweitzer, Frank},
booktitle = {Proceedings of the third ACM conference on Recommender systems - RecSys '09},
doi = {10.1145/1639714.1639747},
eprint = {0902.1475},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/0902.1475.pdf:pdf},
isbn = {9781605584355},
keywords = {information overload,personalisation,recommender,social networks,systems,trust},
pages = {197},
publisher = {ACM Press},
title = {{Personalised and dynamic trust in social networks}},
url = {http://arxiv.org/abs/0902.1475$\backslash$nhttp://portal.acm.org/citation.cfm?doid=1639714.1639747 http://portal.acm.org/citation.cfm?doid=1639714.1639747},
year = {2009}
}
@article{Allen2002,
abstract = {This paper describes the design of computer agents that can collaborate with humans in plan- ning. It includes an explicit problem solving level that mediates between the human-computer in- teraction and the underlying automated plan rea- soners. We also describe a plan reasoning system that allows for incremental, interactive develop- ment of plans to support collaborative planning. This model has been used in a prototype system in which untrained users can successfully de- velop plans in a simple evacuation-planning do- main.},
annote = {Proposes a problem solving module that introduces collaboration between the agent and the user.

Reference for fields of study on agents.},
author = {Allen, James and Ferguson, George},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/allen-ferguson-nasa2002.pdf:pdf},
journal = {International NASA Workshop on Planning},
pages = {1--10},
title = {{Human-machine collaborative planning}},
url = {https://www.cs.rochester.edu/research/cisd/pubs/2002/allen-ferguson-nasa2002.pdf},
year = {2002}
}
@article{Bradshaw2011,
author = {Bradshaw, Jeffrey M and Feltovich, Paul and Johnson, Matthew},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/100823-HAI Chapter.pdf:pdf},
isbn = {978-1-4094-8660-2},
journal = {Handbook of HumanMachine Interaction},
pages = {293--302},
title = {{Human-Agent Interaction}},
url = {http://books.google.com/books?hl=en{\&}amp;lr={\&}amp;id=4opHlu05SNIC{\&}amp;oi=fnd{\&}amp;pg=PA283{\&}amp;dq=Human-agent+interaction{\&}amp;ots=vxrpDdLbSa{\&}amp;sig=07dujtzGjIcBLlZ6FVH33HjrWos},
year = {2011}
}
@phdthesis{Marsh1994,
abstract = {Trust is a judgement of unquestionable utility as humans we use it every day of our lives. However, trust has suffered from an imperfect understanding, a plethora of definitions, and informal use in the literature and in everyday life. It is common to say I trust you, but what does that mean? This thesis provides a clarification of trust. We present a formalism for trust which provides us with a tool for precise discussion. The formalism is implementable: it can be embedded in an artificial agent, enabling the agent to make trust-based decisions. Its applicability in the domain of Distributed Artificial Intelligence (DAI) is raised. The thesis presents a testbed populated by simple trusting agents which substantiates the utility of the formalism. The formalism provides a step in the direction of a proper understanding and definition of human trust. A contribution of the thesis is its detailed exploration of the possibilities of future work in the area.},
annote = {Gives an early definition of reputation},
author = {Marsh, Stephen Paul},
booktitle = {Computing Science and Mathematics eTheses},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/TR133.pdf:pdf},
month = {apr},
pages = {184},
title = {{Formalising Trust as a Computational Concept}},
volume = {NA},
year = {1994}
}
@article{Ramchurn2003,
annote = {Paper preceeding:
Devising a trust model for multi-agent interactions useing confidence and pain by the same authors, check other paper.},
author = {Ramchurn, D. Ramchurn and Jennings, Nicholas R. and Sierra, Carles and Godo, Llu{\'{\i}}s},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/aamas-trust-ws.pdf:pdf},
title = {{A Computational Trust Model for Multi-Agent Interactions based on Confidence and Reputation}},
year = {2003}
}
@article{Carter2002,
abstract = {We propose that through the formalization of concepts related to trust, a more accurate model of trust can be implemented. This paper presents a new model of trust that is based on the formalization of reputation. A multidisciplinary approach is taken to understanding the nature of trust and its relation to reputation. Through this approach, a practical definition of reputation is adopted from sociological contexts and a model of reputation is designed and presented. Reputation is defined as role fulfillment. To formalize reputation, it is necessary to formalize the expectations placed upon an agent within a particular multi-agent system (MAS). In this case, the agents are part of an informationsharing society. Five roles are defined along with the ways in which these roles are objectively fulfilled. Through the measurement of role fulfillment, a vector representing reputation can be developed. This vector embodies the magnitude of the reputation and describes the patterns of behavior associated with the direction of the vector. Experiments are conducted to verify the sensibility of the proposed models for role fulfillment and overall reputation. The simulation results show that the roles, defined for building reputation in an information-sharing MAS environment, react to different agent and user actions in a manner consistent with the formal definitions.},
annote = {Offers a formalization of Reputation. Defines it as one of the dimensions of trust.},
author = {Carter, Jonathan and Bitting, Elijah and Ghorbani, Ali A.},
doi = {10.1111/1467-8640.t01-1-00201},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/Carter{\_}et{\_}al-2002-Computational{\_}Intelligence.pdf:pdf},
isbn = {1011161665},
issn = {0824-7935},
journal = {Computational Intelligence},
keywords = {agent,information sharing,multi-agent systems,reputation,trust},
month = {nov},
number = {4},
pages = {515--534},
title = {{Reputation Formalization for an Information-Sharing Multi-Agent System}},
url = {http://doi.wiley.com/10.1111/1467-8640.t01-1-00201},
volume = {18},
year = {2002}
}
@article{Sabater2002,
abstract = {The use of previous direct interactions is probably the best way to calculate a reputation but, unfortunately this infor- mation is not always available. This is especially true in large multi-agent systems where interaction is scarce. In this paper we present a reputation system that takes advan- tage, among other things, of social relations between agents to overcome this problem.},
annote = {Reputation system focused on social relations.

Regret Model Paper.},
author = {Sabater, Jordi and Sierra, Carles},
doi = {10.1145/544852.544854},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/2002-AAMASa.pdf:pdf},
isbn = {1581134800},
journal = {Proceedings of the first international joint conference on Autonomous agents and multiagent systems part 1 - AAMAS '02},
pages = {475},
title = {{Reputation and social network analysis in multi-agent systems}},
url = {http://portal.acm.org/citation.cfm?doid=544741.544854},
year = {2002}
}
@article{Pinyol2009,
annote = {BDI + Repage},
author = {Pinyol, Isaac},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/main.pdf:pdf},
journal = {Doctoral Mentoring Program},
keywords = {agents,utation-based decisions for cognitive},
number = {Aamas},
title = {{Reputation-Based Decisions for Cognitive Agents (Thesis Abstract)}},
url = {http://ifaamas.org/Proceedings/aamas09/pdf/07{\_}Doctoral/Doct{\_}08.pdf},
year = {2009}
}
@article{Ganesan1997,
abstract = {Previous research has found that trust is positively related to commitment in buyer-seller relationships. However, the validity of this finding is questionable because trust has been operationalized in many different ways. For example, prior research has not distinguished among levels of trust (interpersonal or organizational trust) and dimensions or motives of trust (credibility or benevolence). In this study, we distinguish among the levels and dimensions of trust. The results indicate that trust in a sales representative (interpersonal credibility) is more strongly related to commitment than trust in an organization (organizational credibility). In contrast, trust based on organizational benevolence is a stronger predictor of commitment than interpersonal benevolence.},
archivePrefix = {arXiv},
arxivId = {arXiv:astro-ph/0005074v1},
author = {Ganesan, Shankar and Hess, Ron},
doi = {10.1023/A:1007955514781},
eprint = {0005074v1},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/art{\%}3A10.1023{\%}2FA{\%}3A1007955514781.pdf:pdf},
isbn = {10.1023/A:1007955514781},
issn = {0923-0645, 1573-059X},
journal = {Marketing Letters},
keywords = {1994,along with commitment are,benevolence,commitment,credibility,despite the,for the success of,have argued that trust,in marketing,in marketing and organizational,interfirm alliances,interpersonal trust,is generating increased interest,keys,morgan and hunt,organizational trust,studies,the topic of trust,to cooperative behaviors essential},
number = {4},
pages = {439--448},
pmid = {1284},
primaryClass = {arXiv:astro-ph},
title = {{Dimensions and Levels of Trust: Implications for Commitment to a Relationship}},
url = {http://link.springer.com/article/10.1023/A:1007955514781$\backslash$nhttp://link.springer.com/article/10.1023/A:1007955514781$\backslash$nhttp://link.springer.com/content/pdf/10.1023/A:1007955514781.pdf},
volume = {8},
year = {1997}
}
@article{Castelfranchi2001a,
abstract = {After arguing about the crucial importance of trust for Agents and MAS, we provide a definition of trust both as a mental state and as a social attitude and relation. We present the mental ingredients of trust: its specific beliefs and goals, with special attention to evaluations and expectations. We show the relation between trust and the mental background of delegation. We explain why trust is a bet, and implies some risks, and analyse the more complex forms of social trust, based on a theory of mind and in particular on morality, reputation and disposition, and authority (three party trust). We explain why promises, contracts, authorities can increase our trust by modifying our mental representations. We present a principled quantification of trust, based on its cognitive ingredients, and use this "degree of trust" as the basis for a rational decision to delegate or not to another agent. We explain when trust is rational, and why it is not an irrational decision by definition. We also criticise the economic and game-theoretic view of trust for underestimating the importance of cognitive ingredients of trust and for reducing it to subjective probability and risk. The paper is intended to contribute both to the conceptual analysis and to the practical use of trust in social theory and MAS.},
annote = {A follow-up paper to Castelfranchi,1998},
author = {Castelfranchi, Cristiano and Falcone, Rino},
doi = {10.1007/978-94-017-3614-5{\_}3},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/cast03{\_}1.pdf:pdf},
isbn = {079236919X},
journal = {Trust and deception in virtual societies},
pages = {55--90},
title = {{Social Trust : A Cognitive Approach}},
year = {2001}
}
@article{Lewis1985,
abstract = {Although trust is an underdeveloped concept in sociology, promising theoretical formulations are available in the recent work of Luhmann and Barber. This socio- logica! version complements the psychological and attitudinal conceptualizations of experimental and survey researchers. Trust is seen to include both emotional and cognitive dimensions and to function as a deep assumption underwriting so- cial order. Contemporary examples such as lying, family exchange, monetary atti- tudes, and litigation illustrate the centrality of trust as a sociological reality.},
author = {Lewis, J David and Weigert, Andrew},
doi = {10.1093/sf/63.4.967},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/Social Forces-1985-Lewis-967-85.pdf:pdf},
isbn = {00377732},
issn = {0037-7732},
journal = {Social Forces},
month = {jun},
number = {4},
pages = {967--985},
pmid = {5283486},
title = {{Trust as a Social Reality}},
url = {http://www.jstor.org/stable/2578601$\backslash$nhttp://www.jstor.org/stable/2578601?seq=1{\&}cid=pdf-reference{\#}references{\_}tab{\_}contents$\backslash$nhttp://www.jstor.org/page/info/about/policies/terms.jsp http://sf.oxfordjournals.org/cgi/doi/10.1093/sf/63.4.967},
volume = {63},
year = {1985}
}
@article{Huynh2006,
abstract = {Abstract Trust and reputation are central to effective interactions in open multi-agent systems (MAS) in which agents, that are owned by a variety of stakeholders, continuously enter and leave the system. This openness means existing trust and reputation models cannot readily be used since their performance suffers when there are various (unforseen) changes in the environment. To this end, this paper presents FIRE, a trust and reputation model that integrates a number of information sources to produce a comprehensive assessment of an agents likely performance in open systems. Specifically, FIRE incorporates interaction trust, role-based trust, witness reputation, and certified reputation to provide trust metrics in most circumstances. FIRE is empirically evaluated and is shown to help agents gain better utility (by effectively selecting appropriate interaction partners) than our benchmarks in a variety of agent populations. It is also shown that FIRE is able to effectively respond to changes that occur in an agents environment.},
annote = {AKA Fire},
author = {Huynh, Trung Dong and Jennings, Nicholas R. and Shadbolt, Nigel R.},
doi = {10.1007/s10458-005-6825-4},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/jaamas-dong.pdf:pdf},
isbn = {1387-2532},
issn = {13872532},
journal = {Autonomous Agents and Multi-Agent Systems},
keywords = {Multi-agent systems,Reputation,Trust},
number = {2},
pages = {119--154},
title = {{An integrated trust and reputation model for open multi-agent systems}},
volume = {13},
year = {2006}
}
@article{Nash1951,
author = {Nash, John},
doi = {10.2307/1969529},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/1969529.pdf:pdf},
issn = {0003486X},
journal = {The Annals of Mathematics},
month = {sep},
number = {2},
pages = {286},
title = {{Non-Cooperative Games}},
url = {http://www.jstor.org/stable/1969529?origin=crossref},
volume = {54},
year = {1951}
}
@article{VandenAssem2012,
abstract = {W e examine cooperative behavior when large sums of money are at stake, using data from the television game show Golden Balls. At the end of each episode, contestants play a variant on the classic prisoner’s dilemma for large and widely ranging stakes averaging over {\$}20,000. Cooperation is surprisingly high for amounts that would normally be considered consequential but look tiny in their current context, what we call a “big peanuts” phenomenon. Utilizing the prior interaction among contestants, we find evidence that people have reciprocal preferences. Surprisingly, there is little support for conditional cooperation in our sample. That is, players do not seem to be more likely to cooperate if their opponent might be expected to cooperate. Further, we replicate earlier findings that males are less cooperative than females, but this gender effect reverses for older contestants because men become increasingly cooperative as their age increases.},
author = {van den Assem, M. J. and van Dolder, D. and Thaler, R. H.},
doi = {10.1287/mnsc.1110.1413},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/SplitorSteal{\_}2012.pdf:pdf},
isbn = {0025-1909},
issn = {0025-1909},
journal = {Management Science},
keywords = {2010,2011,accepted april 17,anchoring,and terrance odean,behavior,by brad barber,context effects,cooperation,cooperative behavior,game show,history,in advance october 7,natural experiment,prisoner,published online in articles,received july 14,reciprocal behavior,reciprocity,s dilemma,social,social preferences,special issue editors,teck ho},
number = {1},
pages = {2--20},
pmid = {70894768},
title = {{Split or steal? Cooperative behavior when the stakes are large}},
volume = {58},
year = {2012}
}
@article{Jones1997,
abstract = {Introduction Computer Supported Cooperative Work (CSCW) (Ellis et al., 1991) is ostensibly concerned with supporting the activities of work groups through the use of computer technology. However, to date, CSCW systems (groupware) have emphasised technological issues of support at the expense of social issues such as relationships, roles and social protocols. We postulate that this situation has arisen because the majority of groupware designers are technologists who have both the experience and tools to develop new and effective hardware and software. Unfortunately they do not have tools or experience to effectively analyse and provide support for social facets of group working. Multidisciplinary development teams may contain group work experts, but common languages and vocabulary for precise communication regarding social and relationship aspects of systems are lacking. Groupware designers and developers also require tools to embed their considerations of social issues in systems and then to analyse those systems and the work of the groups which use them. We have attempted to ameliorate this situation by developing a formal notation of the trust that is present between individuals in collaborative activities. The notation can be used in the representation and consideration of social relationships in the context of CSCW. We suggest that trust is a key factor in the efficacy of both intra-group and inter-group activities, and that it can be formalised and then exploited in the design and analysis of CSCW systems. We call our formal description Trust in order to differentiate it from wider definitions. Potential uses of Trust in a group work context include the following: l it can be used as a tool for the discussion of the design of CSCW systems; l it can be embedded in computer systems to mediate cooperative computer based activities; l it can be used to record and analyze group activity; l it provides a tool for the discussion and clarification of trust, and its role in group activities. The development of the formalism addresses the need for support beyond technical issues for designers involved in the development of multi-user-centered systems.},
author = {Jones, Steve and Marsh, Steve},
doi = {10.1145/264853.264872},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/sigbull.pdf:pdf},
issn = {07366906},
journal = {ACM SIGCHI Bulletin},
keywords = {SIGCHI BULLETIN},
month = {jul},
number = {3},
pages = {36--40},
title = {{Human-computer-human interaction}},
url = {http://portal.acm.org/citation.cfm?doid=264853.264872},
volume = {29},
year = {1997}
}
@article{Granatyr2015,
author = {Granatyr, Jones and Botelho, Vanderson and Lessing, Otto Robert and Scalabrin, Edson Em{\'{\i}}lio and Barth{\`{e}}s, Jean-Paul and Enembreck, Fabr{\'{\i}}cio},
doi = {10.1145/2816826},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/a27-granatyr.pdf:pdf},
issn = {03600300},
journal = {ACM Computing Surveys},
month = {oct},
number = {2},
pages = {1--42},
title = {{Trust and Reputation Models for Multiagent Systems}},
url = {http://dl.acm.org/citation.cfm?doid=2830539.2816826},
volume = {48},
year = {2015}
}
@misc{Carvalho,
author = {Carvalho, Andr{\'{e}}},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/58554-andrecarvalho.pdf:pdf},
keywords = {computational humour,humour,interac-,interactive storytelling,sketch,slapstick,tive comedy},
title = {{Laugh To Me : An Affective Narrative Approach to Computational Humour}}
}
@incollection{Neville2004,
abstract = {Agents that behave maliciously or incompetently are a potential hazard in open distributed e-commerce applications. However human societies have evolved signals and mechanisms based on social interaction to defend against such behaviour. In this paper we present a computational socio-cognitive framework which formalises social theories of trust, reputation, recommendation and learning from direct experience which enables agents to cope with malicious or incompetent actions. The framework integrates these socio-cognitive elements with an agent's economic reasoning resulting in an agent whose behaviour in commercial transactions is influenced by its social interactions, whilst being motivated and constrained by economic considerations. The framework thus provides a comprehensive solution to a number of issues ranging from the evolution of a trust belief from individual experiences and recommendations to the use of those beliefs in market place level decisions. The framework is presented in the context of an artificial market place scenario which is part of a simulation environment currently under development. This is planned for use in evaluation of the framework, and hence can inform design of local decision making algorithms and mechanisms to enforce of social order in agent mediated e-commerce. © Springer-Verlag Berlin Heidelberg 2004.},
author = {Neville, Brendan and Pitt, Jeremy},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-25946-6{\_}24},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/10.1.1.196.32.pdf:pdf},
isbn = {978-3-540-22231-6},
issn = {03029743},
pages = {376--391},
title = {{A Computational Framework for Social Agents in Agent Mediated E-commerce}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-26844439149{\&}partnerID=tZOtx3y1 http://link.springer.com/10.1007/978-3-540-25946-6{\_}24},
volume = {3071},
year = {2004}
}
@article{Burnett2011,
abstract = {Trust is crucial in dynamic multi-agent systems where agents may frequently join and leave, and the structure of the society may often change. In these environments, it may be difficult for agents to form stable trust relationships necessary for confident interactions. Societies may break down when trust between agents is too low to motivate interactions. In such settings, agents should make decisions about who to interact with, given their degree of trust in the available partners. We propose a decision-theoretic model of trust decision making allows controls to be used, as well as trust, to increase confidence in initial interactions. We consider explicit incentives, monitoring and reputation as examples of such controls. We evaluate our approach within a simulated, highly-dynamic multiagent environment, and show how this model supports the making of delegation decisions when trust is low.},
author = {Burnett, Chris and Norman, Timothy J. and Sycara, Katia},
doi = {10.5591/978-1-57735-516-8/IJCAI11-031},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/IJCAI11-031.pdf:pdf},
isbn = {9781577355120},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Agent-Based and Multiagent Systems},
pages = {115--120},
title = {{Trust decision-making in multi-agent systems}},
year = {2011}
}
@article{Goodrich2007,
abstract = {Human-Robot Interaction (HRI) has recently received considerable attention in the academic community, in labs, in technology companies, and through the media. Because of this attention, it is desirable to present a survey of HRI to serve as a tutorial to people outside the field and to promote discussion of a unified vision of HRI within the field. The goal of this review is to present a unified treatment of HRI-related problems, to identify key themes, and discuss challenge problems that are likely to shape the field in the near future. Although the review follows a survey structure, the goal of presenting a coherent “story” of HRI means that there are necessarily some well-written, intriguing, and influential papers that are not referenced. Instead of trying to survey every paper, we describe the HRI story from multiple perspectives with an eye toward identifying themes that cross applications. The survey attempts to include papers that represent a fair cross section of the universities, government efforts, industry labs, and countries that contribute to HRI, and a cross section of the disciplines that contribute to the field, such as human, factors, robotics, cognitive psychology, and design.},
author = {Goodrich, Michael a. and Schultz, Alan C.},
doi = {10.1561/1100000005},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/ADA476795.pdf:pdf},
isbn = {9781601980922},
issn = {1551-3955},
journal = {Foundations and Trends® in Human-Computer Interaction},
number = {3},
pages = {203--275},
title = {{Human-Robot Interaction: A Survey}},
url = {http://www.nowpublishers.com/article/Details/HCI-005},
volume = {1},
year = {2007}
}
@book{Russell2009a,
abstract = {The long-anticipated revision of this {\#}1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications. Intelligent Agents. Solving Problems by Searching. Informed},
author = {Russell, Stuart and Norvig, Peter},
booktitle = {Prentice Hall},
doi = {10.1017/S0269888900007724},
isbn = {0136042597},
issn = {0269-8889},
pages = {1 -- 1132},
title = {{Artificial Intelligence: A Modern Approach, 3rd edition}},
url = {http://portal.acm.org/citation.cfm?id=1671238{\&}coll=DL{\&}dl=GUIDE{\&}CFID=190864501{\&}CFTOKEN=29051579$\backslash$npapers2://publication/uuid/4B787E16-89F6-4FF7-A5E5-E59F3CFEFE88},
year = {2009}
}
@article{Cassell2000,
abstract = {Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references.},
annote = {Reference as one of research using ECAs (Embodied Conversational Agents)

Studies and demonstrates Interaction Rituals, used to build trust on the ECA.

Their scennario is one of a Real Estate Agent (REA) to perform small talk.},
author = {Cassell, Justine and Bickmore, Timothy},
doi = {10.1145/355112.355123},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/p50-cassell.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
month = {dec},
number = {12},
pages = {50--56},
pmid = {85634572},
title = {{External manifestations of trustworthiness in the interface}},
url = {http://web.media.mit.edu/{~}bickmore/publications/CACM{\_}trust.pdf http://portal.acm.org/citation.cfm?doid=355112.355123},
volume = {43},
year = {2000}
}
@article{Allen2007,
abstract = {To be effective, an agent that collaborate with humans needs to be able to learn new tasks from humans they work with. This paper describes a system that learns executable task models from a single collaborative learning session consisting of demonstration, explanation and dialogue. To accomplish this, the system integrates a range of AI technologies: deep natural language understanding, knowledge representation and reasoning, dialogue systems, planning/agent-based systems and machine learning. A formal evaluation shows the approach has great promise.},
author = {Allen, James and Chambers, Nathanael and Ferguson, George and Galescu, Lucian and Jung, Hyuckchul and Taysom, William},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/AAAI07-240.pdf:pdf},
isbn = {0006987303},
journal = {Interpreting},
keywords = {Integrated Intelligence,Technical Papers},
pages = {1514--1519},
pmid = {20838071},
title = {{PLOW : A Collaborative Task Learning Agent}},
url = {http://www.aaai.org/Papers/AAAI/2007/AAAI07-240.pdf},
volume = {22},
year = {2007}
}
@article{Castelfranchi2001,
abstract = { The authors argue that it is important to analyse the role of trust and deception in interactions between agents in virtual societies. In particular, in hybrid situations where artificial agents interact with human agents it is important that those artificial agents can reason about the trustworthiness and deceptive actions of the human counterpart. In order to support this interaction between agents in virtual societies, a theory on trust and deception must be developed. In the literature, a wide variety of theories on trust (less so on deception!) have been developed but not specifically for virtual communities. Based on these earlier scientific results, we make a first attempt to develop a general theory on trust and deception for virtual communities, and we discuss a number of examples to illustrate which objectives such a theory should fulfil.},
annote = {Discusses the problems of trust and deception in agent societies.},
author = {Castelfranchi, C. and Tan, Yao-Hua Tan Yao-Hua},
doi = {10.1109/HICSS.2001.927042},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/09817011.pdf:pdf},
isbn = {0-7695-0981-9},
issn = {1086-4415},
journal = {Proceedings of the 34th Annual Hawaii International Conference on System Sciences},
keywords = {and phrases,computer,deception,is in the fact,it behaves in a,multiagent systems,perfectly honest way,put to work,that once programmed and,the inhumanity of the,trust,virtual society},
number = {3},
pages = {55--70},
title = {{The role of trust and deception in virtual societies}},
volume = {6},
year = {2001}
}
@article{Walter2008,
abstract = {In this paper, we present a model of a trust-based recommendation system on a social network. The idea of the model is that agents use their social network to reach information and their trust relationships to filter it. We investigate how the dynamics of trust among agents affect the performance of the system by comparing it to a frequency-based recommendation system. Furthermore, we identify the impact of network density, preference heterogeneity among agents, and knowledge sparseness to be crucial factors for the performance of the system. The system self-organises in a state with performance near to the optimum; the performance on the global level is an emergent property of the system, achieved without explicit coordination from the local interactions of agents.},
archivePrefix = {arXiv},
arxivId = {nlin/0611054},
author = {Walter, Frank Edward and Battiston, Stefano and Schweitzer, Frank},
doi = {10.1007/s10458-007-9021-x},
eprint = {0611054},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/0611054.pdf:pdf},
isbn = {1387-2532},
issn = {1387-2532},
journal = {Autonomous Agents and Multi-Agent Systems},
keywords = {Recommender system,Social network,Trust},
month = {feb},
number = {1},
pages = {57--74},
primaryClass = {nlin},
title = {{A model of a trust-based recommendation system on a social network}},
url = {http://link.springer.com/10.1007/s10458-007-9021-x},
volume = {16},
year = {2007}
}
@book{Mascarenhas2015,
abstract = {This work addresses the challenge of creating virtual agents that are able to portray culturally appropriate behavior when interacting with other agents or humans. Because culture influences how people perceive their social reality it is important to have agent models that explicitly consider social elements, such as existing relational factors. We addressed this necessity by integrating culture into a novel model for simulating human social behavior. With this model, we operationalized a particular dimension of culture—individualism versus collectivism—within the context of an interactive narrative scenario that is part of an agent-based tool for intercultural training. Using this scenario we conducted a cross-cultural study in which participants from a collectivistic country (Portugal) were compared with participants from an individualistic country (the Netherlands) in the way they perceived and interacted with agents whose behavior was either individualistic or collectivistic, according to the configuration of the proposed model. In the obtained results, Portuguese subjects rated the collectivistic agents more positively than the Dutch but both countries had a similarly positive opinion about the individualistic agents. This experiment sheds new light on how people from different countries differ when assessing the social appropriateness of virtual agents, while also raising new research questions on this matter.},
author = {Mascarenhas, Samuel and Degens, Nick and Paiva, Ana and Prada, Rui and Hofstede, Gert Jan and Beulens, Adrie and Aylett, Ruth},
booktitle = {Autonomous Agents and Multi-Agent Systems},
doi = {10.1007/s10458-015-9312-6},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/art{\%}3A10.1007{\%}2Fs10458-015-9312-6.pdf:pdf},
isbn = {1045801593},
issn = {1387-2532},
keywords = {Cognitive models,Collectivism,Culture,Individualism,Intelligent virtual environments,Virtual agents},
publisher = {Springer US},
title = {{Modeling culture in intelligent virtual agents}},
url = {http://link.springer.com/10.1007/s10458-015-9312-6},
year = {2015}
}
@article{Correia,
author = {Correia, Filipa},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/Projecto-Filipa-Final.pdf:pdf},
keywords = {artificial intelligence,formation,hidden in-,interactive companions,socially intelligent behaviour,trick-taking card game},
title = {{EMYS : a social robot that plays “ Sueca ”}}
}
@article{Prada2014,
author = {Prada, R. and Paiva, A.},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/Human-Agent Interaction- Challenges for Bringing Humans and Agents Together.pdf:pdf},
journal = {In Proc. of the 3rd Int. Workshop on Human-Agent Interaction Design and Models (HAIDM 2014)},
pages = {1--10},
title = {{Human-agent interaction: Challenges for bringing humans and agents together}},
year = {2014}
}
@article{Pasternack2010,
author = {Pasternack, Jeff and Roth, D},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/C10-1099.pdf:pdf},
journal = {Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010)},
number = {August},
pages = {877--885},
title = {{Knowing what to believe (when you already know something)}},
url = {http://dl.acm.org/citation.cfm?id=1873880},
year = {2010}
}
@article{HanYu2013,
abstract = {In open and dynamic multiagent systems (MASs), agents often need to rely on resources or services provided by other agents to accomplish their goals. During this process, agents are exposed to the risk of being exploited by others. These risks, if not mitigated, can cause serious breakdowns in the operation of MASs and threaten their long-term wellbeing. To protect agents from the uncertainty in the behavior of their interaction partners, the age-old mechanism of trust between human beings is re-contexted into MASs. The basic idea is to let agents self-police the MAS by rating each other on the basis of their observed behavior and basing future interaction decisions on such information. Over the past decade, a large number of trust management models were proposed. However, there is a lack of research effort in several key areas, which are critical to the success of trust management in MASs where human beings and agents coexist. The purpose of this paper is to give an overview of existing research in trust management in MASs. We analyze existing trust models from a game theoretic perspective to highlight the special implications of including human beings in an MAS, and propose a possible research agenda to advance the state of the art in this field.},
author = {{Han Yu} and {Zhiqi Shen} and Leung, Cyril and {Chunyan Miao} and Lesser, Victor R.},
doi = {10.1109/ACCESS.2013.2259892},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/06514820.pdf:pdf},
isbn = {2169-3536 VO - 1},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {Analytical models,Computational modeling,Context awareness,Decision making,Game theory,Trust management,Uncertainty,multi-agent systems,reputation,trust},
pages = {35--50},
title = {{A Survey of Multi-Agent Trust Management Systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6514820},
volume = {1},
year = {2013}
}
@incollection{Gambetta2000,
abstract = {In this concluding essay I shall try to reconstruct what seem to me the central questions about trust that the individual contributions presented in this volume raise and partly answer.1 In the first section, I briefly qualify the claim that there is a degree of rational cooperation that should but does not exist, and I shall give a preliminary indication of the importance of the beliefs we hold about others, over and above the importance of the motives we may have for cooperation. In the second section, I define trust and the general conditions under which it becomes relevant for cooperation. In the third, I discuss the extent to which cooperation can come about independently of trust, and also whether trust can be seen as a result rather than a precondition of cooperation. In the final section, I address the question of whether there are rational reasons for people to trust - and especially whether there are reasons to trust trust and, correspondingly, distrust distrust.},
author = {Gambetta, Diego},
booktitle = {Trust: Making and Breaking Cooperative Relations},
doi = {10.1.1.24.5695},
isbn = {0631175873},
pages = {213--237},
title = {{Can We Trust Trust?}},
year = {2000}
}
@article{VanKleef2010,
author = {{Van Kleef}, G. A. and Homan, A. C. and Beersma, B. and van Knippenberg, D.},
doi = {10.1177/0956797610387438},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/Van Kleef et al. (2010, PS) - Angry Leaders {\&} Agreeable Followers.pdf:pdf},
issn = {0956-7976},
journal = {Psychological Science},
month = {dec},
number = {12},
pages = {1827--1834},
title = {{On Angry Leaders and Agreeable Followers: How Leaders' Emotions and Followers' Personalities Shape Motivation and Team Performance}},
url = {http://pss.sagepub.com/lookup/doi/10.1177/0956797610387438},
volume = {21},
year = {2010}
}
@inproceedings{Bickmore2001b,
abstract = {What kinds of social relationships can people have with computers? Are there activities that computers can engage in that actively draw people into relationships with them? What are the potential benefits to the people who participate in these human-computer relationships? To address these questions this work introduces a theory of Relational Agents, which are computational artifacts designed to build and maintain long-term, social-emotional relationships with their users. These can be purely software humanoid animated agents-as developed in this work-but they can also be non-humanoid or embodied in various physical forms, from robots, to pets, to jewelry, clothing, hand-helds, and other interactive devices. Central to the notion of relationship is that it is a persistent construct, spanning multiple interactions; thus, Relational Agents are explicitly designed to remember past history and manage future expectations in their interactions with users. Finally, relationships are fundamentally social and emotional, and detailed knowledge of human social psychology-with a particular emphasis on the role of affect-must be incorporated into these agents if they are to effectively leverage the mechanisms of human social cognition in order to build relationships in the most natural manner possible. People build relationships primarily through the use of language, and primarily within the context of face-to-face conversation. Embodied Conversational Agents-anthropomorphic computer characters that emulate the experience of face-to-face conversation-thus provide the substrate for this work, and so the relational activities provided by the theory will primarily be specific types of verbal and nonverbal conversational behaviors used by people to negotiate and maintain relationships. This work also provides an analysis of the types of applications in which having a human-computer relationship is advantageous to the human participant. In addition to applications in which the relationship is an end in itself (e.g., in entertainment systems), human-computer relationships are important in tasks in which the human is attempting to undergo some change in behavior or cognitive or emotional state. One such application is explored here: a system for assisting the user through a month-long health behavior change program in the area of exercise adoption. This application involves the research, design and implementation of relational agents as well as empirical evaluation of their ability to build relationships and effect change over a series of interactions with users.},
address = {New York, New York, USA},
annote = {Defines Relational Agents - agents designed to build long-term relationships with users.

Maybe use as a concept for the project's agent model?},
author = {Bickmore, Timothy and Cassell, Justine},
booktitle = {Proceedings of the SIGCHI conference on Human factors in computing systems - CHI '01},
doi = {10.1145/365024.365304},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/bickmore-thesis.pdf:pdf},
isbn = {1581133278},
keywords = {able to establish social,eases,engage their trust which,in turn,relationships with,this sort must be,users in order to},
number = {3},
pages = {396--403},
publisher = {ACM Press},
title = {{Relational agents}},
url = {http://portal.acm.org/citation.cfm?doid=365024.365304},
volume = {PhD Thesis},
year = {2001}
}
@article{Sabater2005,
abstract = {The scientific research in the area of computational mechanisms for trust and reputation in virtual societies is a recent discipline oriented to increase the reliability and performance of electronic communities. Computer science has moved from the paradigm of isolated machines to the paradigm of networks and distributed computing. Likewise, artificial intelligence is quickly moving from the paradigm of isolated and non-situated intelligence to the paradigm of situated, social and collective intelligence. The new paradigm of the so called intelligent or autonomous agents and multi-agent systems (MAS) together with the spectacular emergence of the information society technologies (specially reflected by the popularization of electronic commerce) are responsible for the increasing interest on trust and reputation mechanisms applied to electronic societies. This review wants to offer a panoramic view on current computational trust and reputation models.},
author = {Sabater, Jordi and Sierra, Carles},
doi = {10.1007/s10462-004-0041-5},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/SabaterSierra.pdf:pdf},
isbn = {02692821 (ISSN)},
issn = {02692821},
journal = {Artificial Intelligence Review},
keywords = {Reputation,Trust},
number = {1},
pages = {33--60},
title = {{Review on computational trust and reputation models}},
volume = {24},
year = {2005}
}
@article{Jackson1993,
abstract = {I provide a (very) brief introduction to game theory. I have developed these notes to provide quick access to some of the basics of game theory; mainly as an aid for students in courses in which I assumed familiarity with game theory but did not require it as a prerequisite.},
author = {Jackson, Matthew O},
doi = {10.2139/ssrn.1968579},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/SSRN-id1968579.pdf:pdf},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
pages = {1--21},
title = {{A Brief Introduction to the Basics of Game Theory}},
url = {http://ssrn.com/paper=1968579 http://www.ssrn.com/abstract=1968579},
year = {2011}
}
@article{Fogg1997,
abstract = {We conducted an experiment to investigate if computers could motivate users to change their behavior. By leveraging a social dynamic called the "rule of reciprocity," this experiment demonstrated that users provided more helping behavior to a computer that had helped them previously than to a different computer. Users also worked longer, performed higher quality work, and felt happier. Conversely, the data provide evidence of a retaliation effect.},
author = {Fogg, B J and Nass, Clifford},
doi = {10.1145/1120212.1120419},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/p331-fogg.pdf:pdf},
isbn = {0897919262},
journal = {CHI EA '97 CHI '97 Extended Abstracts on Human Factors in Computing Systems},
keywords = {agents,computers are social actors,empirical studies,equation,experiments,influence,media,persuasion,reciprocity,retaliation,social dynamics},
number = {March},
pages = {331--332},
title = {{How Users Reciprocate to Computers : An experiment that demonstrates behavior change}},
year = {1997}
}
@article{Yuan2010,
abstract = {The trust network is a social network where nodes are inter-linked by their trust relations. It has been widely used in various applications, however, little is known about its structure due to its highly dynamic nature. Based on five trust networks obtained from the real online sites, we contribute to verify that the trust network is the small-world network: the nodes are highly clustered, while the distance between two randomly selected nodes is short. This has considerable implications on using the trust network in the trust-aware applications. We choose the trust-aware recommender system as an example of such applications and demonstrate its advantages by making use of our verified small-world nature of the trust network. ?? 2010 Elsevier B.V. All rights reserved.},
author = {Yuan, Weiwei and Guan, Donghai and Lee, Young-Koo and Lee, Sungyoung and Hur, Sung Jin},
doi = {10.1016/j.knosys.2009.12.004},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/PhD{\_}Thesis{\_}Weiwei.pdf:pdf},
isbn = {0950-7051},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Recommender system,Small-world network,Trust,Trust network},
month = {apr},
number = {3},
pages = {232--238},
title = {{Improved trust-aware recommender system using small-worldness of trust networks}},
url = {http://dx.doi.org/10.1016/j.knosys.2009.12.004 http://linkinghub.elsevier.com/retrieve/pii/S095070511000002X},
volume = {23},
year = {2010}
}
@misc{Maia,
author = {Maia, Nuno},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/projecto-NunoMaia-final.pdf:pdf},
keywords = {card games,card games framework,fiducial markers,multi-,social robots,touch table},
pages = {1--32},
title = {{I Play , You Play : A Framework for Card Games in a Multi-touch Table}}
}
@article{Ramchurn2004,
abstract = {In open environments in which autonomous agents can break contracts, computational models of trust have an important role to play in determining who to interact with and how interactions unfold. To this end, we develop such a trust model, based on confidence and reputation, and show how it can be concretely applied, using fuzzy sets, to guide agents in evaluating past interactions and in establishing new contracts with one another. Agents generally interact by engaging in some form of negotiation process which results in them making commitments to (contracts with) one another to carry out particular tasks (Jennings et al. 2001). However, in most realistic environments, there is no guarantee that a contracted agent will actually enact its commitments (because it may defect to gain higher utility or because there is uncertainty about whether the task can actually be achieved). In such situations, computational models of trust (here defined as the positive expectation that an interaction partner will act benignly and cooperatively in situations where defecting would prove more profitable to itself Dasgupta 1998) have an important role to play. First, to help determine the most reliable interaction partner (i.e., those in which the agent has the highest trust). Second, to influence the interaction process itself (e.g., an agents negotiation stance may vary according to the opponents trust level). Third, to define the},
annote = {Paper about a trust model that tries to improve upon the problem of reputation and image not being conjuntly used very well},
author = {Ramchurn, Sarvapali and Sierra, C. and Godo, L. and Jennings, N. R.},
doi = {10.1080/0883951049050904509045},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/jaai04.pdf:pdf},
issn = {0883-9514},
journal = {International Journal of Applied Artificial Intelligence},
number = {9-10},
pages = {833--852},
title = {{Devising a trust model for multi-agent interactions using confidence and reputation}},
url = {http://eprints.soton.ac.uk/260155/},
volume = {18},
year = {2004}
}
@article{Gal2010,
abstract = {Computer systems increasingly carry out tasks in mixed networks, that is in group settings in which they interact both with other computer systems and with people. Participants in these heterogeneous human-computer groups vary in their capabilities, goals, and strategies; they may cooperate, collaborate, or compete. The presence of people in mixed networks raises challenges for the design and the evaluation of decision-making strategies for computer agents. This paper describes several new decision-making models that represent, learn and adapt to various social attributes that influence people's decision-making and presents a novel approach to evaluating such models. It identifies a range of social attributes in an open-network setting that influence people's decision-making and thus affect the performance of computer-agent strategies, and establishes the importance of learning and adaptation to the success of such strategies. The settings vary in the capabilities, goals, and strategies that people bring into their interactions. The studies deploy a configurable system called Colored Trails (CT) that generates a family of games. CT is an abstract, conceptually simple but highly versatile game in which players negotiate and exchange resources to enable them to achieve their individual or group goals. It provides a realistic analogue to multi-agent task domains, while not requiring extensive domain modeling. It is less abstract than payoff matrices, and people exhibit less strategic and more helpful behavior in CT than in the identical payoff matrix decision-making context. By not requiring extensive domain modeling, CT enables agent researchers to focus their attention on strategy design, and it provides an environment in which the influence of social factors can be better isolated and studied. ?? 2010 Elsevier B.V.},
author = {Gal, Ya'Akov and Grosz, Barbara and Kraus, Sarit and Pfeffer, Avi and Shieber, Stuart},
doi = {10.1016/j.artint.2010.09.002},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/1-s2.0-S0004370210001451-main.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Human-Computer decision-making,Negotiation},
month = {dec},
number = {18},
pages = {1460--1480},
publisher = {Elsevier B.V.},
title = {{Agent decision-making in open mixed networks}},
url = {http://dx.doi.org/10.1016/j.artint.2010.09.002 http://linkinghub.elsevier.com/retrieve/pii/S0004370210001451},
volume = {174},
year = {2010}
}
@incollection{Simpson2007,
abstract = {(from the chapter) The first section of the chapter reviews basic definitions, conceptualizations, and operationalizations of interpersonal trust. After reviewing some of the linguistic origins of trust, both individualistic (dispositional) and interpersonal (dyadic) definitions and conceptualizations of trust are presented. The second section highlights some of the major theoretical foundations and bases of trust at different levels of conceptual analysis. At the ultimate level of analysis, traditional genetic evolutionary models relevant to trust as well as multilevel selection/cultural coevolutionary models are showcased. At the ontogenetic level, some prominent lifespan models of social and personality development that are most pertinent to interpersonal trust are highlighted. At the proximate level, a few of the most significant social and psychological processes bearing on trust are outlined. Following this, major models specifying the normative (i.e., typical or modal) and individual-difference processes believed to govern the development, maintenance, and deterioration of trust in close relationships are discussed. The third section provides a selective yet representative overview of research on trust, with most attention focusing on interpersonal (rather than intergroup) trust. This overview begins with the seminal contributions of Deutsch and the early Prisoner's Dilemma Game (PDG) studies conducted prior to the mid-1960s, progresses to the dispositional movement that was popular from the late 1960s through the mid 1970s, and concludes with more recent dyadic formulations of trust. In the final section, six core principles of trust are identified. Following this, important constructs from different interpersonal models are merged to form an integrative process model, which suggests how trust might develop and be maintained in relationships. (PsycINFO Database Record (c) 2012 APA, all rights reserved) (chapter)},
author = {Simpson, Jeffry a},
booktitle = {Social psychology: Handbook of basic principles (2nd ed.).},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/Simpson{\_}{\_}Trust{\_}Handbook{\_}chapter{\_}{\_}2007{\_}.pdf:pdf},
isbn = {1-57230-918-0},
keywords = {Interpersonal Interaction,Trust (Social Behavior),interpersonal trust},
pages = {587--607},
title = {{Foundations of interpersonal trust}},
year = {2007}
}
@article{Yu2003,
abstract = {Business and military partners, companies and their customers, and other closely cooperating parties may have a compelling need to conduct sensitive interactions on line, such as accessing each other's local services and other local resources. Automated trust negotiation is an approach to establishing trust between parties so that such interactions can take place, through the use of access control policies that specify what combinations of digital credentials a stranger must disclose to gain access to a local resource. A party can use many different strategies to negotiate trust, offering tradeoffs between the length of the negotiation, the amount of extraneous information disclosed, and the computational effort expended. To preserve parties' autonomy, each party should ideally be able to choose its negotiation strategy independently, while still being guaranteed that negotiations will succeed whenever possible---that the two parties' strategies will interoperate. In this paper we provide the formal underpinnings for that goal, by formalizing the concepts of negotiation protocols, strategies, and interoperation. We show how to model the information flow of a negotiation for use in analyzing strategy interoperation. We also present two large sets of strategies whose members all interoperate with one another, and show that these sets contain many practical strategies. We develop the theory for black-box propositional credentials as well as credentials with internal structure, and for access control policies whose contents are (respectively are not) sensitive. We also discuss how these results fit into TrustBuilder, our prototype system for trust negotiation.},
annote = {security applicaction of trust, mostly between arranging a contract between the parties.

not that interesting besides work reference},
author = {Yu, Ting and Winslett, Marianne and Seamons, Kent E.},
doi = {10.1145/605434.605435},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/5538b77f0cf226723ab6394f.pdf:pdf},
isbn = {1094-9224},
issn = {10949224},
journal = {ACM Transactions on Information and System Security},
number = {1},
pages = {1--42},
title = {{Supporting structured credentials and sensitive policies through interoperable strategies for automated trust negotiation}},
volume = {6},
year = {2003}
}
@article{Grosz1996,
abstract = {The construction of computer systems that are intelligent, collaborative problem-solving partners is an important goal for both the science of AI and its application. From the scientific perspective, the development of theories and mechanisms to enable building collaborative systems presents exciting research challenges across AI subfields. From the applications perspective, the capability to collaborate with users and other systems is essential if large-scale information systems of the future are to assist users in finding the information they need and solving the problems they have. In this address, it is argued that collaboration must be designed into systems from the start; it cannot be patched on. Key features of collaborative activity are described, the scientific base provided by recent AI research is discussed, and several of the research challenges posed by collaboration are presented. It is further argued that research on, and the development of, collaborative systems should itself be a collaborative endeav- or—within AI, across subfields of computer science, and with researchers in other fields},
author = {Grosz, Barbara J.},
journal = {AI Magazine},
keywords = {artificial intelligence,collaboration,collaborative systems},
pages = {67--85},
title = {{Collaborative Systems}},
year = {1996}
}
@article{Mui2002,
annote = {Trust model focused on e-commerce},
author = {Mui, Lik and Mohtashemi, Mojdeh and Halberstadt, Ari},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/14350188.pdf:pdf},
number = {c},
pages = {1--9},
title = {{A Computational Model of Trust and Reputation}},
volume = {00},
year = {2002}
}
@article{Sutcliffe2012,
author = {Sutcliffe, Alistair and Wang, Di},
doi = {10.18564/jasss.1912},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/3 (1).pdf:pdf},
issn = {1460-7425},
journal = {Journal of Artificial Societies and Social Simulation},
keywords = {social agents,social modelling,social networks,trust},
month = {aug},
number = {1},
pages = {523--531},
title = {{Computational Modelling of Trust and Social Relationships}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0022519303001437 http://jasss.soc.surrey.ac.uk/15/1/3.html},
volume = {15},
year = {2012}
}
@article{Abdul-rahman2000,
author = {Abdul-rahman, Alfarez and Hailes, Stephen},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/04936007.pdf:pdf},
isbn = {0769504930},
journal = {System Sciences, 2000. Proceedings of the 33rd Annual Hawaii International Conference on},
number = {c},
pages = {1--9},
title = {{Supporting Trust in Virtual Communities}},
volume = {00},
year = {2000}
}
@phdthesis{Schaefer2009,
abstract = {As robots penetrate further into the everyday environments, trust in these robots becomes a crucial issue. The purpose of this work was to create and validate a reliable scale that could measure changes in an individual’s trust in a robot. Assessment of current trust theory identified measurable antecedents specific to the human, the robot, and the environment. Six experiments subsumed the development of the 40 item trust scale. Scale development included the creation of a 172 item pool. Two experiments identified the robot features and perceived functional characteristics that were related to the classification of a machine as a robot for this item pool. Item pool reduction techniques and subject matter expert (SME) content validation were used to reduce the scale to 40 items. The two final experiments were then conducted to validate the scale. The finalized 40 item pre-post interaction trust scale was designed to measure trust perceptions specific to HRI. The scale measured trust on a 0-100{\%} rating scale and provides a percentage trust score. A 14 item sub-scale of this final version of the test recommended by SMEs may be sufficient for some HRI tasks, and the implications of this proposition were discussed.},
author = {Schaefer, Kristin},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/Kristin (2013) (3).pdf:pdf},
pages = {332},
title = {{The Perception and Measurement of Human-Robot Trust}},
year = {2009}
}
@article{Lashkari1994,
abstract = {Interface agents are semi-intelligent systems which assist users with$\backslash$ndaily computer-based tasks. Recently, various researchers have proposed$\backslash$na learning approach towards building such agents and some working$\backslash$nprototypes have been demonstrated. Such agents learn by `watching$\backslash$nover the shoulder' of the user and detecting patterns and regularities$\backslash$nin the user's behavior. Despite the successes booked, a major problem$\backslash$nwith the learning approach is that the agent has to learn from scratch$\backslash$nand thus takes some time becoming useful. Secondly, the agent's competence$\backslash$nis necessarily limited to actions it has seen the user perform. Collaboration$\backslash$nbetween agents assisting different users can alleviate both of these$\backslash$nproblems. We present a framework for multi-agent collaboration and$\backslash$ndiscuss results of a working prototype, based on learning agents$\backslash$nfor electronic mail.},
author = {Lashkari, Yezdi and Metral, Max and Maes, Pattie},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/lashkari-metral-maes-1994.pdf:pdf},
isbn = {0-262-61102-3},
journal = {AAAI '94: Proceedings of the twelfth national conference on Artificial intelligence},
pages = {444--449},
title = {{Collaborative interface agents}},
volume = {1},
year = {1994}
}
@inproceedings{Bickmore2001,
abstract = {Building trust with users is crucial in a wide range of applications, such as financial transactions, and some minimal degree of trust is required in all applications to even initiate and maintain an interaction with a user. Humans use a variety of relational conversational strategies, including small talk, to establish trusting relationships with each other. We argue that such strategies can also be used by interface agents, and that embodied conversational agents are ideally suited for this task given the myriad cues available to them for signaling trustworthiness. We describe a model of social dialogue, an implementation in an embodied conversation agent, and an experiment in which social dialogue was demonstrated to have an effect on trust, for users with a disposition to be extroverts.},
address = {New York, New York, USA},
annote = {ECAs - Embodied Conversational Agents
Describes a model to create agents capable of social dialogue, with speech and social strategies.
Should look more into it, specially for reference in agent technology.

Paper to the thesis of the same name.},
author = {Bickmore, Timothy and Cassell, Justine},
booktitle = {Proceedings of the SIGCHI conference on Human factors in computing systems - CHI '01},
doi = {10.1145/365024.365304},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/CHI2001.pdf:pdf},
isbn = {1581133278},
keywords = {able establish social,eases,engage trust which,relationships with,sort must,turn,users order},
number = {3},
pages = {396--403},
publisher = {ACM Press},
title = {{Relational agents}},
url = {http://dl.acm.org/citation.cfm?id=365024.365304 http://portal.acm.org/citation.cfm?doid=365024.365304},
volume = {3},
year = {2001}
}
@article{Huang2008,
abstract = {Trust and reputation have been recognized as a key issue in the area of multi-agent systems. And till now, a number of researchers have, in different perspectives, reviewed/surveyed the notions, techniques or models of trust and reputation. However, the notion of trust and reputation still remains somewhat vague, and the techniques and model is yet understood in a disunited way. This paper categorizes the notions, techniques, and models of trust and reputation in terms of the trust management process which can be characterized through three questions: a) why does an agent trust another; b) how do agents judge or evaluate the trustworthiness of others; c) what does an agent do after obtaining the trustworthiness of others. These questions present a unified view of trust and reputation. The two aspects implied in the first question suggest two different understanding of the notion of trust. The second question refers to the techniques of getting and dealing with two different types of trust evidence. And it comes to the third question when trust models take agents' post-evaluation actions and the effect of these actions on their partners/opponents into account to make themselves incentive-compatible.},
author = {Huang, Hongbing and Zhu, Guiming and Jin, Shiyao},
doi = {10.1109/CCCM.2008.122},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/04609545.pdf:pdf},
isbn = {978-0-7695-3290-5},
journal = {Computing, Communication, Control, and Management, 2008. CCCM '08. ISECS International Colloquium on},
pages = {424--429},
title = {{Revisiting Trust and Reputation in Multi-agent Systems}},
volume = {1},
year = {2008}
}
@article{Zacharia2000,
abstract = {The members of electronic communities are often unrelated to each other; they may have never met and have no information on each other's reputation. This kind of information is vital in electronic commerce interactions, where the potential counterpart's reputation can be a significant factor in the negotiation strategy. Two complementary reputation mechanisms are investigated which rely on collaborative rating and personalized evaluation of the various ratings assigned to each user. While these reputationmechanisms are developed in the context of electronic commerce, it is believed that they may have applicability in other types of electronic communities such as chatrooms, newsgroups, mailing lists, etc.},
annote = {Example of application of trust models in ecommerce},
author = {Zacharia, Giorgos and Maes, Pattie},
doi = {10.1080/08839510050144868},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/08839510050144868.pdf:pdf},
isbn = {0218-8430},
issn = {0883-9514},
journal = {Applied Artificial Intelligence},
month = {oct},
number = {9},
pages = {881--907},
title = {{Trust management through reputation mechanisms}},
url = {http://www.tandfonline.com/doi/abs/10.1080/08839510050144868},
volume = {14},
year = {2000}
}
@article{Gao2013,
abstract = {This research is sponsored by the Office of Naval Research and the Air Force Office of Scientific Research.},
author = {Gao, F. and Clare, A. S. and Macbeth, J. C. and Cummings, M. L.},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/5741-24577-1-PB.pdf:pdf},
isbn = {9781577356042},
journal = {AAAI Spring Symposium: Trust and Autonomous Systems,},
keywords = {multiple robot control,system dynamics model,workload},
pages = {16--22},
title = {{Modeling the Impact of Operator Trust on Performance in Multiple Robot Control,}},
url = {http://dspace.mit.edu/handle/1721.1/90334},
year = {2013}
}
@inproceedings{Yu2001,
abstract = {Automated trust negotiation is an approach to establishing trust between strangers through the exchange of digital cre- dentials and the use of access control policies that specify what combinations of credentials a stranger must disclose in order to gain access to each local service or credential. We introduce the concept of a trust negotiation protocol, which defines the ordering of messages and the type of in- formation messages will contain. To carry out trust nego- tiation, a party pairs its negotiation protocol with a trust negotiation strategy that controls the exact content of the messages, i.e., which credentials to disclose, when to dis- close them, and when to terminate a negotiation. There are a huge number of possible strategies for negotiating trust, each with different properties with respect to speed of nego- tiations and caution in giving out credentials and policies. In the autonomous world of the Internet, entities will want the freedom to choose negotiation strategies that meet their own goals, which means that two strangers who negotiate trust will often not use the same strategy. To date, only a tiny fraction of the space of possible negotiation strategies has been explored, and no two of the strategies proposed so far will interoperate. In this paper, we define a large set of strategies called the disclosure tree strategy (DTS) fam- ily. Then we prove that if two parties each choose strategies from the DTS family, then they will be able to negotiate trust as well as if they were both using the same strategy. Further, they can change strategies at any point during ne- gotiation. We also show that the DTS family is closed, i.e., any strategy that can interoperate with every strategy in the DTS family must also be a member of the DTS family. We also give examples of practical strategies that belong to the DTS family and fit within the TrustBuilder architecture and protocol for trust negotiation.},
address = {New York, New York, USA},
annote = {Defines a family of trust negiotiation strategies, Disclosure tree strategy (DTS).

Focused on e-security aspect of Trust, not the focus of the project.},
author = {Yu, Ting and Winslett, Marianne and Seamons, Kent E.},
booktitle = {Proceedings of the 8th ACM conference on Computer and Communications Security - CCS '01},
doi = {10.1145/501983.502004},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/CCS-Yu-2001.pdf:pdf},
isbn = {1581133855},
pages = {146----155},
publisher = {ACM Press},
title = {{Interoperable strategies in automated trust negotiation}},
url = {http://portal.acm.org/citation.cfm?doid=501983.502004},
year = {2001}
}
@article{Ramchurn2004a,
abstract = {Trust is a fundamental concern in large-scale open distributed systems. It lies at the core of all interactions between the entities that have to operate in such uncertain and constantly changing environments. Given this complexity, these components, and the ensuing system, are increasingly being conceptualised, designed, and built using agent-based techniques and, to this end, this paper examines the specific role of trust in multi-agent systems. In particular, we survey the state of the art and provide an account of the main directions along which research efforts are being focused. In so doing, we critically evaluate the relative strengths and weaknesses of the main models that have been proposed and show how, fundamentally, they all seek to minimise the uncertainty in interactions. Finally, we outline the areas that require further research in order to develop a comprehensive treatment of trust in complex computational settings.},
annote = {Survey about the state on Trust Models in MAS.
Useful for inspiration for related work and background.
Oldest review though},
author = {Ramchurn, S.D. and Huynh, T.D. and Jennings, N. R.},
doi = {10.1017/S0269888904000116},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/ker-trust.pdf:pdf},
isbn = {0269-8889},
issn = {0269-8889},
journal = {The Knowledge Engineering Review},
number = {1},
pages = {1--25},
pmid = {1767140064670383174},
title = {{Trust in Multiagent Systems}},
url = {http://eprints.soton.ac.uk/259564/},
volume = {19},
year = {2004}
}
@article{Sabater2006,
abstract = {This paper introduces Repage, a computational system that adopts a cognitive theory of reputation. We propose a fundamental difference between image and reputation, which suggests a way out from the paradox of sociality, i.e. the trade-off between agents' autonomy and their need to adapt to social environment. On one hand, agents are autonomous if they select partners based on their social evaluations (images). On the other, they need to update evaluations by taking into account others'. Hence, social evaluations must circulate and be represented as "reported evaluations" (reputation), before and in order for agents to decide whether to accept them or not. To represent this level of cognitive detail in artificial agents' design, there is a need for a specialised subsystem, which we are in the course of developing for the public domain. In the paper, after a short presentation of the cognitive theory of reputation and its motivations, we describe the implementation of Repage.},
annote = {Semi-Cognitive model.

Direct Trust calculation is heavily based on the ReGreT model.

What brings of new to the table is the distinction betweeen reputation and image. Of what the agent perceives through direct interaction with the target agent and of what he hears of informats about the target agent.

(As a side note: Both Repage and Regret were created by the same head author, Jordi Sabater)},
author = {Sabater, Jordi and Paolucci, Mario and Conte, Rosaria},
doi = {Article},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/3.pdf:pdf},
issn = {14607425},
journal = {Jasss},
keywords = {Agent Systems,Cognitive Design,Fuzzy Evaluation,Reputation},
number = {2},
pages = {117--134},
title = {{Repage: REPutation and ImAGE among limited autonomous partners}},
volume = {9},
year = {2006}
}
@article{Lee2004,
annote = {Describes older literature on Trust, focused on the psycological aspect. 
Good citation for diferent perspective on trust, besides computational trust},
author = {Lee, John D and See, Katrina A and City, Iowa},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/leesee04.pdf:pdf},
number = {1},
pages = {50--80},
title = {{Trust in Automation : Designing for Appropriate Reliance}},
volume = {46},
year = {2004}
}
@misc{Wikipedia.Golden.Balls,
author = {Wikipedia},
booktitle = {Wikipedia},
title = {{Golden Balls: https://en.wikipedia.org/wiki/Golden{\_}Balls}},
url = {https://en.wikipedia.org/wiki/Golden{\_}Balls}
}
