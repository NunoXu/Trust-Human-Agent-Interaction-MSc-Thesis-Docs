Automatically generated by Mendeley Desktop 1.15.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Lee2004,
author = {Lee, John D and See, Katrina A and City, Iowa},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/leesee04.pdf:pdf},
number = {1},
pages = {50--80},
title = {{Trust in Automation : Designing for Appropriate Reliance}},
volume = {46},
year = {2004}
}
@article{Pinto2008,
abstract = {No trabalho},
author = {Pinto, Santos},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/Andrio-Pinto-WTDIA-2008.pdf:pdf},
keywords = {behavioral simulation,cooperation,group interation  simulation,multi-agent systems,reputation},
pages = {99},
title = {{Simula{\c{c}}{\~{a}}o e Avalia{\c{c}}{\~{a}}o de Comportamentos em Sistemas Multi-Agentes baseados em Modelos de Reputa{\c{c}}{\~{a}}o e Intera{\c{c}}{\~{a}}o}},
url = {http://www.unissinos.br},
volume = {2},
year = {2008}
}
@article{Ramchurn2004a,
abstract = {Trust is a fundamental concern in large-scale open distributed systems. It lies at the core of all interactions between the entities that have to operate in such uncertain and constantly changing environments. Given this complexity, these components, and the ensuing system, are increasingly being conceptualised, designed, and built using agent-based techniques and, to this end, this paper examines the specific role of trust in multi-agent systems. In particular, we survey the state of the art and provide an account of the main directions along which research efforts are being focused. In so doing, we critically evaluate the relative strengths and weaknesses of the main models that have been proposed and show how, fundamentally, they all seek to minimise the uncertainty in interactions. Finally, we outline the areas that require further research in order to develop a comprehensive treatment of trust in complex computational settings.},
author = {Ramchurn, S.D. and Huynh, T.D. and Jennings, N. R.},
doi = {10.1017/S0269888904000116},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/ker-trust.pdf:pdf},
isbn = {0269-8889},
issn = {0269-8889},
journal = {The Knowledge Engineering Review},
number = {1},
pages = {1--25},
pmid = {1767140064670383174},
title = {{Trust in Multiagent Systems}},
url = {http://eprints.soton.ac.uk/259564/},
volume = {19},
year = {2004}
}
@article{Parasuraman2000,
author = {Parasuraman, R and Sheridan, T.B.},
doi = {10.1109/3468.844354},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/parasuraman.pdf:pdf},
isbn = {1083-4427},
issn = {1083-4427},
journal = {Systems, Man and {\ldots}},
number = {3},
pages = {286--297},
pmid = {11760769},
title = {{A model for types and levels of human interaction with automation}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=844354},
volume = {30},
year = {2000}
}
@article{Sabater2006,
abstract = {This paper introduces Repage, a computational system that adopts a cognitive theory of reputation. We propose a fundamental difference between image and reputation, which suggests a way out from the paradox of sociality, i.e. the trade-off between agents' autonomy and their need to adapt to social environment. On one hand, agents are autonomous if they select partners based on their social evaluations (images). On the other, they need to update evaluations by taking into account others'. Hence, social evaluations must circulate and be represented as "reported evaluations" (reputation), before and in order for agents to decide whether to accept them or not. To represent this level of cognitive detail in artificial agents' design, there is a need for a specialised subsystem, which we are in the course of developing for the public domain. In the paper, after a short presentation of the cognitive theory of reputation and its motivations, we describe the implementation of Repage.},
annote = {Semi-Cognitive model.

Direct Trust calculation is heavily based on the ReGreT model.

What brings of new to the table is the distinction betweeen reputation and image. Of what the agent perceives through direct interaction with the target agent and of what he hears of informats about the target agent.

(As a side note: Both Repage and Regret were created by the same head author, Jordi Sabater)},
author = {Sabater, Jordi and Paolucci, Mario and Conte, Rosaria},
doi = {Article},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/3.pdf:pdf},
issn = {14607425},
journal = {Jasss},
keywords = {Agent Systems,Cognitive Design,Fuzzy Evaluation,Reputation},
number = {2},
pages = {117--134},
title = {{Repage: REPutation and ImAGE among limited autonomous partners}},
volume = {9},
year = {2006}
}
@article{VanKleef2010,
author = {{Van Kleef}, G. A. and Homan, A. C. and Beersma, B. and van Knippenberg, D.},
doi = {10.1177/0956797610387438},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/Van Kleef et al. (2010, PS) - Angry Leaders {\&} Agreeable Followers.pdf:pdf},
issn = {0956-7976},
journal = {Psychological Science},
month = {dec},
number = {12},
pages = {1827--1834},
title = {{On Angry Leaders and Agreeable Followers: How Leaders' Emotions and Followers' Personalities Shape Motivation and Team Performance}},
url = {http://pss.sagepub.com/lookup/doi/10.1177/0956797610387438},
volume = {21},
year = {2010}
}
@article{Antos2011,
author = {Antos, Dimitrios and Melo, Celso De and Gratch, Jonathan and Grosz, Barbara J},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/antosDeMelo11-aaai.pdf:pdf},
isbn = {9781577355083},
journal = {Aaai},
keywords = {Multidisciplinary Topics},
pages = {772--778},
title = {{The Influence of Emotion Expression on Perceptions of Trustworthiness in Negotiation.}},
url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/viewPDFInterstitial/3438/3951},
year = {2011}
}
@article{Mui2002,
author = {Mui, Lik and Mohtashemi, Mojdeh and Halberstadt, Ari},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/14350188.pdf:pdf},
number = {c},
pages = {1--9},
title = {{A Computational Model of Trust and Reputation}},
volume = {00},
year = {2002}
}
@article{Castelfranchi1998,
abstract = {After arguing about the crucial importance of trust for Agents and MAS, we provide a definition of trust both as a mental state and as a social attitude and relation. We present the mental ingredients of trust: its specific beliefs and goals, with special attention to evaluations and expectations. We show the relation between trust and the mental background of delegation. We explain why trust is a bet, and implies some risks, and analyse the most basic forms of non-social trust (reliance on objects and tools) to arrive at the more complex forms of social trust, based on morality and reputation. Finally we present a principled quantification of trust, based on its cognitive ingredients. And use this “degree of trust” as the basis for a rational decision to delegate or not to another agent. The paper is intended to contribute both to the conceptual analysis and to the practical use of trust in social theory and MAS},
author = {Castelfranchi, C and Falcone, R},
doi = {10.1109/ICMAS.1998.699034},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/2001{\_}Castelfranchi.pdf:pdf},
isbn = {0-8186-8500-X},
issn = {0-8186-8500-X},
journal = {Proceedings of the International Conference on Multi Agent Systems},
pages = {72--79},
title = {{Principles of trust for MAS: cognitive anatomy, social importance, and quantification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=699034},
year = {1998}
}
@article{Noorian2010,
abstract = {We introduce a multidimensional framework for classifying and comparing trust and reputation (T{\&}R) systems. The framework dimensions encompass both hard and soft features of such systems including different witness location approaches, various reputation calculation engines, variety of information sources and rating systems which are categorised as hard features, and also basic reputation measurement parameters, context diversity checking, reliability and honesty assessment and adaptability which are referred to as soft features. Specifically, the framework dimensions answer questions related to major characteristics of T{\&}R systems including those parameters from the real world that should be imitated in a virtual environment. The proposed framework can serve as a basis to understand the current state of the art in the area of computational trust and reputation and also help in designing suitable control mechanisms for online communities. In addition, we haveprovided a critical analysis of some of the existing techniques in the literature compared within the context of the proposed framework dimensions.},
author = {Noorian, Zeinab and Ulieru, Mihaela},
doi = {10.4067/S0718-18762010000200007},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/art07.pdf:pdf},
isbn = {0718-1876},
issn = {0718-1876},
journal = {Journal of theoretical and applied electronic commerce research},
keywords = {Computational trust,Online communities,Reliability assessment,Reputation formalization,T{\&}R taxonomy},
month = {aug},
number = {2},
pages = {97--117},
title = {{The State of the Art in Trust and Reputation Systems: A Framework for Comparison}},
url = {http://www.scielo.cl/scielo.php?script=sci{\_}arttext{\&}pid=S0718-18762010000200007{\&}lng=en{\&}nrm=iso{\&}tlng=en},
volume = {5},
year = {2010}
}
@article{Goodrich2007,
abstract = {Human-Robot Interaction (HRI) has recently received considerable attention in the academic community, in labs, in technology companies, and through the media. Because of this attention, it is desirable to present a survey of HRI to serve as a tutorial to people outside the field and to promote discussion of a unified vision of HRI within the field. The goal of this review is to present a unified treatment of HRI-related problems, to identify key themes, and discuss challenge problems that are likely to shape the field in the near future. Although the review follows a survey structure, the goal of presenting a coherent “story” of HRI means that there are necessarily some well-written, intriguing, and influential papers that are not referenced. Instead of trying to survey every paper, we describe the HRI story from multiple perspectives with an eye toward identifying themes that cross applications. The survey attempts to include papers that represent a fair cross section of the universities, government efforts, industry labs, and countries that contribute to HRI, and a cross section of the disciplines that contribute to the field, such as human, factors, robotics, cognitive psychology, and design.},
author = {Goodrich, Michael a. and Schultz, Alan C.},
doi = {10.1561/1100000005},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/ADA476795.pdf:pdf},
isbn = {9781601980922},
issn = {1551-3955},
journal = {Foundations and Trends® in Human-Computer Interaction},
number = {3},
pages = {203--275},
title = {{Human-Robot Interaction: A Survey}},
url = {http://www.nowpublishers.com/article/Details/HCI-005},
volume = {1},
year = {2007}
}
@misc{Hoc2000,
author = {Hoc, Jean-Michel},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/Hoc 2000a.pdf:pdf},
pages = {833 -- 843},
title = {{From human - machine interaction to human - machine cooperation}},
year = {2000}
}
@phdthesis{Marsh1994,
abstract = {Trust is a judgement of unquestionable utility as humans we use it every day of our lives. However, trust has suffered from an imperfect understanding, a plethora of definitions, and informal use in the literature and in everyday life. It is common to say I trust you, but what does that mean? This thesis provides a clarification of trust. We present a formalism for trust which provides us with a tool for precise discussion. The formalism is implementable: it can be embedded in an artificial agent, enabling the agent to make trust-based decisions. Its applicability in the domain of Distributed Artificial Intelligence (DAI) is raised. The thesis presents a testbed populated by simple trusting agents which substantiates the utility of the formalism. The formalism provides a step in the direction of a proper understanding and definition of human trust. A contribution of the thesis is its detailed exploration of the possibilities of future work in the area.},
author = {Marsh, Stephen Paul},
booktitle = {Computing Science and Mathematics eTheses},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/TR133.pdf:pdf},
month = {apr},
pages = {184},
title = {{Formalising Trust as a Computational Concept}},
volume = {NA},
year = {1994}
}
@article{Ramchurn2003,
author = {Ramchurn, D. Ramchurn and Jennings, Nicholas R. and Sierra, Carles and Godo, Llu{\'{\i}}s},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/aamas-trust-ws.pdf:pdf},
title = {{A Computational Trust Model for Multi-Agent Interactions based on Confidence and Reputation}},
year = {2003}
}
@article{Sabater-Mir2007,
abstract = {Interest for computational trust and reputation models is on the rise. One of the most important aspects of these models is how they deal with information received from other individuals. More generally, the critical choice is how to represent and how to aggregate social evaluations. In this article, we make an analysis of the current approaches of representation and aggregation of social evaluations under the guidelines of a set of basic requirements. Then we present two different proposals of dealing with uncertainty in the context of the Repage system [J. Sabater, M. Paolucci, R. Conte, Repage: Reputation and image among limited autonomous partners, Journal of Artificial Societies and Social Simulation 9 (2). URL http://jasss.soc.surrey.ac.uk/9/2/3.html], a computational module for management of reputational information based on a cognitive model of imAGE, REPutation and their interplay already developed by the authors. We finally discuss these two proposals in the context of several examples. ?? 2007 Elsevier Inc. All rights reserved.},
author = {Sabater-Mir, Jordi and Paolucci, Mario},
doi = {10.1016/j.ijar.2006.12.013},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/1-s2.0-S0888613X07000059-main.pdf:pdf},
isbn = {0888-613X},
issn = {0888613X},
journal = {International Journal of Approximate Reasoning},
keywords = {Aggregation mechanisms,Reputation,Trust},
number = {3},
pages = {458--483},
title = {{On representation and aggregation of social evaluations in computational trust and reputation models}},
volume = {46},
year = {2007}
}
@article{Sabater2002,
abstract = {The use of previous direct interactions is probably the best way to calculate a reputation but, unfortunately this infor- mation is not always available. This is especially true in large multi-agent systems where interaction is scarce. In this paper we present a reputation system that takes advan- tage, among other things, of social relations between agents to overcome this problem.},
annote = {Reputation system focused on social relations.

Regret Model Paper.},
author = {Sabater, Jordi and Sierra, Carles},
doi = {10.1145/544852.544854},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/2002-AAMASa.pdf:pdf},
isbn = {1581134800},
journal = {Proceedings of the first international joint conference on Autonomous agents and multiagent systems part 1 - AAMAS '02},
pages = {475},
title = {{Reputation and social network analysis in multi-agent systems}},
url = {http://portal.acm.org/citation.cfm?doid=544741.544854},
year = {2002}
}
@article{Esfandiari2001,
abstract = {We need models of trust to facilitate cooperation in multi-agent systems, where agents, human and artificial, do not know each other beforehand. This paper lists and proposes simple mechanisms for trust acquisition based on a very basic and general definition of trust, making no assumptions on the internal cognitive models of the involved agents. We also show how trust acquired one-on-one can be propagated in a social network of agents.},
author = {Esfandiari, Babak and Chandrasekharan, Sanjay},
doi = {10.1.1.11.4683},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/download.pdf:pdf},
journal = {Proceedings of the Fourth Workshop on Deception Fraud and Trust in Agent Societies Montreal Canada},
keywords = {Mechanisms for Trust},
number = {19th of June},
pages = {27--34},
title = {{On how agents make friends: Mechanisms for trust acquisition}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.11.4683{\&}amp;rep=rep1{\&}amp;type=pdf},
volume = {222},
year = {2001}
}
@article{Walter2008,
abstract = {In this paper, we present a model of a trust-based recommendation system on a social network. The idea of the model is that agents use their social network to reach information and their trust relationships to filter it. We investigate how the dynamics of trust among agents affect the performance of the system by comparing it to a frequency-based recommendation system. Furthermore, we identify the impact of network density, preference heterogeneity among agents, and knowledge sparseness to be crucial factors for the performance of the system. The system self-organises in a state with performance near to the optimum; the performance on the global level is an emergent property of the system, achieved without explicit coordination from the local interactions of agents.},
archivePrefix = {arXiv},
arxivId = {nlin/0611054},
author = {Walter, Frank Edward and Battiston, Stefano and Schweitzer, Frank},
doi = {10.1007/s10458-007-9021-x},
eprint = {0611054},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/0611054.pdf:pdf},
isbn = {1387-2532},
issn = {1387-2532},
journal = {Autonomous Agents and Multi-Agent Systems},
keywords = {Recommender system,Social network,Trust},
month = {feb},
number = {1},
pages = {57--74},
primaryClass = {nlin},
title = {{A model of a trust-based recommendation system on a social network}},
url = {http://link.springer.com/10.1007/s10458-007-9021-x},
volume = {16},
year = {2008}
}
@article{Pasternack2010,
author = {Pasternack, Jeff and Roth, D},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/C10-1099.pdf:pdf},
journal = {Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010)},
number = {August},
pages = {877--885},
title = {{Knowing what to believe (when you already know something)}},
url = {http://dl.acm.org/citation.cfm?id=1873880},
year = {2010}
}
@article{Sabater2005,
abstract = {The scientific research in the area of computational mechanisms for trust and reputation in virtual societies is a recent discipline oriented to increase the reliability and performance of electronic communities. Computer science has moved from the paradigm of isolated machines to the paradigm of networks and distributed computing. Likewise, artificial intelligence is quickly moving from the paradigm of isolated and non-situated intelligence to the paradigm of situated, social and collective intelligence. The new paradigm of the so called intelligent or autonomous agents and multi-agent systems (MAS) together with the spectacular emergence of the information society technologies (specially reflected by the popularization of electronic commerce) are responsible for the increasing interest on trust and reputation mechanisms applied to electronic societies. This review wants to offer a panoramic view on current computational trust and reputation models.},
author = {Sabater, Jordi and Sierra, Carles},
doi = {10.1007/s10462-004-0041-5},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/SabaterSierra.pdf:pdf},
isbn = {02692821 (ISSN)},
issn = {02692821},
journal = {Artificial Intelligence Review},
keywords = {Reputation,Trust},
number = {1},
pages = {33--60},
title = {{Review on computational trust and reputation models}},
volume = {24},
year = {2005}
}
@article{Pinyol2009,
annote = {BDI + Regt},
author = {Pinyol, Isaac},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/main.pdf:pdf},
journal = {Doctoral Mentoring Program},
keywords = {agents,utation-based decisions for cognitive},
number = {Aamas},
pages = {15--16},
title = {{Reputation-Based Decisions for Cognitive Agents (Thesis Abstract)}},
url = {http://ifaamas.org/Proceedings/aamas09/pdf/07{\_}Doctoral/Doct{\_}08.pdf},
year = {2009}
}
@article{Castelfranchi2001a,
abstract = {After arguing about the crucial importance of trust for Agents and MAS, we provide a definition of trust both as a mental state and as a social attitude and relation. We present the mental ingredients of trust: its specific beliefs and goals, with special attention to evaluations and expectations. We show the relation between trust and the mental background of delegation. We explain why trust is a bet, and implies some risks, and analyse the more complex forms of social trust, based on a theory of mind and in particular on morality, reputation and disposition, and authority (three party trust). We explain why promises, contracts, authorities can increase our trust by modifying our mental representations. We present a principled quantification of trust, based on its cognitive ingredients, and use this "degree of trust" as the basis for a rational decision to delegate or not to another agent. We explain when trust is rational, and why it is not an irrational decision by definition. We also criticise the economic and game-theoretic view of trust for underestimating the importance of cognitive ingredients of trust and for reducing it to subjective probability and risk. The paper is intended to contribute both to the conceptual analysis and to the practical use of trust in social theory and MAS.},
author = {Castelfranchi, Cristiano and Falcone, Rino},
doi = {10.1007/978-94-017-3614-5{\_}3},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/cast03{\_}1.pdf:pdf},
isbn = {079236919X},
journal = {Trust and deception in virtual societies},
pages = {55--90},
title = {{Social Trust : A Cognitive Approach}},
year = {2001}
}
@article{Ramchurn2004,
abstract = {In open environments in which autonomous agents can break contracts, computational models of trust have an important role to play in determining who to interact with and how interactions unfold. To this end, we develop such a trust model, based on confidence and reputation, and show how it can be concretely applied, using fuzzy sets, to guide agents in evaluating past interactions and in establishing new contracts with one another. Agents generally interact by engaging in some form of negotiation process which results in them making commitments to (contracts with) one another to carry out particular tasks (Jennings et al. 2001). However, in most realistic environments, there is no guarantee that a contracted agent will actually enact its commitments (because it may defect to gain higher utility or because there is uncertainty about whether the task can actually be achieved). In such situations, computational models of trust (here defined as the positive expectation that an interaction partner will act benignly and cooperatively in situations where defecting would prove more profitable to itself Dasgupta 1998) have an important role to play. First, to help determine the most reliable interaction partner (i.e., those in which the agent has the highest trust). Second, to influence the interaction process itself (e.g., an agents negotiation stance may vary according to the opponents trust level). Third, to define the},
author = {Ramchurn, Sarvapali and Sierra, C. and Godo, L. and Jennings, N. R.},
doi = {10.1080/0883951049050904509045},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/jaai04.pdf:pdf},
issn = {0883-9514},
journal = {International Journal of Applied Artificial Intelligence},
number = {9-10},
pages = {833--852},
title = {{Devising a trust model for multi-agent interactions using confidence and reputation}},
url = {http://eprints.soton.ac.uk/260155/},
volume = {18},
year = {2004}
}
@article{HanYu2013,
abstract = {In open and dynamic multiagent systems (MASs), agents often need to rely on resources or services provided by other agents to accomplish their goals. During this process, agents are exposed to the risk of being exploited by others. These risks, if not mitigated, can cause serious breakdowns in the operation of MASs and threaten their long-term wellbeing. To protect agents from the uncertainty in the behavior of their interaction partners, the age-old mechanism of trust between human beings is re-contexted into MASs. The basic idea is to let agents self-police the MAS by rating each other on the basis of their observed behavior and basing future interaction decisions on such information. Over the past decade, a large number of trust management models were proposed. However, there is a lack of research effort in several key areas, which are critical to the success of trust management in MASs where human beings and agents coexist. The purpose of this paper is to give an overview of existing research in trust management in MASs. We analyze existing trust models from a game theoretic perspective to highlight the special implications of including human beings in an MAS, and propose a possible research agenda to advance the state of the art in this field.},
author = {{Han Yu} and {Zhiqi Shen} and Leung, Cyril and {Chunyan Miao} and Lesser, Victor R.},
doi = {10.1109/ACCESS.2013.2259892},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/06514820.pdf:pdf},
isbn = {2169-3536 VO - 1},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {Analytical models,Computational modeling,Context awareness,Decision making,Game theory,Trust management,Uncertainty,multi-agent systems,reputation,trust},
pages = {35--50},
title = {{A Survey of Multi-Agent Trust Management Systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6514820},
volume = {1},
year = {2013}
}
@article{Pinyol2013,
abstract = {In open environments, agents depend on reputation and trust mechanisms to evaluate the behavior of potential partners. The scientific research in this field has considerably increased, and in fact, reputation and trust mechanisms have been already considered a key elements in the design of multi-agent systems. In this paper we provide a survey that, far from being exhaustive, intends to show the most representative models that currently exist in the literature. For this enterprise we consider several dimensions of analysis that appeared in three existing surveys, and provide new dimensions that can be complementary to the existing ones and that have not been treated directly. Moreover, besides showing the original classification that each one of the surveys provide, we also classify models that where not taken into account by the original surveys. The paper illustrates the proliferation in the past few years of models that follow a more cognitive approach, in which trust and reputation representation as mental attitudes is as important as the final values of trust and reputation. Furthermore, we provide an objective definition of trust, based on Castelfranchi's idea that trust implies a decision to rely on someone. © 2011 Springer Science+Business Media B.V.},
annote = {Makes a revision of the most representative models on the literature.

Of all the models, only the following were classified as cognitive:

Castelfranchi et al., Repage, BDI + Repage and ForTrust},
author = {Pinyol, Isaac and Sabater-Mir, Jordi},
doi = {10.1007/s10462-011-9277-z},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/2013-AIRb.pdf:pdf},
isbn = {0269-2821},
issn = {0269-2821},
journal = {Artificial Intelligence Review},
keywords = {Cognitive trust and reputation,Computational trust and reputation models,Multiagent systems},
month = {jun},
number = {1},
pages = {1--25},
title = {{Computational trust and reputation models for open multi-agent systems: a review}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84878107220{\&}partnerID=tZOtx3y1 http://link.springer.com/10.1007/s10462-011-9277-z},
volume = {40},
year = {2013}
}
@article{Huang2008,
abstract = {Trust and reputation have been recognized as a key issue in the area of multi-agent systems. And till now, a number of researchers have, in different perspectives, reviewed/surveyed the notions, techniques or models of trust and reputation. However, the notion of trust and reputation still remains somewhat vague, and the techniques and model is yet understood in a disunited way. This paper categorizes the notions, techniques, and models of trust and reputation in terms of the trust management process which can be characterized through three questions: a) why does an agent trust another; b) how do agents judge or evaluate the trustworthiness of others; c) what does an agent do after obtaining the trustworthiness of others. These questions present a unified view of trust and reputation. The two aspects implied in the first question suggest two different understanding of the notion of trust. The second question refers to the techniques of getting and dealing with two different types of trust evidence. And it comes to the third question when trust models take agents' post-evaluation actions and the effect of these actions on their partners/opponents into account to make themselves incentive-compatible.},
author = {Huang, Hongbing and Zhu, Guiming and Jin, Shiyao},
doi = {10.1109/CCCM.2008.122},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/04609545.pdf:pdf},
isbn = {978-0-7695-3290-5},
journal = {Computing, Communication, Control, and Management, 2008. CCCM '08. ISECS International Colloquium on},
pages = {424--429},
title = {{Revisiting Trust and Reputation in Multi-agent Systems}},
volume = {1},
year = {2008}
}
@article{Yuan2010,
abstract = {The trust network is a social network where nodes are inter-linked by their trust relations. It has been widely used in various applications, however, little is known about its structure due to its highly dynamic nature. Based on five trust networks obtained from the real online sites, we contribute to verify that the trust network is the small-world network: the nodes are highly clustered, while the distance between two randomly selected nodes is short. This has considerable implications on using the trust network in the trust-aware applications. We choose the trust-aware recommender system as an example of such applications and demonstrate its advantages by making use of our verified small-world nature of the trust network. ?? 2010 Elsevier B.V. All rights reserved.},
author = {Yuan, Weiwei and Guan, Donghai and Lee, Young-Koo and Lee, Sungyoung and Hur, Sung Jin},
doi = {10.1016/j.knosys.2009.12.004},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/PhD{\_}Thesis{\_}Weiwei.pdf:pdf},
isbn = {0950-7051},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Recommender system,Small-world network,Trust,Trust network},
month = {apr},
number = {3},
pages = {232--238},
title = {{Improved trust-aware recommender system using small-worldness of trust networks}},
url = {http://dx.doi.org/10.1016/j.knosys.2009.12.004 http://linkinghub.elsevier.com/retrieve/pii/S095070511000002X},
volume = {23},
year = {2010}
}
@book{Castelfranchi2010,
address = {Chichester, UK},
author = {Castelfranchi, Cristiano and Falcone, Rino},
booktitle = {Wiley},
doi = {10.1002/9780470519851},
edition = {1},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/TrustTheoryASocioCognitiveandComputationalModel.pdf:pdf},
isbn = {9780470519851},
issn = {0717-6163},
month = {mar},
pages = {1--387},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Trust Theory}},
url = {http://doi.wiley.com/10.1002/9780470519851},
year = {2010}
}
@article{Castelfranchi2001,
abstract = { The authors argue that it is important to analyse the role of trust and deception in interactions between agents in virtual societies. In particular, in hybrid situations where artificial agents interact with human agents it is important that those artificial agents can reason about the trustworthiness and deceptive actions of the human counterpart. In order to support this interaction between agents in virtual societies, a theory on trust and deception must be developed. In the literature, a wide variety of theories on trust (less so on deception!) have been developed but not specifically for virtual communities. Based on these earlier scientific results, we make a first attempt to develop a general theory on trust and deception for virtual communities, and we discuss a number of examples to illustrate which objectives such a theory should fulfil.},
author = {Castelfranchi, C. and Tan, Yao-Hua Tan Yao-Hua},
doi = {10.1109/HICSS.2001.927042},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/09817011.pdf:pdf},
isbn = {0-7695-0981-9},
issn = {1086-4415},
journal = {Proceedings of the 34th Annual Hawaii International Conference on System Sciences},
keywords = {and phrases,computer,deception,is in the fact,it behaves in a,multiagent systems,perfectly honest way,put to work,that once programmed and,the inhumanity of the,trust,virtual society},
number = {3},
pages = {55--70},
title = {{The role of trust and deception in virtual societies}},
volume = {6},
year = {2001}
}
@article{Steinfeld2006,
author = {Steinfeld, A and Fong, T and Kaber, D},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/steinfeld{\_}aaron{\_}m{\_}2006{\_}1.pdf:pdf},
isbn = {1595932941},
journal = {{\ldots}  on Human-robot  {\ldots}},
keywords = {human-robot interaction,metrics,unmanned ground vehicles},
title = {{Common metrics for human-robot interaction}},
url = {http://dl.acm.org/citation.cfm?id=1121249},
year = {2006}
}
@article{Granatyr2015,
author = {Granatyr, Jones and Botelho, Vanderson and Lessing, Otto Robert and Scalabrin, Edson Em{\'{\i}}lio and Barth{\`{e}}s, Jean-Paul and Enembreck, Fabr{\'{\i}}cio},
doi = {10.1145/2816826},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/a27-granatyr.pdf:pdf},
issn = {03600300},
journal = {ACM Computing Surveys},
month = {oct},
number = {2},
pages = {1--42},
title = {{Trust and Reputation Models for Multiagent Systems}},
url = {http://dl.acm.org/citation.cfm?doid=2830539.2816826},
volume = {48},
year = {2015}
}
@article{Artz2007,
author = {Artz, Donovan and Gil, Yolanda},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/112-128-3-PB.pdf:pdf},
keywords = {policies,reputation,trust,web of trust},
title = {{A survey of trust in computer science and the Semantic Web}},
year = {2007}
}
@article{Abdul-rahman2000,
author = {Abdul-rahman, Alfarez and Hailes, Stephen},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/04936007.pdf:pdf},
isbn = {0769504930},
journal = {System Sciences, 2000. Proceedings of the 33rd Annual Hawaii International Conference on},
number = {c},
pages = {1--9},
title = {{Supporting Trust in Virtual Communities}},
volume = {00},
year = {2000}
}
@article{Huynh2006,
abstract = {Abstract Trust and reputation are central to effective interactions in open multi-agent systems (MAS) in which agents, that are owned by a variety of stakeholders, continuously enter and leave the system. This openness means existing trust and reputation models cannot readily be used since their performance suffers when there are various (unforseen) changes in the environment. To this end, this paper presents FIRE, a trust and reputation model that integrates a number of information sources to produce a comprehensive assessment of an agents likely performance in open systems. Specifically, FIRE incorporates interaction trust, role-based trust, witness reputation, and certified reputation to provide trust metrics in most circumstances. FIRE is empirically evaluated and is shown to help agents gain better utility (by effectively selecting appropriate interaction partners) than our benchmarks in a variety of agent populations. It is also shown that FIRE is able to effectively respond to changes that occur in an agents environment.},
annote = {AKA Fire},
author = {Huynh, Trung Dong and Jennings, Nicholas R. and Shadbolt, Nigel R.},
doi = {10.1007/s10458-005-6825-4},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/jaamas-dong.pdf:pdf},
isbn = {1387-2532},
issn = {13872532},
journal = {Autonomous Agents and Multi-Agent Systems},
keywords = {Multi-agent systems,Reputation,Trust},
number = {2},
pages = {119--154},
title = {{An integrated trust and reputation model for open multi-agent systems}},
volume = {13},
year = {2006}
}
@inproceedings{Walter2009,
abstract = {We propose a novel trust metric for social networks which is suitable for application in recommender systems. It is personalised and dynamic and allows to compute the indirect trust between two agents which are not neighbours based on the direct trust between agents that are neighbours. In analogy to some personalised versions of PageRank, this metric makes use of the concept of feedback centrality and overcomes some of the limitations of other trust metrics.In particular, it does not neglect cycles and other patterns characterising social networks, as some other algorithms do. In order to apply the metric to recommender systems, we propose a way to make trust dynamic over time. We show by means of analytical approximations and computer simulations that the metric has the desired properties. Finally, we carry out an empirical validation on a dataset crawled from an Internet community and compare the performance of a recommender system using our metric to one using collaborative filtering.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {0902.1475},
author = {Walter, Frank E. and Battiston, Stefano and Schweitzer, Frank},
booktitle = {Proceedings of the third ACM conference on Recommender systems - RecSys '09},
doi = {10.1145/1639714.1639747},
eprint = {0902.1475},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/0902.1475.pdf:pdf},
isbn = {9781605584355},
keywords = {information overload,personalisation,recommender,social networks,systems,trust},
pages = {197},
publisher = {ACM Press},
title = {{Personalised and dynamic trust in social networks}},
url = {http://arxiv.org/abs/0902.1475$\backslash$nhttp://portal.acm.org/citation.cfm?doid=1639714.1639747 http://portal.acm.org/citation.cfm?doid=1639714.1639747},
year = {2009}
}
@article{Bickmore2001,
abstract = {Building trust with users is crucial in a wide range of applications, such as financial transactions, and some minimal degree of trust is required in all applications to even initiate and maintain an interaction with a user. Humans use a variety of relational conversational strategies, including small talk, to establish trusting relationships with each other. We argue that such strategies can also be used by interface agents, and that embodied conversational agents are ideally suited for this task given the myriad cues available to them for signaling trustworthiness. We describe a model of social dialogue, an implementation in an embodied conversation agent, and an experiment in which social dialogue was demonstrated to have an effect on trust, for users with a disposition to be extroverts.},
annote = {ECAs - Embodied Conversational Agents
Describes a model to create agents capable of social dialogue, with speech and social strategies.
Should look more into it, specially for reference in agent technology.},
author = {Bickmore, Timothy and Cassell, Justine},
doi = {10.1145/365024.365304},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/IST/Thesis/Papers/CHI2001.pdf:pdf},
isbn = {1581133278},
journal = {Small},
keywords = {able establish social,eases,engage trust which,relationships with,sort must,turn,users order},
number = {3},
pages = {396--403},
title = {{Relational Agents : A Model and Implementation of Building User Trust}},
url = {http://dl.acm.org/citation.cfm?id=365024.365304},
volume = {3},
year = {2001}
}
