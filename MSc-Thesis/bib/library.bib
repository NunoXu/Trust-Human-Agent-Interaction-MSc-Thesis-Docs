Automatically generated by Mendeley Desktop 1.16.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{Henriques2016,
author = {Henriques, Bruno},
institution = {Instituto Superior T{\'{e}}cnico},
title = {{Rapport - Establishing Harmonious Relationship Between Robots and Humans}},
year = {2016}
}
@inproceedings{Walter2009,
abstract = {We propose a novel trust metric for social networks which is suitable for application in recommender systems. It is personalised and dynamic and allows to compute the indirect trust between two agents which are not neighbours based on the direct trust between agents that are neighbours. In analogy to some personalised versions of PageRank, this metric makes use of the concept of feedback centrality and overcomes some of the limitations of other trust metrics.In particular, it does not neglect cycles and other patterns characterising social networks, as some other algorithms do. In order to apply the metric to recommender systems, we propose a way to make trust dynamic over time. We show by means of analytical approximations and computer simulations that the metric has the desired properties. Finally, we carry out an empirical validation on a dataset crawled from an Internet community and compare the performance of a recommender system using our metric to one using collaborative filtering.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {0902.1475},
author = {Walter, Frank E. and Battiston, Stefano and Schweitzer, Frank},
booktitle = {Proceedings of the third ACM conference on Recommender systems - RecSys '09},
doi = {10.1145/1639714.1639747},
eprint = {0902.1475},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Walter, Battiston, Schweitzer/2009/Walter, Battiston, Schweitzer{\_}2009{\_}Personalised and dynamic trust in social networks.pdf:pdf},
isbn = {9781605584355},
keywords = {information overload,personalisation,recommender,social networks,systems,trust},
pages = {197},
publisher = {ACM Press},
title = {{Personalised and dynamic trust in social networks}},
url = {http://arxiv.org/abs/0902.1475$\backslash$nhttp://portal.acm.org/citation.cfm?doid=1639714.1639747 http://portal.acm.org/citation.cfm?doid=1639714.1639747},
year = {2009}
}
@article{Ramchurn2003,
annote = {Paper preceeding:
Devising a trust model for multi-agent interactions useing confidence and pain by the same authors, check other paper.},
author = {Ramchurn, D. Ramchurn and Jennings, Nicholas R. and Sierra, Carles and Godo, Llu{\'{i}}s},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Ramchurn et al/2003/Ramchurn et al.{\_}2003{\_}A Computational Trust Model for Multi-Agent Interactions based on Confidence and Reputation.pdf:pdf},
title = {{A Computational Trust Model for Multi-Agent Interactions based on Confidence and Reputation}},
year = {2003}
}
@article{Bickmore2005,
author = {Bickmore, Timothy W and Picard, ROSALIND W.},
doi = {10.1145/1067860.1067867},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Bickmore, Picard/2005/Bickmore, Picard{\_}2005{\_}Establishing and Maintaining Long-Term Human-Computer Relationships.pdf:pdf},
issn = {10730516},
journal = {ACM Transactions on Computer-Human},
number = {2},
pages = {293--327},
title = {{Establishing and Maintaining Long-Term Human-Computer Relationships}},
volume = {12},
year = {2005}
}
@article{Burgoon2000,
abstract = {Advancements in computer technology have allowed the development of human-appearing and -behaving virtual agents. This study examined if increased richness and anthropomorphism in interface design lead to computers being more influential during a decision-making task with a human partner. In addition, user experiences of the communication format, communication process, and the task partner were evaluated for their association with various features of virtual agents. Study participants completed the Desert Survival Problem (DSP) and were then randomly assigned to one of five different computer partners or to a human partner (who was a study confederate). Participants discussed each of the items in the DSP with their partners and were then asked to complete the DSP again. Results showed that computers were more influential than human partners but that the latter were rated more positively on social dimensions of communication than the former. Exploratory analysis of user assessments revealed that some features of human-computer interaction (e.g. utility and feeling understood) were associated with increases in anthropomorphic features of the interface. Discussion focuses on the relation between user perceptions, design features, and task outcomes.},
author = {Burgoon, J.K and Bonito, J.A and Bengtsson, B. and Cederberg, C. and Lundeberg, M. and Allspach, L.},
doi = {10.1016/S0747-5632(00)00029-7},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Burgoon et al/2000/Burgoon et al.{\_}2000{\_}Interactivity in human–computer interaction a study of credibility, understanding, and influence.pdf:pdf},
isbn = {0747-5632},
issn = {07475632},
journal = {Computers in Human Behavior},
keywords = {computer,human},
month = {nov},
number = {6},
pages = {553--574},
title = {{Interactivity in human–computer interaction: a study of credibility, understanding, and influence}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0747563200000297},
volume = {16},
year = {2000}
}
@article{Singh2011,
author = {Singh, MP},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Singh/2011/Singh{\_}2011{\_}Trust as dependence a logical approach.pdf:pdf},
isbn = {978-0-9826571-5-7},
journal = {The 10th International Conference on Autonomous {\ldots}},
keywords = {commitments,service-oriented computing,trust},
pages = {863--870},
title = {{Trust as dependence: a logical approach}},
url = {http://dl.acm.org/citation.cfm?id=2031741},
year = {2011}
}
@article{VandenBrule2014,
abstract = {An important aspect of a robot's social be- havior is to convey the right amount of trustworthi- ness. Task performance has shown to be an important source for trustworthiness judgments. Here, we argue that factors such as a robot's behavioral style, can play an important role as well. Our approach to studying the effects of a robot's performance and behavioral style on human trust involves experiments with simulated robots in Video Human-Robot Interaction (VHRI) and Immersive Virtual Environments (IVE). Although VHRI and IVE settings cannot substi- tute for the genuine interaction with a real robot, they can provide useful complementary approaches to exper- imental research in social Human Robot Interaction. VHRI enables rapid prototyping of robot behaviors. Simulating Human-Robot Interaction in IVEs can be a useful tool for measuring human responses to robots and help avoid the many constraints caused by real- world hardware. However, there are also difficulties with the generalization of results from one setting (e.g.,VHRI) to another (e.g. IVE or the real world), which we dis- cuss. In this paper, we use animated robot avatars in VHRI to rapidly identify robot behavioral styles that affect human trust assessment of the robot. In a subse- quent study, we use an IVE tomeasure behavioral inter- action between humans and an animated robot avatar equipped with behaviors from the VHRI experiment. Our findings reconfirm that a robot's task performance R. van den Brule {\textperiodcentered} W.F.G. Haselager Donders Institute for Brain, Cognition and Behaviour, Rad- boud University Nijmegen, Montessorilaan 3, 6525 HR Nij- megen E-mail: r.vandenbrule@donders.ru.nl R. van den Brule {\textperiodcentered} R. Dotsch {\textperiodcentered} G. Bijlstra {\textperiodcentered} D.H.J. Wigboldus Behavioural Science Institute, Radboud University Nijmegen, Montessorilaan 3, 6525 HR Nijmegen influences its trustworthiness, but the effect of the be- havioral style identified in the VHRI study did not in- fluence the robot's trustworthiness in the IVE study.},
author = {van den Brule, R. and Dotsch, R. and Bijlstra, G. and Wigboldus, D. H. J. and Haselager, W. F. G.},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Brule et al/2014/Brule et al.{\_}2014{\_}Do Robot Performance and Behavioral Style affect Human Trust.pdf:pdf},
journal = {International Journal of Social Robotics},
keywords = {immersive virtual environments,social robotics,trust,video stimuli},
title = {{Do Robot Performance and Behavioral Style affect Human Trust ?}},
year = {2014}
}
@inproceedings{Bickmore2001b,
abstract = {What kinds of social relationships can people have with computers? Are there activities that computers can engage in that actively draw people into relationships with them? What are the potential benefits to the people who participate in these human-computer relationships? To address these questions this work introduces a theory of Relational Agents, which are computational artifacts designed to build and maintain long-term, social-emotional relationships with their users. These can be purely software humanoid animated agents-as developed in this work-but they can also be non-humanoid or embodied in various physical forms, from robots, to pets, to jewelry, clothing, hand-helds, and other interactive devices. Central to the notion of relationship is that it is a persistent construct, spanning multiple interactions; thus, Relational Agents are explicitly designed to remember past history and manage future expectations in their interactions with users. Finally, relationships are fundamentally social and emotional, and detailed knowledge of human social psychology-with a particular emphasis on the role of affect-must be incorporated into these agents if they are to effectively leverage the mechanisms of human social cognition in order to build relationships in the most natural manner possible. People build relationships primarily through the use of language, and primarily within the context of face-to-face conversation. Embodied Conversational Agents-anthropomorphic computer characters that emulate the experience of face-to-face conversation-thus provide the substrate for this work, and so the relational activities provided by the theory will primarily be specific types of verbal and nonverbal conversational behaviors used by people to negotiate and maintain relationships. This work also provides an analysis of the types of applications in which having a human-computer relationship is advantageous to the human participant. In addition to applications in which the relationship is an end in itself (e.g., in entertainment systems), human-computer relationships are important in tasks in which the human is attempting to undergo some change in behavior or cognitive or emotional state. One such application is explored here: a system for assisting the user through a month-long health behavior change program in the area of exercise adoption. This application involves the research, design and implementation of relational agents as well as empirical evaluation of their ability to build relationships and effect change over a series of interactions with users.},
address = {New York, New York, USA},
annote = {Defines Relational Agents - agents designed to build long-term relationships with users.

Maybe use as a concept for the project's agent model?},
author = {Bickmore, Timothy and Cassell, Justine},
booktitle = {Proceedings of the SIGCHI conference on Human factors in computing systems - CHI '01},
doi = {10.1145/365024.365304},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Bickmore, Cassell/2001/Bickmore, Cassell{\_}2001{\_}Relational agents(2).pdf:pdf},
isbn = {1581133278},
keywords = {able to establish social,eases,engage their trust which,in turn,relationships with,this sort must be,users in order to},
number = {3},
pages = {396--403},
publisher = {ACM Press},
title = {{Relational agents}},
url = {http://portal.acm.org/citation.cfm?doid=365024.365304},
volume = {PhD Thesis},
year = {2001}
}
@article{Sutcliffe2012,
author = {Sutcliffe, Alistair and Wang, Di},
doi = {10.18564/jasss.1912},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Sutcliffe, Wang/2012/Sutcliffe, Wang{\_}2012{\_}Computational Modelling of Trust and Social Relationships.pdf:pdf},
issn = {1460-7425},
journal = {Journal of Artificial Societies and Social Simulation},
keywords = {social agents,social modelling,social networks,trust},
month = {aug},
number = {1},
pages = {523--531},
title = {{Computational Modelling of Trust and Social Relationships}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0022519303001437 http://jasss.soc.surrey.ac.uk/15/1/3.html},
volume = {15},
year = {2012}
}
@book{Russell2009a,
abstract = {The long-anticipated revision of this {\#}1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications. Intelligent Agents. Solving Problems by Searching. Informed},
author = {Russell, Stuart and Norvig, Peter},
booktitle = {Prentice Hall},
doi = {10.1017/S0269888900007724},
isbn = {0136042597},
issn = {0269-8889},
pages = {1 -- 1132},
title = {{Artificial Intelligence: A Modern Approach, 3rd edition}},
url = {http://portal.acm.org/citation.cfm?id=1671238{\&}coll=DL{\&}dl=GUIDE{\&}CFID=190864501{\&}CFTOKEN=29051579$\backslash$npapers2://publication/uuid/4B787E16-89F6-4FF7-A5E5-E59F3CFEFE88},
year = {2009}
}
@article{Lee2004,
annote = {Describes older literature on Trust, focused on the psycological aspect. 
Good citation for diferent perspective on trust, besides computational trust},
author = {Lee, John D. and See, Katrina A.},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Lee, See/2004/Lee, See{\_}2004{\_}Trust in Automation Designing for Appropriate Reliance.pdf:pdf},
number = {1},
pages = {50--80},
title = {{Trust in Automation : Designing for Appropriate Reliance}},
volume = {46},
year = {2004}
}
@article{Pinto2008,
abstract = {No trabalho},
author = {Pinto, Santos},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Pinto/2008/Pinto{\_}2008{\_}Simula{\c{c}}{\~{a}}o e Avalia{\c{c}}{\~{a}}o de Comportamentos em Sistemas Multi-Agentes baseados em Modelos de Reputa{\c{c}}{\~{a}}o e Intera{\c{c}}{\~{a}}o.pdf:pdf},
keywords = {behavioral simulation,cooperation,group interation  simulation,multi-agent systems,reputation},
pages = {99},
title = {{Simula{\c{c}}{\~{a}}o e Avalia{\c{c}}{\~{a}}o de Comportamentos em Sistemas Multi-Agentes baseados em Modelos de Reputa{\c{c}}{\~{a}}o e Intera{\c{c}}{\~{a}}o}},
url = {http://www.unissinos.br},
volume = {2},
year = {2008}
}
@misc{Maia,
author = {Maia, Nuno},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Maia/Unknown/Maia{\_}Unknown{\_}I Play , You Play A Framework for Card Games in a Multi-touch Table.pdf:pdf},
keywords = {card games,card games framework,fiducial markers,multi-,social robots,touch table},
pages = {1--32},
title = {{I Play , You Play : A Framework for Card Games in a Multi-touch Table}}
}
@article{Ramchurn2004a,
abstract = {Trust is a fundamental concern in large-scale open distributed systems. It lies at the core of all interactions between the entities that have to operate in such uncertain and constantly changing environments. Given this complexity, these components, and the ensuing system, are increasingly being conceptualised, designed, and built using agent-based techniques and, to this end, this paper examines the specific role of trust in multi-agent systems. In particular, we survey the state of the art and provide an account of the main directions along which research efforts are being focused. In so doing, we critically evaluate the relative strengths and weaknesses of the main models that have been proposed and show how, fundamentally, they all seek to minimise the uncertainty in interactions. Finally, we outline the areas that require further research in order to develop a comprehensive treatment of trust in complex computational settings.},
annote = {Survey about the state on Trust Models in MAS.
Useful for inspiration for related work and background.
Oldest review though},
author = {Ramchurn, S.D. and Huynh, T.D. and Jennings, N. R.},
doi = {10.1017/S0269888904000116},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Ramchurn, Huynh, Jennings/2004/Ramchurn, Huynh, Jennings{\_}2004{\_}Trust in Multiagent Systems.pdf:pdf},
isbn = {0269-8889},
issn = {0269-8889},
journal = {The Knowledge Engineering Review},
number = {1},
pages = {1--25},
pmid = {1767140064670383174},
title = {{Trust in Multiagent Systems}},
url = {http://eprints.soton.ac.uk/259564/},
volume = {19},
year = {2004}
}
@article{Nash1951,
author = {Nash, John},
doi = {10.2307/1969529},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Nash/1951/Nash{\_}1951{\_}Non-Cooperative Games.pdf:pdf},
issn = {0003486X},
journal = {The Annals of Mathematics},
month = {sep},
number = {2},
pages = {286},
title = {{Non-Cooperative Games}},
url = {http://www.jstor.org/stable/1969529?origin=crossref},
volume = {54},
year = {1951}
}
@article{Lewis1998,
abstract = {Interacting with a computer requires adopting some metaphor to guide our actions and expectations. Most human-computer interfaces can be classified according to two dominant metaphors: (1) agent and (2) environment. Interactions based on an agent metaphor treat the computer as an intermediary that responds to user requests. In the environment metaphor, a model of the task domain is presented for the user to interact with directly. The term agent has come to refer to the automation of aspects of human-computer interaction (HCI), such as anticipating commands or autonomously performing actions. Norman's 1984 model of HCI is introduced as reference to organize and evaluate research in human-agent interaction (HAI). A wide variety of heterogeneous research involving HAI is shown to reflect automation of one of the stages of action or evaluation within Norman's model. Improvements in HAI are expected to result from a more heterogeneous use of methods that target multiple stages simultaneously.},
author = {Lewis, Michael},
doi = {10.1609/aimag.v19i2.1369},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Lewis/1998/Lewis{\_}1998{\_}Designing for Human-Agent Interaction.pdf:pdf},
issn = {0738-4602},
journal = {AI Magazine},
number = {2},
pages = {67},
title = {{Designing for Human-Agent Interaction}},
url = {http://www.aaai.org/ojs/index.php/aimagazine/article/view/1369$\backslash$nhttp://www.aaai.org/ojs/index.php/aimagazine/article/download/1369/1269},
volume = {19},
year = {1998}
}
@article{Esfandiari2001,
abstract = {We need models of trust to facilitate cooperation in multi-agent systems, where agents, human and artificial, do not know each other beforehand. This paper lists and proposes simple mechanisms for trust acquisition based on a very basic and general definition of trust, making no assumptions on the internal cognitive models of the involved agents. We also show how trust acquired one-on-one can be propagated in a social network of agents.},
annote = {Small paper describing the different ways trust can be acquired. Is also described in other papers and seems a bit irrelevant. Cited over 100 though, so maybe just reference for the work done?

Reference to the ways of acquiring trust.},
author = {Esfandiari, Babak and Chandrasekharan, Sanjay},
doi = {10.1.1.11.4683},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Esfandiari, Chandrasekharan/2001/Esfandiari, Chandrasekharan{\_}2001{\_}On how agents make friends Mechanisms for trust acquisition.pdf:pdf},
journal = {Proceedings of the Fourth Workshop on Deception Fraud and Trust in Agent Societies Montreal Canada},
keywords = {Mechanisms for Trust},
number = {19th of June},
pages = {27--34},
title = {{On how agents make friends: Mechanisms for trust acquisition}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.11.4683{\&}rep=rep1{\&}type=pdf},
volume = {222},
year = {2001}
}
@inproceedings{Salem2015b,
abstract = {How do mistakes made by a robot affect its trustworthiness and acceptance in human-robot collaboration? We investi- gate how the perception of erroneous robot behavior may influence human interaction choices and the willingness to cooperate with the robot by following a number of its un- usual requests. For this purpose, we conducted an exper- iment in which participants interacted with a home com- panion robot in one of two experimental conditions: (1) the correct mode or (2) the faulty mode. Our findings reveal that, while significantly affecting subjective perceptions of the robot and assessments of its reliability and trustworthi- ness, the robot's performance does not seem to substantially influence participants' decisions to (not) comply with its re- quests. However, our results further suggest that the nature of the task requested by the robot, e.g. whether its effects are revocable as opposed to irrevocable, has a significant im- pact on participants' willingness to follow its instructions.},
address = {New York, New York, USA},
author = {Salem, Maha and Lakatos, Gabriella and Amirabdollahian, Farshid and Dautenhahn, Kerstin},
booktitle = {Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction - HRI '15},
doi = {10.1145/2696454.2696497},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Salem et al/2015/Salem et al.{\_}2015{\_}Would You Trust a (Faulty) Robot.pdf:pdf},
isbn = {9781450328838},
issn = {21672148},
keywords = {cooperation,social human-robot interaction,trust},
pages = {141--148},
publisher = {ACM Press},
title = {{Would You Trust a (Faulty) Robot?}},
url = {http://dl.acm.org/citation.cfm?doid=2696454.2696497},
year = {2015}
}
@article{Reeves1998,
abstract = {Fresh evidence of human gullibility never fails to entertain. Stanford professors Reeves and Nass provide plenty of cocktail-party ammunition with findings from 35 laboratory experiments demonstrating how even technologically sophisticated people treat boxes of circuitry as if they were other human beings. People are polite to computers, respond to praise from them and view them as teammates. They like computers with personalities similar to their own, find masculine-sounding computers extroverted, driven and intelligent while they judge feminine-sounding computers knowledgeable about love and relationships. Viewers rate content on a TV embellished with the label "specialist" superior to identical content on a TV labeled "generalist" (they even found the picture clearer on the "specialist" box). Reeves and Nass, who combine expertise in fine arts, communications, math, sociology, television and computers, were consultants to the world's foremost software corporation on the creation of the Microsoft Bob software package. Not surprisingly, their breezy tone and emphasis on the benign practical applications of their discoveries give their discussion an optimistic bias. Why not make media easier to use and more fun? Yet, their more important contribution may lie in alerting us to specific media dangers. The evidence of our suggestibility offers particularly powerful new arguments for monitoring children's television. And if the mere number of rapid-fire visual cuts in political advertisements really correlates with an impression of honesty, intelligence and sincerity, the more viewers who are put on guard, the better.},
author = {Martin, C.D.},
doi = {10.1109/MSPEC.1997.576013},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Martin/1997/Martin{\_}1997{\_}The Media Equation How People Treat Computers, Television and New Media Like Real People and Places Book Review.pdf:pdf},
isbn = {1575860538},
issn = {0018-9235},
journal = {IEEE Spectrum},
month = {mar},
number = {3},
pages = {9--10},
pmid = {186262},
title = {{The Media Equation: How People Treat Computers, Television and New Media Like Real People and Places [Book Review]}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=576013},
volume = {34},
year = {1997}
}
@article{Katagiri2001,
annote = {Tries to perform social persuasion but with no conclusive results.

Not very interesting},
author = {Katagiri, Yasuhiro and Takahashi, Toru and Takeuchi, Yugo},
doi = {10.1.1.23.6671},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Katagiri, Takahashi, Takeuchi/2001/Katagiri, Takahashi, Takeuchi{\_}2001{\_}Social Persuasion in Human-Agent Interaction.pdf:pdf},
journal = {Interactions},
title = {{Social Persuasion in Human-Agent Interaction}},
year = {2001}
}
@article{Simpson2007a,
abstract = {Trust lies at the foundation of nearly all major theories of interpersonal relationships. Despite its great theoretical importance, a limited amount of research has examined how and why trust develops, is maintained, and occasionally unravels in relationships. Following a brief overview of theoretical and empirical milestones in the interpersonal-trust literature, an integrative process model of trust in dyadic relationships is presented},
author = {Simpson, Jeffry A.},
doi = {10.1111/j.1467-8721.2007.00517.x},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Simpson/2007/Simpson{\_}2007{\_}Psychological Foundations of Trust.pdf:pdf},
isbn = {0-205-33144-0},
issn = {0963-7214},
journal = {Current Directions in Psychological Science},
keywords = {felt security,interdependence,strain tests,trust},
month = {oct},
number = {5},
pages = {264--268},
title = {{Psychological Foundations of Trust}},
url = {http://cdp.sagepub.com/lookup/doi/10.1111/j.1467-8721.2007.00517.x},
volume = {16},
year = {2007}
}
@article{Rau2010,
abstract = {Abstract This study investigates the effects of culture, ro- bot appearance and task on human-robot interaction. We propose a model with culture (Chinese, Korean and Ger- man), robot appearance (anthropomorphic, zoomorphic and machinelike) and task (teaching, guide, entertainment and security guard) as factors, and analyze these factors effects on the robots likeability, and peoples active response to, engagement with, trust in and satisfaction with the robot. We conducted a laboratory experiment with 108 participants to test the model and performed Repeated ANOVA and Kruskal Wallis Test on the data. The results show that cul- tural differences exist in participants perception of likeabil- ity, engagement, trust and satisfaction; a robots appearance affects its likeability, while the task affects participants ac- tive response and engagement.We found the participants ex- pected the robot appearance to match its task only in the in- terviewbut not in the subjective ratings. Interaction between culture and task indicates that participants from low-context culturesmay have significantly decreased engagement when the sociability of a task is lowered. We found strong and positive correlations between interaction performance (ac- tive response and engagement) and preference (likeability, trust and satisfaction) in the human-robot interaction.},
author = {Li, Dingjun and Rau, P. L Patrick and Li, Ye},
doi = {10.1007/s12369-010-0056-9},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Li, Rau, Li/2010/Li, Rau, Li{\_}2010{\_}A Cross-cultural Study Effect of Robot Appearance and Task.pdf:pdf},
isbn = {1236901000569},
issn = {1875-4791},
journal = {International Journal of Social Robotics},
keywords = {Cultural differences,Human robot interaction,Robot appearance,Robot task,Social robot},
month = {jun},
number = {2},
pages = {175--186},
title = {{A Cross-cultural Study: Effect of Robot Appearance and Task}},
url = {http://link.springer.com/10.1007/s12369-010-0056-9},
volume = {2},
year = {2010}
}
@article{Ganesan1997,
abstract = {Previous research has found that trust is positively related to commitment in buyer-seller relationships. However, the validity of this finding is questionable because trust has been operationalized in many different ways. For example, prior research has not distinguished among levels of trust (interpersonal or organizational trust) and dimensions or motives of trust (credibility or benevolence). In this study, we distinguish among the levels and dimensions of trust. The results indicate that trust in a sales representative (interpersonal credibility) is more strongly related to commitment than trust in an organization (organizational credibility). In contrast, trust based on organizational benevolence is a stronger predictor of commitment than interpersonal benevolence.},
archivePrefix = {arXiv},
arxivId = {arXiv:astro-ph/0005074v1},
author = {Ganesan, Shankar and Hess, Ron},
doi = {10.1023/A:1007955514781},
eprint = {0005074v1},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Ganesan, Hess/1997/Ganesan, Hess{\_}1997{\_}Dimensions and Levels of Trust Implications for Commitment to a Relationship.pdf:pdf},
isbn = {10.1023/A:1007955514781},
issn = {0923-0645, 1573-059X},
journal = {Marketing Letters},
keywords = {1994,along with commitment are,benevolence,commitment,credibility,despite the,for the success of,have argued that trust,in marketing,in marketing and organizational,interfirm alliances,interpersonal trust,is generating increased interest,keys,morgan and hunt,organizational trust,studies,the topic of trust,to cooperative behaviors essential},
number = {4},
pages = {439--448},
pmid = {1284},
primaryClass = {arXiv:astro-ph},
title = {{Dimensions and Levels of Trust: Implications for Commitment to a Relationship}},
url = {http://link.springer.com/article/10.1023/A:1007955514781$\backslash$nhttp://link.springer.com/article/10.1023/A:1007955514781$\backslash$nhttp://link.springer.com/content/pdf/10.1023/A:1007955514781.pdf},
volume = {8},
year = {1997}
}
@article{Antos2011,
author = {Antos, Dimitrios and Melo, Celso De and Gratch, Jonathan and Grosz, Barbara J},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Antos et al/2011/Antos et al.{\_}2011{\_}The Influence of Emotion Expression on Perceptions of Trustworthiness in Negotiation.pdf:pdf},
isbn = {9781577355083},
journal = {Aaai},
keywords = {Multidisciplinary Topics},
pages = {772--778},
title = {{The Influence of Emotion Expression on Perceptions of Trustworthiness in Negotiation.}},
url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/viewPDFInterstitial/3438/3951},
year = {2011}
}
@article{Artz2007,
author = {Artz, Donovan and Gil, Yolanda},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Artz, Gil/2007/Artz, Gil{\_}2007{\_}A survey of trust in computer science and the Semantic Web.pdf:pdf},
keywords = {policies,reputation,trust,web of trust},
title = {{A survey of trust in computer science and the Semantic Web}},
year = {2007}
}
@article{Pinyol2009,
annote = {BDI + Repage},
author = {Pinyol, Isaac},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Pinyol/2009/Pinyol{\_}2009{\_}Reputation-Based Decisions for Cognitive Agents (Thesis Abstract).pdf:pdf},
journal = {Doctoral Mentoring Program},
keywords = {agents,utation-based decisions for cognitive},
number = {Aamas},
pages = {33},
title = {{Reputation-Based Decisions for Cognitive Agents (Thesis Abstract)}},
url = {http://ifaamas.org/Proceedings/aamas09/pdf/07{\_}Doctoral/Doct{\_}08.pdf},
year = {2009}
}
@phdthesis{Correia,
abstract = {The computational complexity of some card games attract the interest of Artificial Intelligence (AI) researchers. Their main challenge is to deal with hidden information, nonetheless recent approaches start to overcome this problem, such as Monte-Carlo Methods. On the other hand, the strong social component every multi-player game presents can also be included in an artificial player through an embodied agent that interacts with other players. Therefore, this thesis proposes the development of a social Sueca player that is able of both playing the game and communicate with human players, enhancing their game experience. This agent includes an AI module able of deciding which card to play, based on Perfect Information Monte-Carlo (PIMC) algorithm. Furthermore, in order to be socially present during the game, this agent also contains a decision maker module able of evaluating the game state and producing adequate verbal or non-verbal behaviours. Finally, user studies revealed significant comparisons to human players that encourage future development of this work.},
author = {Correia, Filipa},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Correia/2015/Correia{\_}2015{\_}EMYS a social robot that plays “Sueca”.pdf:pdf},
keywords = {Artificial Intelligence,Hidden Information,Interactive Companions,Socially Intelligent Behaviour,Trick-taking Card Game},
number = {November},
pages = {80},
school = {Instituto Siuperior T{\'{e}}cnico},
title = {{EMYS: a social robot that plays “Sueca”}},
year = {2015}
}
@article{Mui2002,
annote = {Trust model focused on e-commerce},
author = {Mui, Lik and Mohtashemi, Mojdeh and Halberstadt, Ari},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Mui, Mohtashemi, Halberstadt/2002/Mui, Mohtashemi, Halberstadt{\_}2002{\_}A Computational Model of Trust and Reputation.pdf:pdf},
number = {c},
pages = {1--9},
title = {{A Computational Model of Trust and Reputation}},
volume = {00},
year = {2002}
}
@misc{Carvalho,
author = {Carvalho, Andr{\'{e}}},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Carvalho/Unknown/Carvalho{\_}Unknown{\_}Laugh To Me An Affective Narrative Approach to Computational Humour.pdf:pdf},
keywords = {computational humour,humour,interac-,interactive storytelling,sketch,slapstick,tive comedy},
title = {{Laugh To Me : An Affective Narrative Approach to Computational Humour}}
}
@book{Ide2005,
abstract = {The main topic of this volume is natural multimodal interaction. The book is unique in that it brings together a great many contributions regarding aspects of natural and multimodal interaction written by many of the important actors in the field. It is a timely update of Multimodality in Language and Speech Systems by Bj{\"{o}}rn Granstr{\"{o}}m, David House and Inger Karlsson and, at the same time, it presents a much broader overview of the field. Its 17 chapters provide a broad and detailed impression of where the fairly new field of natural and multimodal interactivity engineering stands today. Topics addressed include talking heads, conversational agents, tutoring systems, multimodal communication, machine learning, architectures for multimodal dialogue systems, systems evaluation, and data annotation. This title will prove very valuable to scientists, researchers and practitioners working in the fields of natural interactive systems, multimodal systems, conversational agents, spoken dialogue systems, natural language processing applications, advanced human-computer interfaces, analysis of multimodal data, evaluation of multimodal systems, and educational systems.},
author = {Ide, Nancy and V{\'{e}}ronis, Jean and Kuppevelt, Jan C J and Dybkj{\ae}r, Laila and Bernsen, Niels Ole and Martell, Craig},
booktitle = {New York},
doi = {10.1007/1-4020-3933-6},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Ide et al/2005/Ide et al.{\_}2005{\_}Advances in Natural Multimodal Dialogue Systems.pdf:pdf},
isbn = {1402039328},
issn = {08912017},
number = {December 2004},
pages = {23--54},
title = {{Advances in Natural Multimodal Dialogue Systems}},
url = {http://www.springerlink.com/content/r4qr47070k50l098/},
volume = {30},
year = {2005}
}
@article{Sabater2005,
abstract = {The scientific research in the area of computational mechanisms for trust and reputation in virtual societies is a recent discipline oriented to increase the reliability and performance of electronic communities. Computer science has moved from the paradigm of isolated machines to the paradigm of networks and distributed computing. Likewise, artificial intelligence is quickly moving from the paradigm of isolated and non-situated intelligence to the paradigm of situated, social and collective intelligence. The new paradigm of the so called intelligent or autonomous agents and multi-agent systems (MAS) together with the spectacular emergence of the information society technologies (specially reflected by the popularization of electronic commerce) are responsible for the increasing interest on trust and reputation mechanisms applied to electronic societies. This review wants to offer a panoramic view on current computational trust and reputation models.},
author = {Sabater, Jordi and Sierra, Carles},
doi = {10.1007/s10462-004-0041-5},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Sabater, Sierra/2005/Sabater, Sierra{\_}2005{\_}Review on computational trust and reputation models.pdf:pdf},
isbn = {02692821 (ISSN)},
issn = {02692821},
journal = {Artificial Intelligence Review},
keywords = {Reputation,Trust},
number = {1},
pages = {33--60},
title = {{Review on computational trust and reputation models}},
volume = {24},
year = {2005}
}
@article{Sabater-Mir2007,
abstract = {Interest for computational trust and reputation models is on the rise. One of the most important aspects of these models is how they deal with information received from other individuals. More generally, the critical choice is how to represent and how to aggregate social evaluations. In this article, we make an analysis of the current approaches of representation and aggregation of social evaluations under the guidelines of a set of basic requirements. Then we present two different proposals of dealing with uncertainty in the context of the Repage system [J. Sabater, M. Paolucci, R. Conte, Repage: Reputation and image among limited autonomous partners, Journal of Artificial Societies and Social Simulation 9 (2). URL http://jasss.soc.surrey.ac.uk/9/2/3.html], a computational module for management of reputational information based on a cognitive model of imAGE, REPutation and their interplay already developed by the authors. We finally discuss these two proposals in the context of several examples. ?? 2007 Elsevier Inc. All rights reserved.},
author = {Sabater-Mir, Jordi and Paolucci, Mario},
doi = {10.1016/j.ijar.2006.12.013},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Sabater-Mir, Paolucci/2007/Sabater-Mir, Paolucci{\_}2007{\_}On representation and aggregation of social evaluations in computational trust and reputation models.pdf:pdf},
isbn = {0888-613X},
issn = {0888613X},
journal = {International Journal of Approximate Reasoning},
keywords = {Aggregation mechanisms,Reputation,Trust},
number = {3},
pages = {458--483},
title = {{On representation and aggregation of social evaluations in computational trust and reputation models}},
volume = {46},
year = {2007}
}
@inproceedings{Yu2001,
abstract = {Automated trust negotiation is an approach to establishing trust between strangers through the exchange of digital cre- dentials and the use of access control policies that specify what combinations of credentials a stranger must disclose in order to gain access to each local service or credential. We introduce the concept of a trust negotiation protocol, which defines the ordering of messages and the type of in- formation messages will contain. To carry out trust nego- tiation, a party pairs its negotiation protocol with a trust negotiation strategy that controls the exact content of the messages, i.e., which credentials to disclose, when to dis- close them, and when to terminate a negotiation. There are a huge number of possible strategies for negotiating trust, each with different properties with respect to speed of nego- tiations and caution in giving out credentials and policies. In the autonomous world of the Internet, entities will want the freedom to choose negotiation strategies that meet their own goals, which means that two strangers who negotiate trust will often not use the same strategy. To date, only a tiny fraction of the space of possible negotiation strategies has been explored, and no two of the strategies proposed so far will interoperate. In this paper, we define a large set of strategies called the disclosure tree strategy (DTS) fam- ily. Then we prove that if two parties each choose strategies from the DTS family, then they will be able to negotiate trust as well as if they were both using the same strategy. Further, they can change strategies at any point during ne- gotiation. We also show that the DTS family is closed, i.e., any strategy that can interoperate with every strategy in the DTS family must also be a member of the DTS family. We also give examples of practical strategies that belong to the DTS family and fit within the TrustBuilder architecture and protocol for trust negotiation.},
address = {New York, New York, USA},
annote = {Defines a family of trust negiotiation strategies, Disclosure tree strategy (DTS).

Focused on e-security aspect of Trust, not the focus of the project.},
author = {Yu, Ting and Winslett, Marianne and Seamons, Kent E.},
booktitle = {Proceedings of the 8th ACM conference on Computer and Communications Security - CCS '01},
doi = {10.1145/501983.502004},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Yu, Winslett, Seamons/2001/Yu, Winslett, Seamons{\_}2001{\_}Interoperable strategies in automated trust negotiation.pdf:pdf},
isbn = {1581133855},
pages = {146----155},
publisher = {ACM Press},
title = {{Interoperable strategies in automated trust negotiation}},
url = {http://portal.acm.org/citation.cfm?doid=501983.502004},
year = {2001}
}
@phdthesis{Marsh1994,
abstract = {Trust is a judgement of unquestionable utility as humans we use it every day of our lives. However, trust has suffered from an imperfect understanding, a plethora of definitions, and informal use in the literature and in everyday life. It is common to say I trust you, but what does that mean? This thesis provides a clarification of trust. We present a formalism for trust which provides us with a tool for precise discussion. The formalism is implementable: it can be embedded in an artificial agent, enabling the agent to make trust-based decisions. Its applicability in the domain of Distributed Artificial Intelligence (DAI) is raised. The thesis presents a testbed populated by simple trusting agents which substantiates the utility of the formalism. The formalism provides a step in the direction of a proper understanding and definition of human trust. A contribution of the thesis is its detailed exploration of the possibilities of future work in the area.},
annote = {Gives an early definition of reputation},
author = {Marsh, Stephen Paul},
booktitle = {Computing Science and Mathematics eTheses},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Marsh/1994/Marsh{\_}1994{\_}Formalising Trust as a Computational Concept.pdf:pdf},
month = {apr},
pages = {184},
title = {{Formalising Trust as a Computational Concept}},
volume = {NA},
year = {1994}
}
@article{Yu2003,
abstract = {Business and military partners, companies and their customers, and other closely cooperating parties may have a compelling need to conduct sensitive interactions on line, such as accessing each other's local services and other local resources. Automated trust negotiation is an approach to establishing trust between parties so that such interactions can take place, through the use of access control policies that specify what combinations of digital credentials a stranger must disclose to gain access to a local resource. A party can use many different strategies to negotiate trust, offering tradeoffs between the length of the negotiation, the amount of extraneous information disclosed, and the computational effort expended. To preserve parties' autonomy, each party should ideally be able to choose its negotiation strategy independently, while still being guaranteed that negotiations will succeed whenever possible---that the two parties' strategies will interoperate. In this paper we provide the formal underpinnings for that goal, by formalizing the concepts of negotiation protocols, strategies, and interoperation. We show how to model the information flow of a negotiation for use in analyzing strategy interoperation. We also present two large sets of strategies whose members all interoperate with one another, and show that these sets contain many practical strategies. We develop the theory for black-box propositional credentials as well as credentials with internal structure, and for access control policies whose contents are (respectively are not) sensitive. We also discuss how these results fit into TrustBuilder, our prototype system for trust negotiation.},
annote = {security applicaction of trust, mostly between arranging a contract between the parties.

not that interesting besides work reference},
author = {Yu, Ting and Winslett, Marianne and Seamons, Kent E.},
doi = {10.1145/605434.605435},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Yu, Winslett, Seamons/2003/Yu, Winslett, Seamons{\_}2003{\_}Supporting structured credentials and sensitive policies through interoperable strategies for automated trust.pdf:pdf},
isbn = {1094-9224},
issn = {10949224},
journal = {ACM Transactions on Information and System Security},
number = {1},
pages = {1--42},
title = {{Supporting structured credentials and sensitive policies through interoperable strategies for automated trust negotiation}},
volume = {6},
year = {2003}
}
@article{Pinyol2013,
abstract = {In open environments, agents depend on reputation and trust mechanisms to evaluate the behavior of potential partners. The scientific research in this field has considerably increased, and in fact, reputation and trust mechanisms have been already considered a key elements in the design of multi-agent systems. In this paper we provide a survey that, far from being exhaustive, intends to show the most representative models that currently exist in the literature. For this enterprise we consider several dimensions of analysis that appeared in three existing surveys, and provide new dimensions that can be complementary to the existing ones and that have not been treated directly. Moreover, besides showing the original classification that each one of the surveys provide, we also classify models that where not taken into account by the original surveys. The paper illustrates the proliferation in the past few years of models that follow a more cognitive approach, in which trust and reputation representation as mental attitudes is as important as the final values of trust and reputation. Furthermore, we provide an objective definition of trust, based on Castelfranchi's idea that trust implies a decision to rely on someone. {\textcopyright} 2011 Springer Science+Business Media B.V.},
annote = {Makes a revision of the most representative models on the literature.

Of all the models, only the following were classified as cognitive:

Castelfranchi et al., Repage, BDI + Repage and ForTrust},
author = {Pinyol, Isaac and Sabater-Mir, Jordi},
doi = {10.1007/s10462-011-9277-z},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Pinyol, Sabater-Mir/2013/Pinyol, Sabater-Mir{\_}2013{\_}Computational trust and reputation models for open multi-agent systems a review.pdf:pdf},
isbn = {0269-2821},
issn = {0269-2821},
journal = {Artificial Intelligence Review},
keywords = {Cognitive trust and reputation,Computational trust and reputation models,Multiagent systems},
month = {jun},
number = {1},
pages = {1--25},
title = {{Computational trust and reputation models for open multi-agent systems: a review}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84878107220{\&}partnerID=tZOtx3y1 http://link.springer.com/10.1007/s10462-011-9277-z},
volume = {40},
year = {2013}
}
@article{Lee1992,
abstract = {As automated controllers supplant human intervention in controlling complex systems, the operators' role often changes from that of an active controller to that of a supervisory controller. Acting as supervisors, operators can choose between automatic and manual control. Improperly allocating function between automatic and manual control can have negative consequences for the performance of a system. Previous research suggests that the decision to perform the job manually or automatically depends, in part, upon the trust the operators invest in the automatic controllers. This paper reports an experiment to characterize the changes in operators' trust during an interaction with a semi-automatic pasteurization plant, and investigates the relationship between changes in operators' control strategies and trust. A regression model identifies the causes of changes in trust, and a 'trust transfer function' is developed using time series analysis to describe the dynamics of trust. Based on a detailed analysis of operators' strategies in response to system faults we suggest a model for the choice between manual and automatic control, based on trust in automatic controllers and self-confidence in the ability to control the system manually.},
author = {Lee, J and Moray, N},
doi = {10.1080/00140139208967392},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Lee, Moray/1992/Lee, Moray{\_}1992{\_}Trust, control strategies and allocation of function in human-machine systems.pdf:pdf},
issn = {0014-0139},
journal = {Ergonomics},
month = {oct},
number = {10},
pages = {1243--70},
pmid = {1516577},
title = {{Trust, control strategies and allocation of function in human-machine systems.}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00140139208967392 http://www.ncbi.nlm.nih.gov/pubmed/1516577},
volume = {35},
year = {1992}
}
@article{Haring2013,
abstract = {This paper reports the results from an experiment examining people's perception and trust when interacting with an android robot. Also, they engaged in an economic trust game with the robot. We used proxemics, the physical distance to the robot, and questionnaires to measure the participants` character and their perception of the robot. We found influences of the subject's character onto the amount sent in the trust game and distance changes over the 3 interaction tasks. The perception of the robot changed after the interaction trials towards less anthropomorph and less intelligent, but safer. This study would enable future researches to compare different robot types, personality traits and cross-cultural effects},
author = {Haring, Kerstin Sophie and Matsumoto, Yoshio and Watanabe, Katsumi},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Haring, Matsumoto, Watanabe/2013/Haring, Matsumoto, Watanabe{\_}2013{\_}How do people perceive and trust a lifelike robot.pdf:pdf},
isbn = {9789881925237},
issn = {20780958},
journal = {Proceedings of the World Congress of Engineering and Computer Science 2013},
pages = {23--25},
title = {{How do people perceive and trust a lifelike robot}},
volume = {I},
year = {2013}
}
@article{Allen2007,
abstract = {To be effective, an agent that collaborate with humans needs to be able to learn new tasks from humans they work with. This paper describes a system that learns executable task models from a single collaborative learning session consisting of demonstration, explanation and dialogue. To accomplish this, the system integrates a range of AI technologies: deep natural language understanding, knowledge representation and reasoning, dialogue systems, planning/agent-based systems and machine learning. A formal evaluation shows the approach has great promise.},
author = {Allen, James and Chambers, Nathanael and Ferguson, George and Galescu, Lucian and Jung, Hyuckchul and Taysom, William},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Allen et al/2007/Allen et al.{\_}2007{\_}PLOW A Collaborative Task Learning Agent.pdf:pdf},
isbn = {0006987303},
journal = {Interpreting},
keywords = {Integrated Intelligence,Technical Papers},
pages = {1514--1519},
pmid = {20838071},
title = {{PLOW : A Collaborative Task Learning Agent}},
url = {http://www.aaai.org/Papers/AAAI/2007/AAAI07-240.pdf},
volume = {22},
year = {2007}
}
@article{Piunti2012,
author = {Piunti, Michele and Venanzi, Matteo and Falcone, Rino},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Piunti, Venanzi, Falcone/2012/Piunti, Venanzi, Falcone{\_}2012{\_}Multimodal Trust Formation with Uninformed Cognitive Maps ( UnCM ) ( Extended Abstract ).pdf:pdf},
journal = {Aamas},
keywords = {cognitive models,social simulation,social systems,trust},
pages = {1241--1242},
title = {{Multimodal Trust Formation with Uninformed Cognitive Maps ( UnCM ) ( Extended Abstract )}},
year = {2012}
}
@article{VanKleef2010,
author = {{Van Kleef}, G. A. and Homan, A. C. and Beersma, B. and van Knippenberg, D.},
doi = {10.1177/0956797610387438},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Van Kleef et al/2010/Van Kleef et al.{\_}2010{\_}On Angry Leaders and Agreeable Followers How Leaders' Emotions and Followers' Personalities Shape Motivation and T.pdf:pdf},
issn = {0956-7976},
journal = {Psychological Science},
month = {dec},
number = {12},
pages = {1827--1834},
title = {{On Angry Leaders and Agreeable Followers: How Leaders' Emotions and Followers' Personalities Shape Motivation and Team Performance}},
url = {http://pss.sagepub.com/lookup/doi/10.1177/0956797610387438},
volume = {21},
year = {2010}
}
@article{Allen2002,
abstract = {This paper describes the design of computer agents that can collaborate with humans in plan- ning. It includes an explicit problem solving level that mediates between the human-computer in- teraction and the underlying automated plan rea- soners. We also describe a plan reasoning system that allows for incremental, interactive develop- ment of plans to support collaborative planning. This model has been used in a prototype system in which untrained users can successfully de- velop plans in a simple evacuation-planning do- main.},
annote = {Proposes a problem solving module that introduces collaboration between the agent and the user.

Reference for fields of study on agents.},
author = {Allen, James and Ferguson, George},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Allen, Ferguson/2002/Allen, Ferguson{\_}2002{\_}Human-machine collaborative planning.pdf:pdf},
journal = {International NASA Workshop on Planning},
pages = {1--10},
title = {{Human-machine collaborative planning}},
url = {https://www.cs.rochester.edu/research/cisd/pubs/2002/allen-ferguson-nasa2002.pdf},
year = {2002}
}
@article{Ramchurn2004,
abstract = {In open environments in which autonomous agents can break contracts, computational models of trust have an important role to play in determining who to interact with and how interactions unfold. To this end, we develop such a trust model, based on confidence and reputation, and show how it can be concretely applied, using fuzzy sets, to guide agents in evaluating past interactions and in establishing new contracts with one another. Agents generally interact by engaging in some form of negotiation process which results in them making commitments to (contracts with) one another to carry out particular tasks (Jennings et al. 2001). However, in most realistic environments, there is no guarantee that a contracted agent will actually enact its commitments (because it may defect to gain higher utility or because there is uncertainty about whether the task can actually be achieved). In such situations, computational models of trust (here defined as the positive expectation that an interaction partner will act benignly and cooperatively in situations where defecting would prove more profitable to itself Dasgupta 1998) have an important role to play. First, to help determine the most reliable interaction partner (i.e., those in which the agent has the highest trust). Second, to influence the interaction process itself (e.g., an agents negotiation stance may vary according to the opponents trust level). Third, to define the},
annote = {Paper about a trust model that tries to improve upon the problem of reputation and image not being conjuntly used very well},
author = {Ramchurn, Sarvapali and Sierra, C. and Godo, L. and Jennings, N. R.},
doi = {10.1080/0883951049050904509045},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Ramchurn et al/2004/Ramchurn et al.{\_}2004{\_}Devising a trust model for multi-agent interactions using confidence and reputation.pdf:pdf},
issn = {0883-9514},
journal = {International Journal of Applied Artificial Intelligence},
number = {9-10},
pages = {833--852},
title = {{Devising a trust model for multi-agent interactions using confidence and reputation}},
url = {http://eprints.soton.ac.uk/260155/},
volume = {18},
year = {2004}
}
@inproceedings{Billings2012,
abstract = {Abstract In all human - robot interaction , trust is an important element to consider because the presence or absence of trust certainly impacts the ultimate outcome of that interaction . Limited research exists that delineates the development and maintenance of this trust in ...},
address = {New York, New York, USA},
author = {Billings, Deborah R. and Schaefer, Kristin E. and Chen, Jessie Y.C. and Hancock, Peter a.},
booktitle = {Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction - HRI '12},
doi = {10.1145/2157689.2157709},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Billings et al/2012/Billings et al.{\_}2012{\_}Human-robot interaction.pdf:pdf},
isbn = {9781450310635},
issn = {2167-2121},
keywords = {1,a,a literature review,design,existing theoretical empirical,first,general literature,human factors,introductory survey,performance,reliability},
pages = {109},
publisher = {ACM Press},
title = {{Human-robot interaction}},
url = {http://dl.acm.org/citation.cfm?doid=2157689.2157709},
year = {2012}
}
@article{Gao2013,
abstract = {This research is sponsored by the Office of Naval Research and the Air Force Office of Scientific Research.},
author = {Gao, F. and Clare, A. S. and Macbeth, J. C. and Cummings, M. L.},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Gao et al/2013/Gao et al.{\_}2013{\_}Modeling the Impact of Operator Trust on Performance in Multiple Robot Control,.pdf:pdf},
isbn = {9781577356042},
journal = {AAAI Spring Symposium: Trust and Autonomous Systems,},
keywords = {multiple robot control,system dynamics model,workload},
pages = {16--22},
title = {{Modeling the Impact of Operator Trust on Performance in Multiple Robot Control,}},
url = {http://dspace.mit.edu/handle/1721.1/90334},
year = {2013}
}
@article{Ososky2013,
author = {Ososky, Scott and Schuster, David and Phillips, Elizabeth and Jentsch, Florian},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Ososky et al/2013/Ososky et al.{\_}2013{\_}Building Appropriate Trust in Human-Robot Teams Mental Models Building Blocks of Trust.pdf:pdf},
isbn = {9781577356042},
journal = {AAAI Spring Symposium},
keywords = {AAAI Technical Report SS-13-07},
pages = {60--65},
title = {{Building Appropriate Trust in Human-Robot Teams Mental Models : Building Blocks of Trust}},
year = {2013}
}
@article{Parasuraman2000,
author = {Parasuraman, R and Sheridan, T.B.},
doi = {10.1109/3468.844354},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Parasuraman, Sheridan/2000/Parasuraman, Sheridan{\_}2000{\_}A model for types and levels of human interaction with automation.pdf:pdf},
isbn = {1083-4427},
issn = {1083-4427},
journal = {Systems, Man and {\ldots}},
number = {3},
pages = {286--297},
pmid = {11760769},
title = {{A model for types and levels of human interaction with automation}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=844354},
volume = {30},
year = {2000}
}
@article{Walter2008,
abstract = {In this paper, we present a model of a trust-based recommendation system on a social network. The idea of the model is that agents use their social network to reach information and their trust relationships to filter it. We investigate how the dynamics of trust among agents affect the performance of the system by comparing it to a frequency-based recommendation system. Furthermore, we identify the impact of network density, preference heterogeneity among agents, and knowledge sparseness to be crucial factors for the performance of the system. The system self-organises in a state with performance near to the optimum; the performance on the global level is an emergent property of the system, achieved without explicit coordination from the local interactions of agents.},
archivePrefix = {arXiv},
arxivId = {nlin/0611054},
author = {Walter, Frank Edward and Battiston, Stefano and Schweitzer, Frank},
doi = {10.1007/s10458-007-9021-x},
eprint = {0611054},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Walter, Battiston, Schweitzer/2007/Walter, Battiston, Schweitzer{\_}2007{\_}A model of a trust-based recommendation system on a social network.pdf:pdf},
isbn = {1387-2532},
issn = {1387-2532},
journal = {Autonomous Agents and Multi-Agent Systems},
keywords = {Recommender system,Social network,Trust},
month = {feb},
number = {1},
pages = {57--74},
primaryClass = {nlin},
title = {{A model of a trust-based recommendation system on a social network}},
url = {http://link.springer.com/10.1007/s10458-007-9021-x},
volume = {16},
year = {2007}
}
@inproceedings{Yu2002,
abstract = {For agents to function effectively in large and open networks, they must ensure that their correspondents, i.e., the agents they interact with, are trustworthy. Since no central authorities may exist, the only way agents can find trustworthy correspondents is by collaborating with others to identify those whose past behavior has been untrustworthy. In other words, finding trustworthy correspondents reduces to the problem of distributed reputation management.Our approach adapts the mathematical theory of evidence to represent and propagate the ratings that agents give to their correspondents. When evaluating the trustworthiness of a correspondent, an agent combines its local evidence (based on direct prior interactions with the correspondent) with the testimonies of other agents regarding the same correspondent. We experimentally studied this approach to establish that some important properties of trust are captured by it.},
address = {New York, New York, USA},
author = {Yu, Bin and Singh, Munindar P.},
booktitle = {Proceedings of the first international joint conference on Autonomous agents and multiagent systems part 1 - AAMAS '02},
doi = {10.1145/544805.544809},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Yu, Singh/2002/Yu, Singh{\_}2002{\_}An evidential model of distributed reputation management.pdf:pdf},
isbn = {1581134800},
issn = {08247935},
keywords = {belief functions,distributed reputation management,trust networks},
pages = {294},
publisher = {ACM Press},
title = {{An evidential model of distributed reputation management}},
url = {http://dl.acm.org/citation.cfm?id=544809 http://portal.acm.org/citation.cfm?doid=544741.544809},
year = {2002}
}
@misc{Hoc2000,
annote = {Reference as noting the importance of trust in human-machine interaction},
author = {Hoc, Jean-Michel},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Hoc/2000/Hoc{\_}2000{\_}From human - machine interaction to human - machine cooperation.pdf:pdf},
pages = {833 -- 843},
title = {{From human - machine interaction to human - machine cooperation}},
year = {2000}
}
@article{Pasternack2010,
author = {Pasternack, Jeff and Roth, D},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Pasternack, Roth/2010/Pasternack, Roth{\_}2010{\_}Knowing what to believe (when you already know something).pdf:pdf},
journal = {Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010)},
number = {August},
pages = {877--885},
title = {{Knowing what to believe (when you already know something)}},
url = {http://dl.acm.org/citation.cfm?id=1873880},
year = {2010}
}
@incollection{Neville2004,
abstract = {Agents that behave maliciously or incompetently are a potential hazard in open distributed e-commerce applications. However human societies have evolved signals and mechanisms based on social interaction to defend against such behaviour. In this paper we present a computational socio-cognitive framework which formalises social theories of trust, reputation, recommendation and learning from direct experience which enables agents to cope with malicious or incompetent actions. The framework integrates these socio-cognitive elements with an agent's economic reasoning resulting in an agent whose behaviour in commercial transactions is influenced by its social interactions, whilst being motivated and constrained by economic considerations. The framework thus provides a comprehensive solution to a number of issues ranging from the evolution of a trust belief from individual experiences and recommendations to the use of those beliefs in market place level decisions. The framework is presented in the context of an artificial market place scenario which is part of a simulation environment currently under development. This is planned for use in evaluation of the framework, and hence can inform design of local decision making algorithms and mechanisms to enforce of social order in agent mediated e-commerce. {\textcopyright} Springer-Verlag Berlin Heidelberg 2004.},
author = {Neville, Brendan and Pitt, Jeremy},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-25946-6_24},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Neville, Pitt/2004/Neville, Pitt{\_}2004{\_}A Computational Framework for Social Agents in Agent Mediated E-commerce.pdf:pdf},
isbn = {978-3-540-22231-6},
issn = {03029743},
pages = {376--391},
title = {{A Computational Framework for Social Agents in Agent Mediated E-commerce}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-26844439149{\&}partnerID=tZOtx3y1 http://link.springer.com/10.1007/978-3-540-25946-6{\_}24},
volume = {3071},
year = {2004}
}
@article{HanYu2013,
abstract = {In open and dynamic multiagent systems (MASs), agents often need to rely on resources or services provided by other agents to accomplish their goals. During this process, agents are exposed to the risk of being exploited by others. These risks, if not mitigated, can cause serious breakdowns in the operation of MASs and threaten their long-term wellbeing. To protect agents from the uncertainty in the behavior of their interaction partners, the age-old mechanism of trust between human beings is re-contexted into MASs. The basic idea is to let agents self-police the MAS by rating each other on the basis of their observed behavior and basing future interaction decisions on such information. Over the past decade, a large number of trust management models were proposed. However, there is a lack of research effort in several key areas, which are critical to the success of trust management in MASs where human beings and agents coexist. The purpose of this paper is to give an overview of existing research in trust management in MASs. We analyze existing trust models from a game theoretic perspective to highlight the special implications of including human beings in an MAS, and propose a possible research agenda to advance the state of the art in this field.},
author = {{Han Yu} and {Zhiqi Shen} and Leung, Cyril and {Chunyan Miao} and Lesser, Victor R.},
doi = {10.1109/ACCESS.2013.2259892},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Han Yu et al/2013/Han Yu et al.{\_}2013{\_}A Survey of Multi-Agent Trust Management Systems.pdf:pdf},
isbn = {2169-3536 VO - 1},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {Analytical models,Computational modeling,Context awareness,Decision making,Game theory,Trust management,Uncertainty,multi-agent systems,reputation,trust},
pages = {35--50},
title = {{A Survey of Multi-Agent Trust Management Systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6514820},
volume = {1},
year = {2013}
}
@article{Sabater2006,
abstract = {This paper introduces Repage, a computational system that adopts a cognitive theory of reputation. We propose a fundamental difference between image and reputation, which suggests a way out from the paradox of sociality, i.e. the trade-off between agents' autonomy and their need to adapt to social environment. On one hand, agents are autonomous if they select partners based on their social evaluations (images). On the other, they need to update evaluations by taking into account others'. Hence, social evaluations must circulate and be represented as "reported evaluations" (reputation), before and in order for agents to decide whether to accept them or not. To represent this level of cognitive detail in artificial agents' design, there is a need for a specialised subsystem, which we are in the course of developing for the public domain. In the paper, after a short presentation of the cognitive theory of reputation and its motivations, we describe the implementation of Repage.},
annote = {Semi-Cognitive model.

Direct Trust calculation is heavily based on the ReGreT model.

What brings of new to the table is the distinction betweeen reputation and image. Of what the agent perceives through direct interaction with the target agent and of what he hears of informats about the target agent.

(As a side note: Both Repage and Regret were created by the same head author, Jordi Sabater)},
author = {Sabater, Jordi and Paolucci, Mario and Conte, Rosaria},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Sabater, Paolucci, Conte/2006/Sabater, Paolucci, Conte{\_}2006{\_}Repage REPutation and ImAGE among limited autonomous partners.pdf:pdf},
journal = {Jasss},
keywords = {Agent Systems,Cognitive Design,Fuzzy Evaluation,Reputation},
number = {2},
pages = {117--134},
title = {{Repage: REPutation and ImAGE among limited autonomous partners}},
volume = {9},
year = {2006}
}
@book{Tirole2005,
author = {Fudenberg, Drew and Tirole, Jean},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Fudenberg, Tirole/1991/Fudenberg, Tirole{\_}1991{\_}Game Theory.pdf:pdf},
isbn = {9780262061414},
pages = {1--579},
publisher = {MIT Press},
title = {{Game Theory}},
url = {http://gen.lib.rus.ec/search.php?req=fudenberg+tirole{\&}nametype=orig{\&}view=simple{\&}column[]=title{\&}column[]=author{\&}column[]=series{\&}column[]=periodical{\&}column[]=publisher{\&}column[]=year$\backslash$npapers2://publication/uuid/918CE64C-88D8-4659-AE2B-94217E083CEA},
year = {1991}
}
@article{Yuan2010,
abstract = {The trust network is a social network where nodes are inter-linked by their trust relations. It has been widely used in various applications, however, little is known about its structure due to its highly dynamic nature. Based on five trust networks obtained from the real online sites, we contribute to verify that the trust network is the small-world network: the nodes are highly clustered, while the distance between two randomly selected nodes is short. This has considerable implications on using the trust network in the trust-aware applications. We choose the trust-aware recommender system as an example of such applications and demonstrate its advantages by making use of our verified small-world nature of the trust network. ?? 2010 Elsevier B.V. All rights reserved.},
author = {Yuan, Weiwei and Guan, Donghai and Lee, Young-Koo and Lee, Sungyoung and Hur, Sung Jin},
doi = {10.1016/j.knosys.2009.12.004},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Yuan et al/2010/Yuan et al.{\_}2010{\_}Improved trust-aware recommender system using small-worldness of trust networks.pdf:pdf},
isbn = {0950-7051},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Recommender system,Small-world network,Trust,Trust network},
month = {apr},
number = {3},
pages = {232--238},
title = {{Improved trust-aware recommender system using small-worldness of trust networks}},
url = {http://dx.doi.org/10.1016/j.knosys.2009.12.004 http://linkinghub.elsevier.com/retrieve/pii/S095070511000002X},
volume = {23},
year = {2010}
}
@article{Jackson1993,
abstract = {I provide a (very) brief introduction to game theory. I have developed these notes to provide quick access to some of the basics of game theory; mainly as an aid for students in courses in which I assumed familiarity with game theory but did not require it as a prerequisite.},
author = {Jackson, Matthew O},
doi = {10.2139/ssrn.1968579},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Jackson/2011/Jackson{\_}2011{\_}A Brief Introduction to the Basics of Game Theory.pdf:pdf},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
pages = {1--21},
title = {{A Brief Introduction to the Basics of Game Theory}},
url = {http://ssrn.com/paper=1968579 http://www.ssrn.com/abstract=1968579},
year = {2011}
}
@article{Granatyr2015,
author = {Granatyr, Jones and Botelho, Vanderson and Lessing, Otto Robert and Scalabrin, Edson Em{\'{i}}lio and Barth{\`{e}}s, Jean-Paul and Enembreck, Fabr{\'{i}}cio},
doi = {10.1145/2816826},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Granatyr et al/2015/Granatyr et al.{\_}2015{\_}Trust and Reputation Models for Multiagent Systems.pdf:pdf},
issn = {03600300},
journal = {ACM Computing Surveys},
month = {oct},
number = {2},
pages = {1--42},
title = {{Trust and Reputation Models for Multiagent Systems}},
url = {http://dl.acm.org/citation.cfm?doid=2830539.2816826},
volume = {48},
year = {2015}
}
@article{Dunbar1998,
archivePrefix = {arXiv},
arxivId = {arXiv:1301.2464v1},
author = {Dunbar, Robin I.M.},
doi = {10.1002/(SICI)1520-6505(1998)6:5<178::AID-EVAN5>3.3.CO;2-P},
eprint = {arXiv:1301.2464v1},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Dunbar/1998/Dunbar{\_}1998{\_}The social brain hypothesis.pdf:pdf},
isbn = {1520-6505},
issn = {10601538},
journal = {Evolutionary Anthropology: Issues, News, and Reviews},
number = {5},
pages = {178--190},
pmid = {19575315},
title = {{The social brain hypothesis}},
url = {http://doi.wiley.com/10.1002/{\%}28SICI{\%}291520-6505{\%}281998{\%}296{\%}3A5{\%}3C178{\%}3A{\%}3AAID-EVAN5{\%}3E3.3.CO{\%}3B2-P},
volume = {6},
year = {1998}
}
@misc{Wikipedia.Golden.Balls,
author = {Wikipedia},
booktitle = {Wikipedia},
title = {{Golden Balls: https://en.wikipedia.org/wiki/Golden{\_}Balls}},
url = {https://en.wikipedia.org/wiki/Golden{\_}Balls}
}
@inproceedings{Traum2003,
abstract = {The effectiveness of simulation-based training for individual tasks -- such as piloting skills -- is well established, but its use for team training raises challenging technical issues. Ideally, human users could gain valuable leadership experience by interacting with synthetic teammates in realistic and potentially stressful scenarios. However, creating human-like teammates that can support flexible, natural interactions with humans and other synthetic agents requires integrating a wide variety of capabilities, including models of teamwork, models of human negotiation, and the ability to participate in face-to-face spoken conversations in virtual worlds. We have developed such virtual humans by integrating and extending prior work in these areas, and we have applied our virtual humans to an example peacekeeping training scenario to guide and evaluate our research. Our models allow agents to reason about authority and responsibility for individual actions in a team task and, as appropriate, to carry out actions, give and accept orders, monitor task execution, and negotiate options. Negotiation is guided by the agents' dynamic assessment of alternative actions given the current scenario conditions, with the aim of guiding the human user towards an ability to make similar assessments.},
address = {New York, New York, USA},
author = {Traum, David and Rickel, Jeff and Gratch, Jonathan and Marsella, Stacy},
booktitle = {Proceedings of the second international joint conference on Autonomous agents and multiagent systems - AAMAS '03},
doi = {10.1145/860575.860646},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Traum et al/2003/Traum et al.{\_}2003{\_}Negotiation over tasks in hybrid human-agent teams for simulation-based training.pdf:pdf},
isbn = {1581136838},
keywords = {animated agents,conversational agents,negotiation},
pages = {441},
pmid = {7904065819273664397},
publisher = {ACM Press},
title = {{Negotiation over tasks in hybrid human-agent teams for simulation-based training}},
url = {http://dl.acm.org/citation.cfm?id=860646$\backslash$nhttp://portal.acm.org/citation.cfm?doid=860575.860646 http://portal.acm.org/citation.cfm?doid=860575.860646},
year = {2003}
}
@article{Castelfranchi1998,
abstract = {After arguing about the crucial importance of trust for Agents and MAS, we provide a definition of trust both as a mental state and as a social attitude and relation. We present the mental ingredients of trust: its specific beliefs and goals, with special attention to evaluations and expectations. We show the relation between trust and the mental background of delegation. We explain why trust is a bet, and implies some risks, and analyse the most basic forms of non-social trust (reliance on objects and tools) to arrive at the more complex forms of social trust, based on morality and reputation. Finally we present a principled quantification of trust, based on its cognitive ingredients. And use this “degree of trust” as the basis for a rational decision to delegate or not to another agent. The paper is intended to contribute both to the conceptual analysis and to the practical use of trust in social theory and MAS},
author = {Castelfranchi, C and Falcone, R},
doi = {10.1109/ICMAS.1998.699034},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Castelfranchi, Falcone/1998/Castelfranchi, Falcone{\_}1998{\_}Principles of trust for MAS cognitive anatomy, social importance, and quantification.pdf:pdf},
isbn = {0-8186-8500-X},
issn = {0-8186-8500-X},
journal = {Proceedings of the International Conference on Multi Agent Systems},
pages = {72--79},
title = {{Principles of trust for MAS: cognitive anatomy, social importance, and quantification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=699034},
year = {1998}
}
@article{Russell1995,
abstract = {The long-anticipated revision of this best-selling book offers the most comprehensive, up-to-date introduction to the theory and practice of artificial intelligence. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For those interested in artificial intelligence.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Ligeza, Antoni},
doi = {10.1016/0925-2312(95)90020-9},
eprint = {9809069v1},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Ligeza/1995/Ligeza{\_}1995{\_}Artificial Intelligence A Modern Approach.pdf:pdf},
isbn = {9780131038059},
issn = {09252312},
journal = {Neurocomputing},
month = {oct},
number = {2},
pages = {215--218},
pmid = {20949757},
primaryClass = {arXiv:gr-qc},
title = {{Artificial Intelligence: A Modern Approach}},
url = {http://portal.acm.org/citation.cfm?id=773294 http://linkinghub.elsevier.com/retrieve/pii/0925231295900209},
volume = {9},
year = {1995}
}
@article{Rapoport1988,
abstract = {The fastener design for the transfer of concentrated transverse (out of plane, pull-out) loads to random glass fiber reinforced thermoset polymers was investigated. The elastic material properties, void content,a nd glass content of the composite were determined and a finite element model was used to analyze and compare the performance of the various washer designs for reducing the stress and strain levels near the edge of the washer at a bolted joint. Experimental studies were conducted to verify the finite element model.},
author = {Rapoport, A.},
doi = {10.1177/0022002788032003003},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Rapoport/1988/Rapoport{\_}1988{\_}Experiments with N-Person Social Traps I Prisoner's Dilemma, Weak Prisoner's Dilemma, Volunteer's Dilemma, and Largest Num.pdf:pdf},
journal = {Journal of Conflict Resolution},
month = {sep},
number = {3},
pages = {457--472},
title = {{Experiments with N-Person Social Traps I: Prisoner's Dilemma, Weak Prisoner's Dilemma, Volunteer's Dilemma, and Largest Number}},
url = {http://jcr.sagepub.com/cgi/doi/10.1177/0022002788032003003},
volume = {32},
year = {1988}
}
@incollection{Gambetta2000,
abstract = {In this concluding essay I shall try to reconstruct what seem to me the central questions about trust that the individual contributions presented in this volume raise and partly answer.1 In the first section, I briefly qualify the claim that there is a degree of rational cooperation that should but does not exist, and I shall give a preliminary indication of the importance of the beliefs we hold about others, over and above the importance of the motives we may have for cooperation. In the second section, I define trust and the general conditions under which it becomes relevant for cooperation. In the third, I discuss the extent to which cooperation can come about independently of trust, and also whether trust can be seen as a result rather than a precondition of cooperation. In the final section, I address the question of whether there are rational reasons for people to trust - and especially whether there are reasons to trust trust and, correspondingly, distrust distrust.},
author = {Gambetta, Diego},
booktitle = {Trust: Making and Breaking Cooperative Relations},
doi = {10.1.1.24.5695},
isbn = {0631175873},
pages = {213--237},
title = {{Can We Trust Trust?}},
year = {2000}
}
@article{Sabater2002,
abstract = {The use of previous direct interactions is probably the best way to calculate a reputation but, unfortunately this infor- mation is not always available. This is especially true in large multi-agent systems where interaction is scarce. In this paper we present a reputation system that takes advan- tage, among other things, of social relations between agents to overcome this problem.},
annote = {Reputation system focused on social relations.

Regret Model Paper.},
author = {Sabater, Jordi and Sierra, Carles},
doi = {10.1145/544852.544854},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Sabater, Sierra/2002/Sabater, Sierra{\_}2002{\_}Reputation and social network analysis in multi-agent systems.pdf:pdf},
isbn = {1581134800},
journal = {Proceedings of the first international joint conference on Autonomous agents and multiagent systems part 1 - AAMAS '02},
pages = {475},
title = {{Reputation and social network analysis in multi-agent systems}},
url = {http://portal.acm.org/citation.cfm?doid=544741.544854},
year = {2002}
}
@article{Tobergte2013,
abstract = {This study is intended to explore two questions: Does the level of trust within a group effect group performance? If so, how does this relationship operate? To answer these questions I use an experimental method to examine two roles through which interpersonal trust could effect group performance: a main effect and a moderating effect. The data do not support the main effect that has dominated the literature on interpersonal trust. The data do support the moderating role: trust seems to influence how motivation is converted into work group processes and performance. On the basis of these findings, I suggest that trust may be best understood as a construct that influences group performance indirectly by channeling group member's energy toward reaching alternative goals.},
address = {Cambridge},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Dirks, Kurt T.},
doi = {10.1037/0021-9010.84.3.445},
editor = {{Intergovernmental Panel on Climate Change}},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Dirks/1999/Dirks{\_}1999{\_}The effects of interpersonal trust on work group performance.pdf:pdf},
isbn = {9788578110796},
issn = {0021-9010},
journal = {Journal of Applied Psychology},
keywords = {icle},
number = {3},
pages = {445--455},
pmid = {25246403},
publisher = {Cambridge University Press},
title = {{The effects of interpersonal trust on work group performance.}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781107415324A009 http://doi.apa.org/getdoi.cfm?doi=10.1037/0021-9010.84.3.445},
volume = {84},
year = {1999}
}
@book{Mascarenhas2015,
abstract = {This work addresses the challenge of creating virtual agents that are able to portray culturally appropriate behavior when interacting with other agents or humans. Because culture influences how people perceive their social reality it is important to have agent models that explicitly consider social elements, such as existing relational factors. We addressed this necessity by integrating culture into a novel model for simulating human social behavior. With this model, we operationalized a particular dimension of culture—individualism versus collectivism—within the context of an interactive narrative scenario that is part of an agent-based tool for intercultural training. Using this scenario we conducted a cross-cultural study in which participants from a collectivistic country (Portugal) were compared with participants from an individualistic country (the Netherlands) in the way they perceived and interacted with agents whose behavior was either individualistic or collectivistic, according to the configuration of the proposed model. In the obtained results, Portuguese subjects rated the collectivistic agents more positively than the Dutch but both countries had a similarly positive opinion about the individualistic agents. This experiment sheds new light on how people from different countries differ when assessing the social appropriateness of virtual agents, while also raising new research questions on this matter.},
author = {Mascarenhas, Samuel and Degens, Nick and Paiva, Ana and Prada, Rui and Hofstede, Gert Jan and Beulens, Adrie and Aylett, Ruth},
booktitle = {Autonomous Agents and Multi-Agent Systems},
doi = {10.1007/s10458-015-9312-6},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Mascarenhas et al/2015/Mascarenhas et al.{\_}2015{\_}Modeling culture in intelligent virtual agents.pdf:pdf},
isbn = {1045801593},
issn = {1387-2532},
keywords = {Cognitive models,Collectivism,Culture,Individualism,Intelligent virtual environments,Virtual agents},
publisher = {Springer US},
title = {{Modeling culture in intelligent virtual agents}},
url = {http://link.springer.com/10.1007/s10458-015-9312-6},
year = {2015}
}
@article{Castelfranchi2001,
abstract = { The authors argue that it is important to analyse the role of trust and deception in interactions between agents in virtual societies. In particular, in hybrid situations where artificial agents interact with human agents it is important that those artificial agents can reason about the trustworthiness and deceptive actions of the human counterpart. In order to support this interaction between agents in virtual societies, a theory on trust and deception must be developed. In the literature, a wide variety of theories on trust (less so on deception!) have been developed but not specifically for virtual communities. Based on these earlier scientific results, we make a first attempt to develop a general theory on trust and deception for virtual communities, and we discuss a number of examples to illustrate which objectives such a theory should fulfil.},
annote = {Discusses the problems of trust and deception in agent societies.},
author = {Castelfranchi, C. and Tan, Yao-Hua Tan Yao-Hua},
doi = {10.1109/HICSS.2001.927042},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Castelfranchi, Tan/2001/Castelfranchi, Tan{\_}2001{\_}The role of trust and deception in virtual societies.pdf:pdf},
isbn = {0-7695-0981-9},
issn = {1086-4415},
journal = {Proceedings of the 34th Annual Hawaii International Conference on System Sciences},
keywords = {and phrases,computer,deception,is in the fact,it behaves in a,multiagent systems,perfectly honest way,put to work,that once programmed and,the inhumanity of the,trust,virtual society},
number = {3},
pages = {55--70},
title = {{The role of trust and deception in virtual societies}},
volume = {6},
year = {2001}
}
@article{VandenAssem2012,
abstract = {W e examine cooperative behavior when large sums of money are at stake, using data from the television game show Golden Balls. At the end of each episode, contestants play a variant on the classic prisoner's dilemma for large and widely ranging stakes averaging over {\$}20,000. Cooperation is surprisingly high for amounts that would normally be considered consequential but look tiny in their current context, what we call a “big peanuts” phenomenon. Utilizing the prior interaction among contestants, we find evidence that people have reciprocal preferences. Surprisingly, there is little support for conditional cooperation in our sample. That is, players do not seem to be more likely to cooperate if their opponent might be expected to cooperate. Further, we replicate earlier findings that males are less cooperative than females, but this gender effect reverses for older contestants because men become increasingly cooperative as their age increases.},
author = {van den Assem, M. J. and van Dolder, D. and Thaler, R. H.},
doi = {10.1287/mnsc.1110.1413},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/van den Assem, van Dolder, Thaler/2012/van den Assem, van Dolder, Thaler{\_}2012{\_}Split or steal Cooperative behavior when the stakes are large.pdf:pdf},
isbn = {0025-1909},
issn = {0025-1909},
journal = {Management Science},
keywords = {2010,2011,accepted april 17,anchoring,and terrance odean,behavior,by brad barber,context effects,cooperation,cooperative behavior,game show,history,in advance october 7,natural experiment,prisoner,published online in articles,received july 14,reciprocal behavior,reciprocity,s dilemma,social,social preferences,special issue editors,teck ho},
number = {1},
pages = {2--20},
pmid = {70894768},
title = {{Split or steal? Cooperative behavior when the stakes are large}},
volume = {58},
year = {2012}
}
@article{Huang2008,
abstract = {Trust and reputation have been recognized as a key issue in the area of multi-agent systems. And till now, a number of researchers have, in different perspectives, reviewed/surveyed the notions, techniques or models of trust and reputation. However, the notion of trust and reputation still remains somewhat vague, and the techniques and model is yet understood in a disunited way. This paper categorizes the notions, techniques, and models of trust and reputation in terms of the trust management process which can be characterized through three questions: a) why does an agent trust another; b) how do agents judge or evaluate the trustworthiness of others; c) what does an agent do after obtaining the trustworthiness of others. These questions present a unified view of trust and reputation. The two aspects implied in the first question suggest two different understanding of the notion of trust. The second question refers to the techniques of getting and dealing with two different types of trust evidence. And it comes to the third question when trust models take agents' post-evaluation actions and the effect of these actions on their partners/opponents into account to make themselves incentive-compatible.},
author = {Huang, Hongbing and Zhu, Guiming and Jin, Shiyao},
doi = {10.1109/CCCM.2008.122},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Huang, Zhu, Jin/2008/Huang, Zhu, Jin{\_}2008{\_}Revisiting Trust and Reputation in Multi-agent Systems.pdf:pdf},
isbn = {978-0-7695-3290-5},
journal = {Computing, Communication, Control, and Management, 2008. CCCM '08. ISECS International Colloquium on},
pages = {424--429},
title = {{Revisiting Trust and Reputation in Multi-agent Systems}},
volume = {1},
year = {2008}
}
@book{Castelfranchi2010,
address = {Chichester, UK},
author = {Castelfranchi, Cristiano and Falcone, Rino},
booktitle = {Wiley},
doi = {10.1002/9780470519851},
edition = {1},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Castelfranchi, Falcone/2010/Castelfranchi, Falcone{\_}2010{\_}Trust Theory.pdf:pdf},
isbn = {9780470519851},
issn = {0717-6163},
month = {mar},
pages = {1--387},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Trust Theory}},
url = {http://doi.wiley.com/10.1002/9780470519851},
year = {2010}
}
@article{Noorian2010,
abstract = {We introduce a multidimensional framework for classifying and comparing trust and reputation (T{\&}R) systems. The framework dimensions encompass both hard and soft features of such systems including different witness location approaches, various reputation calculation engines, variety of information sources and rating systems which are categorised as hard features, and also basic reputation measurement parameters, context diversity checking, reliability and honesty assessment and adaptability which are referred to as soft features. Specifically, the framework dimensions answer questions related to major characteristics of T{\&}R systems including those parameters from the real world that should be imitated in a virtual environment. The proposed framework can serve as a basis to understand the current state of the art in the area of computational trust and reputation and also help in designing suitable control mechanisms for online communities. In addition, we haveprovided a critical analysis of some of the existing techniques in the literature compared within the context of the proposed framework dimensions.},
author = {Noorian, Zeinab and Ulieru, Mihaela},
doi = {10.4067/S0718-18762010000200007},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Noorian, Ulieru/2010/Noorian, Ulieru{\_}2010{\_}The State of the Art in Trust and Reputation Systems A Framework for Comparison.pdf:pdf},
isbn = {0718-1876},
issn = {0718-1876},
journal = {Journal of theoretical and applied electronic commerce research},
keywords = {Computational trust,Online communities,Reliability assessment,Reputation formalization,T{\&}R taxonomy},
month = {aug},
number = {2},
pages = {97--117},
title = {{The State of the Art in Trust and Reputation Systems: A Framework for Comparison}},
url = {http://www.scielo.cl/scielo.php?script=sci{\_}arttext{\&}pid=S0718-18762010000200007{\&}lng=en{\&}nrm=iso{\&}tlng=en},
volume = {5},
year = {2010}
}
@article{Grosz1996,
abstract = {The construction of computer systems that are intelligent, collaborative problem-solving partners is an important goal for both the science of AI and its application. From the scientific perspective, the development of theories and mechanisms to enable building collaborative systems presents exciting research challenges across AI subfields. From the applications perspective, the capability to collaborate with users and other systems is essential if large-scale information systems of the future are to assist users in finding the information they need and solving the problems they have. In this address, it is argued that collaboration must be designed into systems from the start; it cannot be patched on. Key features of collaborative activity are described, the scientific base provided by recent AI research is discussed, and several of the research challenges posed by collaboration are presented. It is further argued that research on, and the development of, collaborative systems should itself be a collaborative endeav- or—within AI, across subfields of computer science, and with researchers in other fields},
author = {Grosz, Barbara J.},
journal = {AI Magazine},
keywords = {artificial intelligence,collaboration,collaborative systems},
pages = {67--85},
title = {{Collaborative Systems}},
year = {1996}
}
@article{Lee2013,
abstract = {We present a computational model capable of predicting-above human accuracy-the degree of trust a person has toward their novel partner by observing the trust-related nonverbal cues expressed in their social interaction. We summarize our prior work, in which we identify nonverbal cues that signal untrustworthy behavior and also demonstrate the human mind's readiness to interpret those cues to assess the trustworthiness of a social robot. We demonstrate that domain knowledge gained from our prior work using human-subjects experiments, when incorporated into the feature engineering process, permits a computational model to outperform both human predictions and a baseline model built in naivet{\'{e}} of this domain knowledge. We then present the construction of hidden Markov models to investigate temporal relationships among the trust-related nonverbal cues. By interpreting the resulting learned structure, we observe that models built to emulate different levels of trust exhibit different sequences of nonverbal cues. From this observation, we derived sequence-based temporal features that further improve the accuracy of our computational model. Our multi-step research process presented in this paper combines the strength of experimental manipulation and machine learning to not only design a computational trust model but also to further our understanding of the dynamics of interpersonal trust.},
author = {Lee, Jin Joo and Knox, W. Bradley and Wormwood, Jolie B. and Breazeal, Cynthia and DeSteno, David},
doi = {10.3389/fpsyg.2013.00893},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Lee et al/2013/Lee et al.{\_}2013{\_}Computationally modeling interpersonal trust.pdf:pdf},
isbn = {1664-1078 (Electronic)},
issn = {1664-1078},
journal = {Frontiers in Psychology},
keywords = {Computational trust model,Human-robot interaction,Interpersonal trust,Machine learning,Nonverbal behavior,Social signal processing},
number = {DEC},
pages = {1--14},
pmid = {24363649},
title = {{Computationally modeling interpersonal trust}},
url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2013.00893/abstract},
volume = {4},
year = {2013}
}
@inproceedings{Bickmore2001,
abstract = {Building trust with users is crucial in a wide range of applications, such as financial transactions, and some minimal degree of trust is required in all applications to even initiate and maintain an interaction with a user. Humans use a variety of relational conversational strategies, including small talk, to establish trusting relationships with each other. We argue that such strategies can also be used by interface agents, and that embodied conversational agents are ideally suited for this task given the myriad cues available to them for signaling trustworthiness. We describe a model of social dialogue, an implementation in an embodied conversation agent, and an experiment in which social dialogue was demonstrated to have an effect on trust, for users with a disposition to be extroverts.},
address = {New York, New York, USA},
annote = {ECAs - Embodied Conversational Agents
Describes a model to create agents capable of social dialogue, with speech and social strategies.
Should look more into it, specially for reference in agent technology.

Paper to the thesis of the same name.},
author = {Bickmore, Timothy and Cassell, Justine},
booktitle = {Proceedings of the SIGCHI conference on Human factors in computing systems - CHI '01},
doi = {10.1145/365024.365304},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Bickmore, Cassell/2001/Bickmore, Cassell{\_}2001{\_}Relational agents.pdf:pdf},
isbn = {1581133278},
keywords = {able establish social,eases,engage trust which,relationships with,sort must,turn,users order},
number = {3},
pages = {396--403},
publisher = {ACM Press},
title = {{Relational agents}},
url = {http://dl.acm.org/citation.cfm?id=365024.365304 http://portal.acm.org/citation.cfm?doid=365024.365304},
volume = {3},
year = {2001}
}
@phdthesis{Schaefer2009,
abstract = {As robots penetrate further into the everyday environments, trust in these robots becomes a crucial issue. The purpose of this work was to create and validate a reliable scale that could measure changes in an individual's trust in a robot. Assessment of current trust theory identified measurable antecedents specific to the human, the robot, and the environment. Six experiments subsumed the development of the 40 item trust scale. Scale development included the creation of a 172 item pool. Two experiments identified the robot features and perceived functional characteristics that were related to the classification of a machine as a robot for this item pool. Item pool reduction techniques and subject matter expert (SME) content validation were used to reduce the scale to 40 items. The two final experiments were then conducted to validate the scale. The finalized 40 item pre-post interaction trust scale was designed to measure trust perceptions specific to HRI. The scale measured trust on a 0-100{\%} rating scale and provides a percentage trust score. A 14 item sub-scale of this final version of the test recommended by SMEs may be sufficient for some HRI tasks, and the implications of this proposition were discussed.},
author = {Schaefer, Kristin},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Schaefer/2009/Schaefer{\_}2009{\_}The Perception and Measurement of Human-Robot Trust.pdf:pdf},
pages = {332},
title = {{The Perception and Measurement of Human-Robot Trust}},
year = {2009}
}
@article{Hancock2011,
abstract = {It is proposed that trust is a critical element in the interactive relations between humans and the automated and robotic technology they create. This article pres- ents (a) why trust is an important issue for this type of interac- tion, (b) a brief history of the development of human-robot trust issues, and (c) guidelines for input by human factors/ergonomics professionals to the design of human-robot systems with empha- sis on trust issues. Our work considers trust an ongoing and dynamic dimension as robots evolve from simple tools to active, sentient teammates. KEYWORDS:},
author = {Hancock, P. A. and Billings, D. R. and Schaefer, K. E.},
doi = {10.1177/1064804611415045},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Hancock, Billings, Schaefer/2011/Hancock, Billings, Schaefer{\_}2011{\_}Can you trust your robot.pdf:pdf},
isbn = {1064-8046},
issn = {1064-8046,},
journal = {Ergonomics in Design},
number = {3},
pages = {24--29},
pmid = {15706357},
title = {{Can you trust your robot?}},
volume = {19},
year = {2011}
}
@article{Cassell2000,
abstract = {Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references.},
annote = {Reference as one of research using ECAs (Embodied Conversational Agents)

Studies and demonstrates Interaction Rituals, used to build trust on the ECA.

Their scennario is one of a Real Estate Agent (REA) to perform small talk.},
author = {Cassell, Justine and Bickmore, Timothy},
doi = {10.1145/355112.355123},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Cassell, Bickmore/2000/Cassell, Bickmore{\_}2000{\_}External manifestations of trustworthiness in the interface.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
month = {dec},
number = {12},
pages = {50--56},
pmid = {85634572},
title = {{External manifestations of trustworthiness in the interface}},
url = {http://web.media.mit.edu/{~}bickmore/publications/CACM{\_}trust.pdf http://portal.acm.org/citation.cfm?doid=355112.355123},
volume = {43},
year = {2000}
}
@inproceedings{Bickmore2001c,
abstract = {What kinds of social relationships can people have with computers? Are there activities that computers can engage in that actively draw people into relationships with them? What are the potential benefits to the people who participate in these human-computer relationships? To address these questions this work introduces a theory of Relational Agents, which are computational artifacts designed to build and maintain long-term, social-emotional relationships with their users. These can be purely software humanoid animated agents-as developed in this work-but they can also be non-humanoid or embodied in various physical forms, from robots, to pets, to jewelry, clothing, hand-helds, and other interactive devices. Central to the notion of relationship is that it is a persistent construct, spanning multiple interactions; thus, Relational Agents are explicitly designed to remember past history and manage future expectations in their interactions with users. Finally, relationships are fundamentally social and emotional, and detailed knowledge of human social psychology-with a particular emphasis on the role of affect-must be incorporated into these agents if they are to effectively leverage the mechanisms of human social cognition in order to build relationships in the most natural manner possible. People build relationships primarily through the use of language, and primarily within the context of face-to-face conversation. Embodied Conversational Agents-anthropomorphic computer characters that emulate the experience of face-to-face conversation-thus provide the substrate for this work, and so the relational activities provided by the theory will primarily be specific types of verbal and nonverbal conversational behaviors used by people to negotiate and maintain relationships. This work also provides an analysis of the types of applications in which having a human-computer relationship is advantageous to the human participant. In addition to applications in which the relationship is an end in itself (e.g., in entertainment systems), human-computer relationships are important in tasks in which the human is attempting to undergo some change in behavior or cognitive or emotional state. One such application is explored here: a system for assisting the user through a month-long health behavior change program in the area of exercise adoption. This application involves the research, design and implementation of relational agents as well as empirical evaluation of their ability to build relationships and effect change over a series of interactions with users.},
address = {New York, New York, USA},
author = {Bickmore, Timothy and Cassell, Justine},
booktitle = {Proceedings of the SIGCHI conference on Human factors in computing systems - CHI '01},
doi = {10.1145/365024.365304},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Bickmore, Cassell/2001/Bickmore, Cassell{\_}2001{\_}Relational agents A Model and Implementation of Building User Trust.pdf:pdf},
isbn = {1581133278},
keywords = {able to establish social,eases,engage their trust which,in turn,relationships with,this sort must be,users in order to},
number = {3},
pages = {396--403},
publisher = {ACM Press},
title = {{Relational agents: A Model and Implementation of Building User Trust}},
url = {http://portal.acm.org/citation.cfm?doid=365024.365304},
volume = {PhD Thesis},
year = {2001}
}
@misc{Salem2015a,
abstract = {In an effort to increase the acceptance and persuasiveness of socially assistive robots in home and healthcare envi- ronments, HRI researchers attempt to identify factors that promote human trust and perceived safety with regard to robots. Especially in collaborative contexts in which hu- mans are requested to accept information provided by the robot and follow its suggestions, trust plays a crucial role, as it is strongly linked to persuasiveness. As a result, human- robot trust can directly affect people's willingness to coop- erate with the robot, while under- or overreliance could have severe or even dangerous consequences. Problematically, in- vestigating trust and human perceptions of safety in HRI experiments is not a straightforward task and, in light of a number of ethical concerns and risks, proves quite challeng- ing. This position statement highlights a few of these points based on experiences from HRI practice and raises a few important questions that HRI researchers should consider.},
author = {Salem, Maha and Dautenhahn, Kerstin},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Salem, Dautenhahn/2015/Salem, Dautenhahn{\_}2015{\_}Evaluating Trust and Safety in HRI Practical Issues and Ethical Challenges.pdf:pdf},
keywords = {cooperation,socially assistive robots,trust and safety},
title = {{Evaluating Trust and Safety in HRI: Practical Issues and Ethical Challenges}},
url = {http://hdl.handle.net/2299/16336},
year = {2015}
}
@article{Lewis1985,
abstract = {Although trust is an underdeveloped concept in sociology, promising theoretical formulations are available in the recent work of Luhmann and Barber. This socio- logica! version complements the psychological and attitudinal conceptualizations of experimental and survey researchers. Trust is seen to include both emotional and cognitive dimensions and to function as a deep assumption underwriting so- cial order. Contemporary examples such as lying, family exchange, monetary atti- tudes, and litigation illustrate the centrality of trust as a sociological reality.},
author = {Lewis, J David and Weigert, Andrew},
doi = {10.1093/sf/63.4.967},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Lewis, Weigert/1985/Lewis, Weigert{\_}1985{\_}Trust as a Social Reality.pdf:pdf},
isbn = {00377732},
issn = {0037-7732},
journal = {Social Forces},
month = {jun},
number = {4},
pages = {967--985},
pmid = {5283486},
title = {{Trust as a Social Reality}},
url = {http://www.jstor.org/stable/2578601$\backslash$nhttp://www.jstor.org/stable/2578601?seq=1{\&}cid=pdf-reference{\#}references{\_}tab{\_}contents$\backslash$nhttp://www.jstor.org/page/info/about/policies/terms.jsp http://sf.oxfordjournals.org/cgi/doi/10.1093/sf/63.4.967},
volume = {63},
year = {1985}
}
@article{Prada2014,
author = {Prada, R. and Paiva, A.},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Prada, Paiva/2014/Prada, Paiva{\_}2014{\_}Human-agent interaction Challenges for bringing humans and agents together.pdf:pdf},
journal = {In Proc. of the 3rd Int. Workshop on Human-Agent Interaction Design and Models (HAIDM 2014)},
pages = {1--10},
title = {{Human-agent interaction: Challenges for bringing humans and agents together}},
year = {2014}
}
@article{Jones1997,
abstract = {Introduction Computer Supported Cooperative Work (CSCW) (Ellis et al., 1991) is ostensibly concerned with supporting the activities of work groups through the use of computer technology. However, to date, CSCW systems (groupware) have emphasised technological issues of support at the expense of social issues such as relationships, roles and social protocols. We postulate that this situation has arisen because the majority of groupware designers are technologists who have both the experience and tools to develop new and effective hardware and software. Unfortunately they do not have tools or experience to effectively analyse and provide support for social facets of group working. Multidisciplinary development teams may contain group work experts, but common languages and vocabulary for precise communication regarding social and relationship aspects of systems are lacking. Groupware designers and developers also require tools to embed their considerations of social issues in systems and then to analyse those systems and the work of the groups which use them. We have attempted to ameliorate this situation by developing a formal notation of the trust that is present between individuals in collaborative activities. The notation can be used in the representation and consideration of social relationships in the context of CSCW. We suggest that trust is a key factor in the efficacy of both intra-group and inter-group activities, and that it can be formalised and then exploited in the design and analysis of CSCW systems. We call our formal description Trust in order to differentiate it from wider definitions. Potential uses of Trust in a group work context include the following: l it can be used as a tool for the discussion of the design of CSCW systems; l it can be embedded in computer systems to mediate cooperative computer based activities; l it can be used to record and analyze group activity; l it provides a tool for the discussion and clarification of trust, and its role in group activities. The development of the formalism addresses the need for support beyond technical issues for designers involved in the development of multi-user-centered systems.},
author = {Jones, Steve and Marsh, Steve},
doi = {10.1145/264853.264872},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Jones, Marsh/1997/Jones, Marsh{\_}1997{\_}Human-computer-human interaction.pdf:pdf},
issn = {07366906},
journal = {ACM SIGCHI Bulletin},
keywords = {SIGCHI BULLETIN},
month = {jul},
number = {3},
pages = {36--40},
title = {{Human-computer-human interaction}},
url = {http://portal.acm.org/citation.cfm?doid=264853.264872},
volume = {29},
year = {1997}
}
@article{Carter2002,
abstract = {We propose that through the formalization of concepts related to trust, a more accurate model of trust can be implemented. This paper presents a new model of trust that is based on the formalization of reputation. A multidisciplinary approach is taken to understanding the nature of trust and its relation to reputation. Through this approach, a practical definition of reputation is adopted from sociological contexts and a model of reputation is designed and presented. Reputation is defined as role fulfillment. To formalize reputation, it is necessary to formalize the expectations placed upon an agent within a particular multi-agent system (MAS). In this case, the agents are part of an informationsharing society. Five roles are defined along with the ways in which these roles are objectively fulfilled. Through the measurement of role fulfillment, a vector representing reputation can be developed. This vector embodies the magnitude of the reputation and describes the patterns of behavior associated with the direction of the vector. Experiments are conducted to verify the sensibility of the proposed models for role fulfillment and overall reputation. The simulation results show that the roles, defined for building reputation in an information-sharing MAS environment, react to different agent and user actions in a manner consistent with the formal definitions.},
annote = {Offers a formalization of Reputation. Defines it as one of the dimensions of trust.},
author = {Carter, Jonathan and Bitting, Elijah and Ghorbani, Ali A.},
doi = {10.1111/1467-8640.t01-1-00201},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Carter, Bitting, Ghorbani/2002/Carter, Bitting, Ghorbani{\_}2002{\_}Reputation Formalization for an Information-Sharing Multi-Agent System.pdf:pdf},
isbn = {1011161665},
issn = {0824-7935},
journal = {Computational Intelligence},
keywords = {agent,information sharing,multi-agent systems,reputation,trust},
month = {nov},
number = {4},
pages = {515--534},
title = {{Reputation Formalization for an Information-Sharing Multi-Agent System}},
url = {http://doi.wiley.com/10.1111/1467-8640.t01-1-00201},
volume = {18},
year = {2002}
}
@incollection{Simpson2007,
abstract = {(from the chapter) The first section of the chapter reviews basic definitions, conceptualizations, and operationalizations of interpersonal trust. After reviewing some of the linguistic origins of trust, both individualistic (dispositional) and interpersonal (dyadic) definitions and conceptualizations of trust are presented. The second section highlights some of the major theoretical foundations and bases of trust at different levels of conceptual analysis. At the ultimate level of analysis, traditional genetic evolutionary models relevant to trust as well as multilevel selection/cultural coevolutionary models are showcased. At the ontogenetic level, some prominent lifespan models of social and personality development that are most pertinent to interpersonal trust are highlighted. At the proximate level, a few of the most significant social and psychological processes bearing on trust are outlined. Following this, major models specifying the normative (i.e., typical or modal) and individual-difference processes believed to govern the development, maintenance, and deterioration of trust in close relationships are discussed. The third section provides a selective yet representative overview of research on trust, with most attention focusing on interpersonal (rather than intergroup) trust. This overview begins with the seminal contributions of Deutsch and the early Prisoner's Dilemma Game (PDG) studies conducted prior to the mid-1960s, progresses to the dispositional movement that was popular from the late 1960s through the mid 1970s, and concludes with more recent dyadic formulations of trust. In the final section, six core principles of trust are identified. Following this, important constructs from different interpersonal models are merged to form an integrative process model, which suggests how trust might develop and be maintained in relationships. (PsycINFO Database Record (c) 2012 APA, all rights reserved) (chapter)},
author = {Simpson, Jeffry A.},
booktitle = {Social psychology: Handbook of basic principles (2nd ed.).},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Simpson/2007/Simpson{\_}2007{\_}Foundations of interpersonal trust.pdf:pdf},
isbn = {1-57230-918-0},
keywords = {Interpersonal Interaction,Trust (Social Behavior),interpersonal trust},
pages = {587--607},
title = {{Foundations of interpersonal trust}},
year = {2007}
}
@article{Billings2012a,
author = {Billings, Deborah R. and Schaefer, Kristin E. and Chen, Jessie Y.C. and Hancock, Peter A.},
doi = {10.1145/2157689.2157709},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Billings et al/2012/Billings et al.{\_}2012{\_}Human-robot interaction(2).pdf:pdf},
isbn = {9781450310635},
issn = {2167-2121},
journal = {Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction - HRI '12},
number = {January},
pages = {109},
title = {{Human-robot interaction}},
url = {http://dl.acm.org/citation.cfm?doid=2157689.2157709},
year = {2012}
}
@article{JoyceBergJohnDickhaut,
abstract = {We designed an experiment to study trust and reciprocity in an investment setting. This design controls for alternative explanations of behavior including repeat game reputation effects, contractual precommitments, and punishment threats. Observed decisions suggest that reciprocity exists as a basic element of human behavior and that this is accounted for in the trust extended to an anonymous counterpart. A second treatment, social history, identifies conditions which strengthen the relationship between trust and reciprocity.},
author = {Berg, Joyce and Dickhaut, John and McCabe, Kevin},
doi = {10.1006/game.1995.1027},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Berg, Dickhaut, McCabe/1995/Berg, Dickhaut, McCabe{\_}1995{\_}Trust, Reciprocity, and Social History.pdf:pdf},
journal = {Games and Economic Behavior},
number = {1},
pages = {122--142},
title = {{Trust, Reciprocity, and Social History}},
volume = {10},
year = {1995}
}
@article{Fogg1997,
abstract = {We conducted an experiment to investigate if computers could motivate users to change their behavior. By leveraging a social dynamic called the "rule of reciprocity," this experiment demonstrated that users provided more helping behavior to a computer that had helped them previously than to a different computer. Users also worked longer, performed higher quality work, and felt happier. Conversely, the data provide evidence of a retaliation effect.},
author = {Fogg, B J and Nass, Clifford},
doi = {10.1145/1120212.1120419},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Fogg, Nass/1997/Fogg, Nass{\_}1997{\_}How Users Reciprocate to Computers An experiment that demonstrates behavior change.pdf:pdf},
isbn = {0897919262},
journal = {CHI EA '97 CHI '97 Extended Abstracts on Human Factors in Computing Systems},
keywords = {agents,computers are social actors,empirical studies,equation,experiments,influence,media,persuasion,reciprocity,retaliation,social dynamics},
number = {March},
pages = {331--332},
title = {{How Users Reciprocate to Computers : An experiment that demonstrates behavior change}},
year = {1997}
}
@incollection{Adali2013,
address = {New York, NY},
author = {Adali, Sibel},
booktitle = {Modeling Trust Context in Networks},
doi = {10.1007/978-1-4614-7031-1_2},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Adali/2013/Adali{\_}2013{\_}Trust as a Computational Concept.pdf:pdf},
isbn = {978-1-4614-7030-4},
pages = {5--24},
publisher = {Springer New York},
series = {SpringerBriefs in Computer Science},
title = {{Trust as a Computational Concept}},
url = {http://link.springer.com/10.1007/978-1-4614-7031-1 http://link.springer.com/10.1007/978-1-4614-7031-1{\_}2},
year = {2013}
}
@article{Correia,
author = {Correia, Filipa},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Correia/Unknown/Correia{\_}Unknown{\_}EMYS a social robot that plays “ Sueca ”.pdf:pdf},
keywords = {artificial intelligence,formation,hidden in-,interactive companions,socially intelligent behaviour,trick-taking card game},
title = {{EMYS : a social robot that plays “ Sueca ”}}
}
@article{Huynh2006,
abstract = {Abstract Trust and reputation are central to effective interactions in open multi-agent systems (MAS) in which agents, that are owned by a variety of stakeholders, continuously enter and leave the system. This openness means existing trust and reputation models cannot readily be used since their performance suffers when there are various (unforseen) changes in the environment. To this end, this paper presents FIRE, a trust and reputation model that integrates a number of information sources to produce a comprehensive assessment of an agents likely performance in open systems. Specifically, FIRE incorporates interaction trust, role-based trust, witness reputation, and certified reputation to provide trust metrics in most circumstances. FIRE is empirically evaluated and is shown to help agents gain better utility (by effectively selecting appropriate interaction partners) than our benchmarks in a variety of agent populations. It is also shown that FIRE is able to effectively respond to changes that occur in an agents environment.},
annote = {AKA Fire},
author = {Huynh, Trung Dong and Jennings, Nicholas R. and Shadbolt, Nigel R.},
doi = {10.1007/s10458-005-6825-4},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Huynh, Jennings, Shadbolt/2006/Huynh, Jennings, Shadbolt{\_}2006{\_}An integrated trust and reputation model for open multi-agent systems.pdf:pdf},
isbn = {1387-2532},
issn = {13872532},
journal = {Autonomous Agents and Multi-Agent Systems},
keywords = {Multi-agent systems,Reputation,Trust},
number = {2},
pages = {119--154},
title = {{An integrated trust and reputation model for open multi-agent systems}},
volume = {13},
year = {2006}
}
@article{Abdul-rahman2000,
author = {Abdul-rahman, Alfarez and Hailes, Stephen},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Abdul-rahman, Hailes/2000/Abdul-rahman, Hailes{\_}2000{\_}Supporting Trust in Virtual Communities.pdf:pdf},
isbn = {0769504930},
journal = {System Sciences, 2000. Proceedings of the 33rd Annual Hawaii International Conference on},
number = {c},
pages = {1--9},
title = {{Supporting Trust in Virtual Communities}},
volume = {00},
year = {2000}
}
@article{Rousseau1998,
abstract = {none},
author = {Rousseau, Denise and Sitkin, Sim and Burt, Ronald and Camerer, Colin},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Rousseau et al/1998/Rousseau et al.{\_}1998{\_}Not so different after all A cross-discipline view of trust.pdf:pdf},
issn = {0042-0980},
journal = {Academy of Management Review},
number = {3},
pages = {393--404},
title = {{Not so different after all: A cross-discipline view of trust.}},
volume = {23},
year = {1998}
}
@article{Desai2009,
abstract = {One of the most significant challenges of human-robot interaction research is designing systems which foster an appropriate level of trust in their users: in order to use a robot effectively and safely, a user must place neither too little nor too much trust in the system. In order to better understand the factors which influence trust in a robot, we present a survey of prior work on trust in automated systems.We also discuss issues specific to robotics which pose chal- lenges not addressed in the automation literature, particularly related to reliability, capability, and adjustable autonomy.We conclude with the results of a preliminary web-based questionnaire which illustrate some of the biases which autonomous robots may need to overcome in order to promote trust in users.},
author = {Desai, Munjal and Stubbs, Kristen and Steinfeld, Aaron and Yanco, Holly a.},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Desai et al/2009/Desai et al.{\_}2009{\_}Creating Trustworthy Robots Lessons and Inspirations from Automated Systems.pdf:pdf},
isbn = {1902956850},
journal = {Robotics Institute},
title = {{Creating Trustworthy Robots: Lessons and Inspirations from Automated Systems}},
url = {http://holman.cs.uml.edu/fileadmin/content/publications/2009/desai{\_}paper.pdf},
year = {2009}
}
@incollection{Gambetta1988,
annote = {Provides most cited defiinition of trust, but in contrast with the definition provided by Castelfranchi

While this one is more statistical/numeric based, Castelfranchi provides one more logical based.},
author = {Gambetta, Diego},
booktitle = {Trust: Making and Breaking Cooperative Relations},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Gambetta/1988/Gambetta{\_}1988{\_}Can We Trust Trust.pdf:pdf},
pages = {213--237},
publisher = {Blackwell},
title = {{Can We Trust Trust?}},
url = {http://sieci.pjwstk.edu.pl/media/bibl/[Gambetta]{\_}[Can We]{\_}[Trust]{\_}[1988].pdf},
year = {1988}
}
@article{Bradshaw2011,
author = {Bradshaw, Jeffrey M and Feltovich, Paul and Johnson, Matthew},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Bradshaw, Feltovich, Johnson/2011/Bradshaw, Feltovich, Johnson{\_}2011{\_}Human-Agent Interaction.pdf:pdf},
isbn = {978-1-4094-8660-2},
journal = {Handbook of HumanMachine Interaction},
pages = {293--302},
title = {{Human-Agent Interaction}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=4opHlu05SNIC{\&}oi=fnd{\&}pg=PA283{\&}dq=Human-agent+interaction{\&}ots=vxrpDdLbSa{\&}sig=07dujtzGjIcBLlZ6FVH33HjrWos},
year = {2011}
}
@article{Kidd2004,
abstract = {Social robots are robots that help people as capable partners rather than as tools, are believed to be of greatest use for applications in entertainment, education, and healthcare because of their potential to be perceived as trusting, helpful, reliable, and engaging. This paper explores how the robot's physical presence influences a person's perception of these characteristics. The first study reported here demonstrates the differences between a robot and an animated character in terms a person's engagement and perceptions of the robot and character. The second study shows that this difference is a result of the physical presence of the robot and that a person's reactions would be similar even if the robot is not physically collocated. Implications to the design of socially communicative and interactive robots are discussed.},
author = {Kidd, C.D. and Breazeal, C.},
doi = {10.1109/IROS.2004.1389967},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Kidd, Breazeal/2004/Kidd, Breazeal{\_}2004{\_}Effect of a robot on user perceptions.pdf:pdf},
isbn = {0-7803-8463-6},
issn = {0780384636},
journal = {2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)},
keywords = {- human-robot inferaction,ace,affeeeh,as with other,aspects of social robots,emotion ond,in an interaction,mcr studies,of engaging the person,robofs,that affect,there are many issues,ve user inte},
pages = {3559--3564},
title = {{Effect of a robot on user perceptions}},
volume = {4},
year = {2004}
}
@article{Tseng1999,
abstract = {For most of computing's brief history, people have held computers in high regard. A quick review of the popular culture from the past few decades reflects people's general confidence in computing systems. In cinema and lit- erature, computers are often portrayed as infallible sidekicks in the service of humanity. In the consumer realm, computer-based information and services have been marketed as bet- ter, more reliable, and more credible sources of information than humans. Consider, for example, computerized weather prediction, computerized automotive analysis, and so- called computer dating. In these and other areas, the public has generally been led to believe that if a computer said it or produced it, it was believable},
author = {Tseng, Shawn and Fogg, B. J.},
doi = {10.1145/301353.301402},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Tseng, Fogg/1999/Tseng, Fogg{\_}1999{\_}Credibility and computing technology.pdf:pdf},
isbn = {00010782},
issn = {00010782},
journal = {Communications of the ACM},
number = {5},
pages = {39--44},
title = {{Credibility and computing technology}},
volume = {42},
year = {1999}
}
@article{Burnett2011,
abstract = {Trust is crucial in dynamic multi-agent systems where agents may frequently join and leave, and the structure of the society may often change. In these environments, it may be difficult for agents to form stable trust relationships necessary for confident interactions. Societies may break down when trust between agents is too low to motivate interactions. In such settings, agents should make decisions about who to interact with, given their degree of trust in the available partners. We propose a decision-theoretic model of trust decision making allows controls to be used, as well as trust, to increase confidence in initial interactions. We consider explicit incentives, monitoring and reputation as examples of such controls. We evaluate our approach within a simulated, highly-dynamic multiagent environment, and show how this model supports the making of delegation decisions when trust is low.},
annote = {Decision making in who to delegate to. Not what to do, not worth it},
author = {Burnett, Chris and Norman, Timothy J. and Sycara, Katia},
doi = {10.5591/978-1-57735-516-8/IJCAI11-031},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Burnett, Norman, Sycara/2011/Burnett, Norman, Sycara{\_}2011{\_}Trust decision-making in multi-agent systems.pdf:pdf},
isbn = {9781577355120},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Agent-Based and Multiagent Systems},
pages = {115--120},
title = {{Trust decision-making in multi-agent systems}},
year = {2011}
}
@article{Zacharia2000,
abstract = {The members of electronic communities are often unrelated to each other; they may have never met and have no information on each other's reputation. This kind of information is vital in electronic commerce interactions, where the potential counterpart's reputation can be a significant factor in the negotiation strategy. Two complementary reputation mechanisms are investigated which rely on collaborative rating and personalized evaluation of the various ratings assigned to each user. While these reputationmechanisms are developed in the context of electronic commerce, it is believed that they may have applicability in other types of electronic communities such as chatrooms, newsgroups, mailing lists, etc.},
annote = {Example of application of trust models in ecommerce},
author = {Zacharia, Giorgos and Maes, Pattie},
doi = {10.1080/08839510050144868},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Zacharia, Maes/2000/Zacharia, Maes{\_}2000{\_}Trust management through reputation mechanisms.pdf:pdf},
isbn = {0218-8430},
issn = {0883-9514},
journal = {Applied Artificial Intelligence},
month = {oct},
number = {9},
pages = {881--907},
title = {{Trust management through reputation mechanisms}},
url = {http://www.tandfonline.com/doi/abs/10.1080/08839510050144868},
volume = {14},
year = {2000}
}
@book{Reeves1998a,
annote = {Cite about how people see computers as personified entities, even if unconciously},
author = {Reeves, Byron and Nass, Clifford},
booktitle = {Cambridge University Press},
doi = {10.1109/MSPEC.1997.576013},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Martin/1997/Martin{\_}1997{\_}The Media Equation How People Treat Computers, Television and New Media Like Real People and Places Book Review.pdf:pdf},
isbn = {1-57586-053-8},
issn = {0018-9235},
month = {mar},
pages = {19--36},
title = {{The Media Equation: How People Treat Computers, Television, and New Media Like Real People and Places}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=576013},
year = {1998}
}
@article{Rao1995,
abstract = {The study of computational agents capable of rational behaviour has received a great deal of attention in recent years. Theoretical formal- izations of such agents and their implementations have proceeded in parallel with little or no connection between them. This paper explores a particular type of rational agent, a Belief-Desire-Intention (BDI) agent. The primary aim of this paper is to integrate (a) the theoretical foundations of BDI agents from both a quantitative decision-theoretic perspective and a symbolic reasoning perspective; (b) the implementations of BDI agents from an ideal theoretical perspective and a more practical perspective; and (c) the building of large-scale applications based on BDI agents. In particular, an air-traffic management application will be described from both a theoretical and an implementation perspective.},
author = {Rao, Anand S and Georgeff, Michael P},
doi = {10.1.1.51.9247},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Rao, Georgeff/1995/Rao, Georgeff{\_}1995{\_}BDI agents From theory to practice.pdf:pdf},
isbn = {0262621029},
journal = {Icmas},
keywords = {Multi-agent system,agent,belief desire intention},
pages = {312--319},
title = {{BDI agents: From theory to practice.}},
volume = {95},
year = {1995}
}
@article{Stanton2014,
abstract = {Between people, eye gaze and other forms of nonverbal communication can influence trust. We hypothesised similar effects would occur during human-robot interaction, predicting a humanoid robot's eye gaze and lifelike bodily movements (eye tracking movements and simulated “breathing”) would increase participants' likelihood of seeking and trusting the robot's opinion in a cooperative visual tracking task. However, we instead found significant interactions between robot gaze and task difficulty, indicating that robot gaze had a positive impact upon trust for difficult decisions and a negative impact for easier decisions. Furthermore, a significant effect of robot gaze was found on task performance, with gaze improving participants' performance on easy trials but hindering performance on difficult trials. Participants also responded significantly faster when the robot looked at them. Results suggest that robot gaze exerts “pressure” upon participants, causing audience effects similar to social facilitation and inhibition. Lifelike bodily movements had no significant effect upon participant behaviour.},
author = {Stanton, Christopher and Stevens, Catherine J.},
doi = {10.1007/978-3-319-11973-1},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Stanton, Stevens/2014/Stanton, Stevens{\_}2014{\_}Robot Pressure The Impact of Robot Eye Gaze and Lifelike Bodily Movements upon Decision-Making and Trust.pdf:pdf},
isbn = {978-3-319-11972-4},
issn = {16113349},
journal = {Social Robotics: 6th International Conference, ICSR 2014, Sydney, NSW, Australia, October 27-29, 2014},
keywords = {compliance,eye,gaze,human-robot interaction,nonverbal communication,persuasion,trust},
pages = {330--339},
title = {{Robot Pressure: The Impact of Robot Eye Gaze and Lifelike Bodily Movements upon Decision-Making and Trust}},
url = {http://link.springer.com/10.1007/978-3-319-11973-1},
volume = {8755},
year = {2014}
}
@article{Steinfeld2006,
author = {Steinfeld, A and Fong, T and Kaber, D},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Steinfeld, Fong, Kaber/2006/Steinfeld, Fong, Kaber{\_}2006{\_}Common metrics for human-robot interaction.pdf:pdf},
isbn = {1595932941},
journal = {{\ldots}  on Human-robot  {\ldots}},
keywords = {human-robot interaction,metrics,unmanned ground vehicles},
title = {{Common metrics for human-robot interaction}},
url = {http://dl.acm.org/citation.cfm?id=1121249},
year = {2006}
}
@article{Core2006,
abstract = {Although the representation of physical environments and behaviors will continue to play an important role in simulation-based training, an emerging challenge is the representation of virtual humans with rich mental models (e.g., including emotions, trust) that interact through conversational as well as physical behaviors. The motivation for such simulations is training soft skills such as leadership, cultural awareness, and negotiation, where the majority of actions are conversational, and the problem solving involves consideration of the emotions, attitudes, and desires of others. The educational power of such simulations can be enhanced by the integration of an intelligent tutoring system to support learners' understanding of the effect of their actions on virtual humans and how they might improve their performance. In this paper, we discuss our efforts to build such virtual humans, along with an accompanying intelligent tutor, for the domain of negotiation and cultural awareness. {\textcopyright} 2007 The Society for Modeling and Simulation International.},
author = {Core, M. and Traum, D. and Lane, H. C. and Swartout, W. and Gratch, J. and van Lent, M. and Marsella, S.},
doi = {10.1177/0037549706075542},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Core et al/2006/Core et al.{\_}2006{\_}Teaching Negotiation Skills through Practice and Reflection with Virtual Humans.pdf:pdf},
isbn = {0037-5497},
issn = {0037-5497},
journal = {SIMULATION},
keywords = {conversation strategies,emotion modeling,explanation systems,intelligent tutoring systems,negotiation training,virtual humans},
month = {nov},
number = {11},
pages = {685--701},
title = {{Teaching Negotiation Skills through Practice and Reflection with Virtual Humans}},
url = {http://sim.sagepub.com/cgi/doi/10.1177/0037549706075542},
volume = {82},
year = {2006}
}
@article{Gal2010,
abstract = {Computer systems increasingly carry out tasks in mixed networks, that is in group settings in which they interact both with other computer systems and with people. Participants in these heterogeneous human-computer groups vary in their capabilities, goals, and strategies; they may cooperate, collaborate, or compete. The presence of people in mixed networks raises challenges for the design and the evaluation of decision-making strategies for computer agents. This paper describes several new decision-making models that represent, learn and adapt to various social attributes that influence people's decision-making and presents a novel approach to evaluating such models. It identifies a range of social attributes in an open-network setting that influence people's decision-making and thus affect the performance of computer-agent strategies, and establishes the importance of learning and adaptation to the success of such strategies. The settings vary in the capabilities, goals, and strategies that people bring into their interactions. The studies deploy a configurable system called Colored Trails (CT) that generates a family of games. CT is an abstract, conceptually simple but highly versatile game in which players negotiate and exchange resources to enable them to achieve their individual or group goals. It provides a realistic analogue to multi-agent task domains, while not requiring extensive domain modeling. It is less abstract than payoff matrices, and people exhibit less strategic and more helpful behavior in CT than in the identical payoff matrix decision-making context. By not requiring extensive domain modeling, CT enables agent researchers to focus their attention on strategy design, and it provides an environment in which the influence of social factors can be better isolated and studied. ?? 2010 Elsevier B.V.},
author = {Gal, Ya'Akov and Grosz, Barbara and Kraus, Sarit and Pfeffer, Avi and Shieber, Stuart},
doi = {10.1016/j.artint.2010.09.002},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Gal et al/2010/Gal et al.{\_}2010{\_}Agent decision-making in open mixed networks.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Human-Computer decision-making,Negotiation},
month = {dec},
number = {18},
pages = {1460--1480},
publisher = {Elsevier B.V.},
title = {{Agent decision-making in open mixed networks}},
url = {http://dx.doi.org/10.1016/j.artint.2010.09.002 http://linkinghub.elsevier.com/retrieve/pii/S0004370210001451},
volume = {174},
year = {2010}
}
@article{Lashkari1994,
abstract = {Interface agents are semi-intelligent systems which assist users with$\backslash$ndaily computer-based tasks. Recently, various researchers have proposed$\backslash$na learning approach towards building such agents and some working$\backslash$nprototypes have been demonstrated. Such agents learn by `watching$\backslash$nover the shoulder' of the user and detecting patterns and regularities$\backslash$nin the user's behavior. Despite the successes booked, a major problem$\backslash$nwith the learning approach is that the agent has to learn from scratch$\backslash$nand thus takes some time becoming useful. Secondly, the agent's competence$\backslash$nis necessarily limited to actions it has seen the user perform. Collaboration$\backslash$nbetween agents assisting different users can alleviate both of these$\backslash$nproblems. We present a framework for multi-agent collaboration and$\backslash$ndiscuss results of a working prototype, based on learning agents$\backslash$nfor electronic mail.},
author = {Lashkari, Yezdi and Metral, Max and Maes, Pattie},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Lashkari, Metral, Maes/1994/Lashkari, Metral, Maes{\_}1994{\_}Collaborative interface agents.pdf:pdf},
isbn = {0-262-61102-3},
journal = {AAAI '94: Proceedings of the twelfth national conference on Artificial intelligence},
pages = {444--449},
title = {{Collaborative interface agents}},
volume = {1},
year = {1994}
}
@article{Castelfranchi2001a,
abstract = {After arguing about the crucial importance of trust for Agents and MAS, we provide a definition of trust both as a mental state and as a social attitude and relation. We present the mental ingredients of trust: its specific beliefs and goals, with special attention to evaluations and expectations. We show the relation between trust and the mental background of delegation. We explain why trust is a bet, and implies some risks, and analyse the more complex forms of social trust, based on a theory of mind and in particular on morality, reputation and disposition, and authority (three party trust). We explain why promises, contracts, authorities can increase our trust by modifying our mental representations. We present a principled quantification of trust, based on its cognitive ingredients, and use this "degree of trust" as the basis for a rational decision to delegate or not to another agent. We explain when trust is rational, and why it is not an irrational decision by definition. We also criticise the economic and game-theoretic view of trust for underestimating the importance of cognitive ingredients of trust and for reducing it to subjective probability and risk. The paper is intended to contribute both to the conceptual analysis and to the practical use of trust in social theory and MAS.},
annote = {A follow-up paper to Castelfranchi,1998},
author = {Castelfranchi, Cristiano and Falcone, Rino},
doi = {10.1007/978-94-017-3614-5_3},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Castelfranchi, Falcone/2001/Castelfranchi, Falcone{\_}2001{\_}Social Trust A Cognitive Approach.pdf:pdf},
isbn = {079236919X},
journal = {Trust and deception in virtual societies},
pages = {55--90},
title = {{Social Trust : A Cognitive Approach}},
year = {2001}
}
@article{Goodrich2007,
abstract = {Human-Robot Interaction (HRI) has recently received considerable attention in the academic community, in labs, in technology companies, and through the media. Because of this attention, it is desirable to present a survey of HRI to serve as a tutorial to people outside the field and to promote discussion of a unified vision of HRI within the field. The goal of this review is to present a unified treatment of HRI-related problems, to identify key themes, and discuss challenge problems that are likely to shape the field in the near future. Although the review follows a survey structure, the goal of presenting a coherent “story” of HRI means that there are necessarily some well-written, intriguing, and influential papers that are not referenced. Instead of trying to survey every paper, we describe the HRI story from multiple perspectives with an eye toward identifying themes that cross applications. The survey attempts to include papers that represent a fair cross section of the universities, government efforts, industry labs, and countries that contribute to HRI, and a cross section of the disciplines that contribute to the field, such as human, factors, robotics, cognitive psychology, and design.},
author = {Goodrich, Michael a. and Schultz, Alan C.},
doi = {10.1561/1100000005},
file = {:C$\backslash$:/Users/nunox{\_}000/Work/Bibliography/Goodrich, Schultz/2007/Goodrich, Schultz{\_}2007{\_}Human-Robot Interaction A Survey.pdf:pdf},
isbn = {9781601980922},
issn = {1551-3955},
journal = {Foundations and Trends{\textregistered} in Human-Computer Interaction},
number = {3},
pages = {203--275},
title = {{Human-Robot Interaction: A Survey}},
url = {http://www.nowpublishers.com/article/Details/HCI-005},
volume = {1},
year = {2007}
}
