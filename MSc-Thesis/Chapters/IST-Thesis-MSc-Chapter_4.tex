% #############################################################################
% This is Chapter 4
% !TEX root = ../main.tex
% #############################################################################
% Change the Name of the Chapter i the following line
\fancychapter{Trust Model}
%\cleardoublepage
% The following line allows to ref this chapter
\label{chap:TrustModel}

We sought out to develop a trust model definition that would be easily implementable, but generic enough to be able to adapt to various testing scenarios. To do this we took inspiration from the work by Sabater et al. \cite{Sabater2006} described in Section \ref{subsec:Related work:Trust Models:Repage}, by taking a similar approach to architecture where a central memory component holds the model's current state, getting updated by inputs received from the environment. But while Repage describes a third module that suggests actions to resolve belief conflicts in the model, we instead defined such module to assume the point of view of one of the agents in the scenario and, if granted an opportune moment, it suggests actions to improve the trust relationship with a trustor. In fact, most of the design of the model was made with the intent that it would be used by one of the agents in the scenario, where the model created would be his own trust model of the world environment. And so, the model is composed by 3 main components, represented in Figure \ref{fig:ModelArchitecture}, and described as follows:
\begin{itemize}
    \item \textbf{Memory}, which defines and stores the main model structure;
    \item \textbf{Perceptions}, a series of environment inputs mapped to changes in the Memory;
    \item \textbf{Action Suggestion}, a module that outputs different actions depending on current inputs and the state of the model.
\end{itemize}

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/ModelDiagram.jpg}
    \caption{Model Architecture with brief descriptions, their interactions with the scenario and what they contain.}
    \label{fig:ModelArchitecture}
\end{figure}


\section{Memory}
\label{sec:Memory}
One of the main concerns while designing the model was how trust would be calculated, as we wanted to use Castlefranchi and Falcone's conceptualization of trust \cite{Castelfranchi2010} as a basis for our definition of trust, focusing specially on it being dependent on the task entrusted, and the transferability of trust between different tasks. But starting from the five-part definition of trust, as seen in Equation \ref{eq:TrustRelation}, we decided that inserting context (\textbf{C}) and the trustor's goal (\bm{$g_x$}) into the model would bring in too much complexity for the scope of this thesis, as it would require for a world state model to be kept, as well as some way to predict the trustor's goal. So we simplified, defining trust through a simpler three-part relation, involving just the trustor (\textbf{X}), the trustee (\textbf{Y}) and the task ($\bm{\tau}$), represented in Equation \ref{eq:TrustCalc}.
\begin{equation}
TRUST(X\ Y\ \tau)
\label{eq:TrustCalc}
\end{equation}

So we designed the structure with the concepts and relations represented in Figure \ref{fig:MemoryArchitecture}, and we can describe them as follows:

\begin{itemize}
    \item \textbf{Agent}: a simple representation of a known entity in the scenario world space, serving mostly as an identifier for the entity and a container for the other agents it has information about, represented as Trustees;
    \item \textbf{Trustee}: each agent contains a collection of other agents it has information about, either by reputation, or by interaction, which we represent as their Trustees;
    \item \textbf{Trust Feature}: a piece of information a trustor has on a trustee is represented in a Trust Feature, which contains the Belief Sources of said information. The Feature Model defines and uniquely identifies what feature is represented.
    \item \textbf{Feature Model}: the possible set of trust features from which a trustee can be assigned is defined in a collection of Feature Models where each one uniquely identifies a possible piece of trust related information relevant to the model scenario (e.g. The trustee's ability to cook, or the willingness to drive);
    \item \textbf{Category}: a Feature Model must belong to a Category, making it easier to  present the different type of Trust Features. This is usually intended to separate features between those relating to Ability and those related to Willingness.
    \item \textbf{Belief Source}: this represents a source of information on the corresponding feature, belonging to one of the 3 sub-classes depending on the origin of the information, Reputation for when reported from other agents (whether directly (e.g. talking) or indirectly (e.g. report on newspaper)), Bias for pre-existing beliefs on the feature, and Direct Contact for direct observations of the trustee. 3 values are contained to determine the associated feature's belief value: 
    \begin{itemize}
        \item Belief Value, a number between 0.0 and 1.0 describing the trustor evaluation;
        \item Certainty describes how well the trustee was evaluated, in Reputation for instance, this might represent how well we trust in the reporter, and in Direct Contact how well the trustor observed the trustee performing said feature;
        \item Time is just a record of when was this belief source recorded, as older records might have a lower impact in the overall belief value score, compared to newer records.
    \end{itemize} 
    \item \textbf{Task}: a representation of the possible delegation tasks in the scenario, containing the Feature Models associated with the performance of this task (e.g. The ability to serve drinks if the task is bartending). A weight is given to each Feature corresponding to its importance in the task. The various weights are normalized so that their sum is 1.0.
\end{itemize} 

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/TrustMemoryDiagram.jpg}
    \caption{Memory Architecture (represented in UML)}
    \label{fig:MemoryArchitecture}
\end{figure}


\subsection{Trust Calculation}
\label{subsec:TrustCalculation}
Taking a Trustor $X$, a Trustee $Y$ and a delegated task $\tau$, Trust can then be calculated by taking the Trustee's Trust Features $F_y$, the Task's Feature Models $F_\tau$ and checking which they have in common, which we can represent as $F_{y\cap\tau}$~. Remember that Trust Features are uniquely identified by a Feature Model. So after getting $F_{y\cap\tau}$ we can apply a linear function to each of the features in $F_{y\cap\tau}$, where for each element $F_i$ we multiply the trustee's feature's belief value $B(F_i)$ with the weight of the feature for the task $W(F_i)$, as represented in Equation \ref{eq:TrustCalculation}.
\begin{equation}
Trust_{X,Y,\tau}=\sum_{i=0}^{n}W(F_i) B(F_i)
\label{eq:TrustCalculation}
\end{equation}

The belief value of the feature itself, $B(F_i)$, is also calculated through a sum of parameters pertaining to each of the $n$ belief sources $B_{F_i}^j$ composing the feature, as represented in Equation \ref{eq:TrustFeatureBeliefValueCalculation}, with each parameter described as follows: 

\begin{equation}
B(F_i) = \sum_{j=0}^{n} D^j_{F_i} C^j_{F_i} B_j 
\label{eq:TrustFeatureBeliefValueCalculation}
\end{equation}

\begin{itemize}
    \item $D^j_{F_i}$, a value from 0.0 to 1.0 that represents how far ago in time was this belief source received compared to the last one, being 0.0 a long time ago, and 1.0 the most recent belief. We wished to represent the rapid decay of value of old beliefs when compared to new ones, but also making sure recent memories would not fall quickly in value, so we chose to describe this parameter with a Gaussian Function, as represented in Equation \ref{eq:TimeValue}, where $T^{Last}_{F_i}$ is the most recent belief value's time stamp, $T^j_{F_i}$ is $B_{F_i}^j$ belief value's time stamp, and $L$ is the difference between the oldest and newest belief value's time stamps. We decided that $\frac{L}{4}$ defines a good mid drop-off point for the function.
    \item $C^j_{F_i}$, the certainty value stored in the Belief Source;
    \item $B^j_{F_i}$, the belief value stored in the Belief Source;
\end{itemize}


\begin{equation}
D^j_{F_i} = \euler^{-\frac{T^{Last}_{F_i} - T^j_{F_i}}{2(\frac{L}{4})^2}}
\label{eq:TimeValue}
\end{equation}


% Agent contains Trustees, which are representations of the other agents.
% A trustee has a set of features that the trustor has assigned as representative of it's trust.
% A feature belongs to a certain category, which in most scenarios, would be Ability and Willingness
% A feature has a belief value, that is calculated from a set of belief sources
% A belief source can either be Direct Contact, Reputation or Bias
% Each belief source has three values, a belief value, a certainty value, and a time value.
% Belief value is a normalized number that describes the trustee's evaluation on that feature
% Certainty describes how well the trustee was evaluated, in Reputation for instance, this might represent how well we trust in the reporter, and in Direct Contact how well the trustor observed the trustee performing said feature.
% Time is just a record of when was this belief source recorded, as older records might have a lower impact in the overall belief value score, compared to newer records.
% Perceptions correspond to the inputs from the environment that are inserted into the model as belief values to associated features.
% 


\section{Perceptions}
Another issue we encountered in literature was a lack of detail on how changes in the environment would be inserted into the model, so we try to solve that issue by inserting relevant perceptions as part of the model. As a result, a variety of environment inputs are defined in the model. This is done through a Perception object, representing some possible environment input, and containing a map of what target features should have belief sources added, what kind of belief sources they are, and how to translate the values received from the environment to belief value and certainty, as exemplified in Figure \ref{fig:Perceptions Diagram}. When adding a Belief Source to a Trustee, if the associated Feature is not present, then it is added with the Belief Source. The affected Trustor and Trustee are received as arguments.

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/PerceptionsDiagram.jpg}
    \caption{Perception Example}
    \label{fig:Perceptions Diagram}
\end{figure}



\section{Action Suggestion}
\label{sec:ActionSuggestion}
This is the module that is responsible for suggesting actions to the agent, in order to improve trust. It is composed by a series of Action objects, each represented by $A$ and containing the following:
\begin{itemize}
    \item $A_F = \{F_1, F_2, ..., F_i, ..., F_n\}$: A collection of $n$ relevant Feature Models that this Action will affect. At least 1 Feature Model needs to be present in the action, but $n$ is only limited by number available in the Model;
    \item $A_B(F_i) = \{B_1^{F_i}, B_2^{F_i}, ..., B_j^{F_i}, ..., B_m^{F_i}\}$: Each $F_i$ Feature Model belonging to $F$ has a collection of Belief Sources that describe how will the Feature be affected by the Action. Through this Belief Sources it is possible to predict how will the model change with this action, by inserting this Belief Sources in a mock model;
    \item $A_E = \{E_1, E_2, ..., E_i, ... E_p\}$: Each Action is mapped into $p$ Environment Inputs, serving as flags to signal when it is appropriate to perform said Action;
    \item $A_a$: The action plan that is actually sent to the agent's planner. The definition of this plan is obviously dependent on the agent's architecture receiving the plan. While the complexity of this plans can achieve the implementation of social strategies, in the scope of this thesis, the actions were restricted to utterances that try to justify low ability or willingness (e.g. Saying that last game's low score was due to bad luck, but Jon can confirm my ability).
\end{itemize}

The Action Suggestion module tries to provide a suggestion when it is triggered by the reception of an Environment Input $E_i$. It then selects the Actions that have the received Environment Input mapped to them $E_i \in A_E$. The selected Actions are then sorted by a function $S_F$ representing the potential increase in trust on the associated Features $A_F$. How $S_F$ is defined is left as a parameter of the model, but we suggest a linear sum of all the differences in the affected features, as a simple solution (represented in Equation \ref{eq:ActionSuggestionLinear}). After sorting through the selected Actions, the top-most ranked is sent to the Agent's Planner, and if it is in fact performed, the predicted Feature's Belief Sources are inserted into Memory. This process is represented in Figure \ref{fig:ActionSuggestionDiagram}.

\begin{equation}
    S_F = \sum_{i=0}^{n} \Delta B(F_i)
    \label{eq:ActionSuggestionLinear}
\end{equation}


% A perception is mapped to relevant actions, and actions are allocated depending on time available and what is the lowest scoring relevant feature.
\begin{figure}[hbt]
    \centering
    \includegraphics[width=\textwidth]{figures/ActionSuggestionDiagram.jpg}
    \caption{Action Suggestion Behaviour Flow}
    \label{fig:ActionSuggestionDiagram}
\end{figure}


\section{Scenario Ontology}
\label{sec:Scenario Ontology}
While creating the model, we focused in making it generic, but easily adaptable and transferable between scenarios. So when using the model into a new scenario, a Scenario Ontology must be provided, consisting in 6 entity collections, containing objects previously described along this chapter: Agents, Tasks, Feature Models, Categories, Perceptions and Actions. These collections are composed of what is considered relevant in the scenario, but members can be easily added throughout scenario development and piloting, as new situations occur. Even when actively using the model, this collections are not static, as new Agents can be added, although the dynamic creation of new Tasks and Perceptions goes beyond the scope of this thesis. 

 